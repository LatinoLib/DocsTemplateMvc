Top/Arts	Complex Systems Art Performance The tasks of the artist in between this self-organizing process is firstly in the preparation of the used objects which are mostly liquids. The objects are put into physical relation in certain amounts and in certain configurations. This is equivalent to the initial state of the self-organizing. Proceeding from this initial state develops the process of creating a form, while the artist can influence the self-organization by regulations, like for example adding new objects or physical reconfigurations of existing objects.
Top/Business	Industry 1: Semantic Solutions: Generating Business Value from Semantic Web Technologies Thanks to the efforts of many researchers over the past five years, Semantic Web technologies have reached the point where they are now enabling new types of business solutions. In this presentation, we will show how IBM Research is using Semantic Web technologies and Semantic Super Computing to generate new insight from the Web, intranets and large document repositories. It includes an introduction and overview of the use of Semantic Super Computing to automatically identify, index and augment semantic information, covers relevant foundational technologies and concludes with a case study of one emerging application of the technology that we refer to as the Wikification of Corporate Corpora
Top/Business	Semantic Business Automation
Top/Business	Model-based Integration of Business and technology
Top/Business	Cambridge fenomenon of new technologies transfer into the economy Dr. Smeets will introduce the experiences of the university town of Cambridge, one of the most important European centers for the transfer of new technologies into the economy. He will focus on the development of the 'Cambridge phenomenon of new technologies transfer into the economy' and the conditions that influenced the success of this concept. On the basis of the above listed facts he will discuss the mechanisms for achieving this phenomenon. The lecture is being organized by the Slovenian Technology Park in Ljubljana in cooperation with the Jozef Stefan Institute and the British Embassy in Slovenia.
Top/Business	Future of Technology by Google co-founder Eric Schmidt
Top/Business	Calculating Latent Demand in the Long Tail An analytical framework for using powerlaw theory to estimate market size for niche products and consumer groups.He is the author of New York Times bestselling book The Long Tail: Why the Future of Business is Selling Less of More, which as published in 2006, and runs a blog on the subject at longtail.com. In 2007 he was named one of the Time 100, the newsmagazines list of the 100 men and women whose power, talent or moral example is transforming the world. [[http://en.wikipedia.org/wiki/Chris_Anderson_%28writer%29|Chris_Anderson - WIKIPEDIA]]
Top/Business	Technology Management
Top/Business	CNO trends and emerging collaborative forms
Top/Business	Open Source Enterprise Resource Planning and Order Management System for Eastern European Tool and Die Making Workshops
Top/Business	NGOSS Key Domains Fulfillment, Assurance, Billing
Top/Business	Preliminary Experiments with On-Line Adaptive GARCH Models
Top/Business	Chaordic Systems Thinking: An Introduction of Chaos and Complexity in Organisations and Management
Top/Business	Taking causality seriously: Propensity score methodology applied to estimate the effects of marketing interventions - Best PKDD paper
Top/Business	'VEN' The Story so Far
Top/Business	Comparisson Of Technologies For Connecting Business Processes Among Enterprises
Top/Business	Conclusion and Debate
Top/Business	Statistical techniques for fraud detection, prevention, and evaluation The talk begins by setting the context: fraud is defined and its breadth outlined; figures are given showing how significant fraud is; and different areas of fraud are examined, including health care fraud, banking fraud, and scientific fraud. The particular data analytic challenges of banking fraud are described and illustrated in detail. These include the fact that the classes are highly unbalanced (with typically no more than 1 in a 1000 transactions being fraudulent), that class labels may often be incorrect, that there will typically be delays in discovering the true labels, that the transaction arrival times are random, that the data are dynamic, and, perhaps most challenging of all, that the distributions are reactive, changing in response to the implementation of fraud detection systems. The role of mechanistic and empirical models in tackling these problems is described. Both have been widely used, and both have a contribution to make. Banking data, and in particular banking fraud data are examined in detail. Raw credit card transaction data have 70-80 variables per transaction, and this can be multiplied many-fold for behavioural data, as in fraud detection problems. Questions arise as to how to aggregate the data: should one try to classify individual transactions or should activity records be constructed? A fundamental aspect of any predictive problem in data analysis is the choice of an appropriate criterion for estimation and performance assessment. In the case of fraud, one needs, in particular, to combine both classification accuracy and timeliness of classification. This means that standard measures of classification performance, such as error rate, AUC, KS statistic, information value, etc, are not sufficient. Suitable measures and performance curves are described which combine these aspects and which are now being adopted by the industry. Various statistical (used here in John Chamberss sense of greater statistics) approaches have been developed for fraud detection problems, and some are described and illustrated, using data from some of the banks which have been collaborating with us. In particular, we look at supervised classification and anomaly detection methods. Finally in the context of banking fraud, some of the deeper but very important conceptual issues are outlined, including the economic imperative, whether fraud is now becoming acceptable, and what exactly we learn from empirical comparisons, Scientific fraud is contrasted with banking fraud. They have rather different drivers. In particular, financial gain is generally irrelevant to scientific fraud, which makes it an unusual kind of fraud - although, of course, the impact can be even more serious. Several examples are given, from a range of disciplines. The role of data analytic tools in detecting scientific fraud, and the nature of such tools, is described
Top/Business	Electronic Invoicing - 238 bilion reasons - to begin with...
Top/Business	Telework in Slovenia
Top/Business	andEuros d.o.o. The andEuros company is active in electronics and software research and development to provide complete solutions on system architecture, sensors, sensor networks, mixed signal hardware designs, distributed software and hardware platforms, embedded systems, FPGA/VHDL system-on-chip design, powerline communications and sensor protocols.
Top/Business	Interactive Session 'Barriers and Solutions'
Top/Business	Large Enterprise Open Innovation Strategy - NOKIA Open Innvation
Top/Business	Testability Snc Development, Deployement, Installation and Startup of: * Production Testing Lines for Electronic devices and equipment * Automated Test Benches * Mechanical Toolings for Electronics, i.e. fixtures * Replicas of existing Production/Testing lines * Technology transfer
Top/Business	Management Education in Emerging Markets; What Is the Same, What Is Different?
Top/Business	Lessons for Business School Research in Emerging Markets
Top/Computers	BulkFS: a Distributed Fault-Tolerant File System for Massive Data Applications
Top/Computers	Amazon Web Services Amazon has spent over a decade and $2 billion building the infrastructure, technical knowledge, and operational excellence to operate a world class web-scale computing platform. Amazon Web Services (AWS) has now released a variety of web services (programmatic access to its open APIs) that provide access to Amazon's robust infrastructure, easily and inexpensively. These fundamental services allow developers and their companies to build web applications in a reliable, scalable, and cost-effective manner. [[http://aws-portal.amazon.com/|Amazon Web Services @ Amazon.com]]
Top/Computers	7 Habits For Effective Text Editing 2.0 A large percentage of time behind the computer screen is spent on editing text. Investing a little time in learning more efficient ways to use a text editor pays itself back fairly quickly. This presentation will give an overview of the large number of ways of using Vim in a smart way to edit programs, structured text and documentation. Examples will be used to make clear how learning a limited number of habits will avoid wasting time and lower the number of mistakes. [[bram_moolenaar]] is mostly known for being the benevolent dictator of the text editor Vim. His roots are in electrical engineering and for a long time he worked on inventing image processing algorithms and software for big photo copying machines. At some point his work on Open-Source software became more important, making the development of Vim his full time job. He also did the A-A-P project in between Vim version 6.0 and 7.0. Now he works for Google in Zurich, still improving Vim on the side. His home page is [[http://www.moolenaar.net/]]. A [[http://en.wikipedia.org/wiki/Bram_Moolenaar|Wikipedia]] article is also available.
Top/Science	The Next Fifty Years of Science The scientific method which provides us with so many technological goodies does not resemble the science of 1600. Ever since Bacon, science has undergone a slow evolution. Landmarks in the history of the scientific method are the invention of libraries, indexes, citations, controlled experiments, peer review, placebos, double blind experiments, randomization, and search among others. At the core of the scientific method is the structuring of information. In the next 50 years, as the technologies of information and knowledge accelerate, the nature of the scientific process will change even more than it has in the last 400 years. We can't predict what specific inventions will arise in the next 50 years, but based on long-term trends in epistemic tools, I believe we can speculate on how the scientific method itself -- that is, how we know -- will change in the next five decades.
Top/Science	Dangerous Knowledge
Top/Science	E-Science and Computational Scientific Discovery
Top/Science	Predstavitev pravil sodelovanja
Top/Science	Evaluation of project proposals at European Commission : Experience of an evaluator
Top/Science	Universal Access to Human Knowledge (Or Public Access to Digital Materials) The goal of universal access to our cultural heritage is within our grasp. With current digital technology we can build comprehensive collections, and with digital networks we can make these available to students and scholars all over the world. The current challenge is establishing the roles, rights, and responsibilities of our libraries and archives in providing public access to this information. With these roles defined, our institutions will help fulfill this epic opportunity of our digital age.
Top/Science	Review of Russian research
Top/Science	What are most common reasons for rejecting a paper?
Top/Science	Science Policy in Slovenia Nagovor dravnega sekretarja
Top/Science	Popularising the science and researchers of PASCAL
Top/Society	Force, law and the prospects of survival
Top/Society	Design Thinking as a Problem Solving Tool in technology and life Design thinking is a process for practical, creative resolution of problems or issues. The stages of this process are suggested as: Define | Research | Ideate | Prototype | Choose | Implement | Learn Within these seven steps, problems can be framed, the right questions can be asked, more ideas can be created, and the best answers can be chosen. The steps aren't linear; they can occur simultaneously or be repeated. Although design is always subject to personal taste, design thinkers share a common set of values that drive innovation: these values are mainly creativity, ambidextrous thinking, teamwork, end-user focus, curiosity. There is considerable academic interest in understanding design thinking or design cognition, including an ongoing series of symposia on 'research in design thinking'.
Top/Society	Visual Lexicons: The Quest for Data - Driven Decision Making The eternal AI quest - can machines think as well as man? - seems quaint today compared to the question of how can machines help man to think. True, Deep Blue can beat the world's best chess player, not by thinking, but by exhaustively examining all permutations and combinations in blinding time against a predetermined outcome set of rules. The questions for mankind, though, seem of the form where rules are imprecise at best, and essentially unknowable perhaps. If learnable and knowable even, many other constraints exist that mitigate against 'data-driven decision-making'. This presentation assesses some of these constraints, and offers some perspective on the value of using visual dynamics and analytics to help overcome such issues.
Top/Society	Rules, Race, and Mel Gibson 2006 Slavoj Zizek talking about the explicit, truth, rules, politics, Mel Gibson, society, race, racism, antisemitism; lecturing and developing a psychoanalysis of culture and societies. Public open lecture for the students of the European Graduate School EGS, Media and Communication Studies department program, Saas-Fee, Switzerland, Europe, 2006,
Top/Society	Identity difference and tolleration Prof. Anna Elisabetta Galeotti z Univerze v Torinu. Predavateljica, ki je profesorica politine filozofije, in je med drugim tudirala v Cambridgeu, predavala na Evropski univerzi v Firencah, na Institute for Advanced Studies v Princetonu in na harvardski univerzi, svoje znanstvenoraziskovalno delo posebej posvea omenjeni tematiki, o njej je izdala tudi knjigo 'Una proposta pluralista', (Liguori, 1994)
Top/Society	Everything is Miscellaneous David Weinberger's new book covers the breakdown of the established order of ordering. He explains how methods of categorization designed for physical objects fail when we can instead put things in multiple categoreis at once, and search them in many ways. This is no dry book on taxonomy, but has the insight and wit you'd expect from the author of The Cluetrain Manifesto, Small Pieces Loosely Joined, and a former writer for Woody Allen.
Top/Society	Debunking third-world myths with the best stats you've ever seen You've never seen data presented like this. With the drama and urgency of a sportscaster, [[http://www.ted.com/index.php/speakers/view/id/90|Hans Rosling]] debunks myths about the so-called 'developing world' using extraordinary animation software developed by his Gapminder Foundation. The Trendalyzer software (recently acquired by Google) turns complex global trends into lively animations, making decades of data pop. Asian countries, as colorful bubbles, float across the grid -- toward better national health and wealth. Animated bell curves representing national income distribution squish and flatten. In Rosling's hands, global trends -- life expectancy, child mortality, poverty rates -- become clear, intuitive and even playful. (Recorded February 2006 in Monterey, CA. Duration: 20:35) - More TEDTalks at [[http://www.ted.com/]] ;//'Rosling believes that making information more accessible has the potential to change the quality of the information itself.' ://Business Week Online//
Top/Society	The Last Lecture of Randy Pausch Almost all of us have childhood dreams: for example, being an astronaut, or making movies or video games for a living. Sadly, most people don't achieve theirs, and I think that's a shame. I had several specific childhood dreams, and I've actually achieved most of them. More importantly, I have found ways, in particular the creation (with Don Marinelli), of CMU's ([[http://etc.cmu.edu/|Entertainment Technology Center]]), of helping many young people actually **achieve** their childhood dreams. This talk will discuss how I achieved my childhood dreams (being in zero gravity, designing theme park rides for Disney, and a few others), and will contain realistic advice on how **you** can live your life so that you can make your childhood dreams come true, too.
Top/Society	Wikipedia: a social innovation ?
Top/Society	Outdoctrination: Society, Children, Technology and Self Organisation in Education
Top/Society	Distorted Morality Speech at Harvard University about America's war on terror
Top/Society	Wireless Tech & Regulatory Reality: Policy and Fantasy in the 21st Century Sascha Meinrath has been described as a '[[http://www.savetheinternet.com/=coalition|community Internet pioneer]]' and an '[[http://infodev-study.oplan.org/the-study/1-background-and-introduction/copy4_of_1-background-and-introduction|entrepreneurial visionary]]' and is a well-known expert on [[http://en.wikipedia.org/wiki/Wireless_community_network|community wireless networks]] (CWNs) and [[http://en.wikipedia.org/wiki/Municipal_broadband|municipal broadband]]. Leading news sources, including [[http://www.economist.com/science/displayStory.cfm?story_id=3535732|the Economist]], [[http://www.nytimes.com/2006/09/27/technology/circuits/27fon.html?ex=1160539200&en=53c38adbd350e304&ei=5070|the New York Times]], [[http://www.thenation.com/blogs/edcut?pid=77928|the Nation]], and [[http://www.npr.org/templates/story/story.php?storyId=4834612|National Public Radio]], often cite Sascha's work in covering issues related to CWNs. Sascha is the Research Director for the [[http://www.newamerica.net/|New America Foundation's]] [[http://www.spectrumpolicy.org/|Wireless Future Program]]. Additionally, he coordinates the [[http://www.oswc.net/|Open Source Wireless Coalition]], a global partnership of open source wireless integrators, researchers, implementors and companies dedicated to the development of open source, interoperable, low-cost wireless technologies. He is a regular contributor to [[http://www.muniwireless.com/|MuniWireless.com]], the leading source for municipal wireless news and information, and a regular contributor to [[http://govtech.net/digitalcommunities|Government Technology's Digital Communities]], the online portal and comprehensive information resource for the public sector. Sascha has also worked with [[http://www.freepress.net/|Free Press]], the [[http://www.caida.org/|the Cooperative Association for Internet Data Analysis (CAIDA)]], the [[http://www.acornactivemedia.com/| Acorn Active Media Foundation]], the [[http://www.ethoswireless.com/|Ethos Group]], and the [[http://www.cuwin.net/|CUWiN Foundation]]. Sascha holds a Bachelor's Degree from [[http://www.yale.edu/|Yale University]] and a Master's Degree from the [[http://www.psych.uiuc.edu/|University of Illinois at Urbana-Champaign]], both in psychology. He is a Telecommunications Fellow at the University of Illinois in the [[http://www.comm.uiuc.edu/icr/|Institute for Communications Research]], where he is finishing his PhD on community empowerment and the impacts and interactions of participatory media, wireless communications, and emergent technologies. more >>> [[http://www.saschameinrath.com/|saschameinrath.com]]
Top/Society	Telework in Slovenia
Top/Society	Conversations with History: Noam Chomsky On this edition of Conversations with History, UC Berkeley's Harry Kreisler is joined by linguist and political activist Noam Chomsky to discuss activism, anarchism and the role the United States plays in the world today.
Top/Society	Demography - Who pays my pension ? Simple computer simulations indicate in many European countries serious problems for old-age pensions around 2030 or later: Many more old people and much less young people. Possible remedies are immigration and increase in retirement ages. In Algeria the present situation seems better balanced, in the Palestinian Territories it is the opposite, for the next few decades.
Top/Computer_Science	Pipelined Vector Processing and Scientific Computation For almost two decades (1976-1993) technical-scientific high-performance computing has been dominated by vector processing, pioneered by Seymour Cray and his Cray Research Incorporated. This technology is explained in detail, and the significant cost-performance advantages are outlined. While parallel processing is more en-vogue today, and in fact is dominating scientific computation since 1993, a comeback of vector processing may be on the horizon due to the IBM/Sony/Toshiba Cell processor being a parallel vector processor, and being the highest-performance single chip available today.
Top/Computer_Science	Challenges in Social Network Data: Processes, Privacy and Paradoxes The proliferation of rich social media, on-line communities, and collectively produced knowledge resources has accelerated the convergence of technological and social networks, producing environments that reflect both the architecture of the underlying information systems and the social structure on their members. In studying the consequences of these developments, we are faced with the opportunity to analyze social network data at unprecedented levels of scale and temporal resolution; this has led to a growing body of research at the intersection of the computing and social sciences. **Do you have a question for this lecturer&#160;at the KDD 2007?&nbsp; We encourage you to start a debate, comment on each lecturers video or send us an email and we will ask them for you! ** //**Disclamer:**// //Videolectures.Net emphasises that the quality of this video was notably improved, because of low light quality conditions provided in the lecture auditorium. //
Top/Computer_Science	Signal Processing
Top/Computer_Science	Bipartite Graph Matching for Computing the Edit Distance of Graphs In the field of structural pattern recognition graphs constitute a very common and powerful way of representing patterns. In contrast to string representations, graphs allow us to describe relational information in the patterns under consideration. One of the main drawbacks of graph representations is that the computation of standard graph similarity measures is exponential in the number of involved nodes. Hence, such computations are feasible for rather small graphs only. One of the most flexible error-tolerant graph similarity measures is based on graph edit distance. In this paper we propose an approach for the efficient compuation of edit distance based on bipartite graph matching by means of Munkres algorithm, sometimes referred to as the Hungarian algorithm. Our proposed algorithm runs in polynomial time, but provides only suboptimal edit distance results. The reason for its suboptimality is that implied edge operations are not considered during the process of finding the optimal node assignment. In experiments on semi-artificial and real data we demonstrate the speedup of our proposed method over a traditional tree search based algorithm for graph edit distance computation. Also we show that classification accuracy remains nearly unaffected.
Top/Computer_Science	The Complexity of Computation
Top/Computer_Science	How much are Computers able to Understand text?
Top/Computer_Science	Computers versus Common Sense It's way past 2001 now, where the heck is HAL? For several decades now we've had high hopes for computers amplifying our mental abilities not just giving us access to relevant stored information, but answering our complex, contextual questions. Even applications like human-level unrestricted speech understanding continue to dangle close but just out of reach. What's been holding AI up? The short answer is that while computers make fine idiot savants, they lack common sense: the millions of pieces of general knowledge we all share, and fall back on as needed, to cope with the rough edges of the real world. I
Top/Computer_Science	Freebase: An Open, Writable Database of the Worlds Information Freebase is an open database of the worlds information, built by a global community and free for anyone to query, contribute to, and build applications on. Drawing from large open data sets like Wikipedia, MusicBrainz, GNIS, EDGAR etc., Freebase is curated by a passionate community of users and contains structured information on millions of topics such as people, places, music, film, food, science, historical events, and more. Part of what makes this open database unique is that it spans domains, but requires that a particular topic exist only once in Freebase. Thus freebase is an identity database with a user contributed schema which spans multiple domains. For example, Arnold Schwarzenegger may appear in a movie database as an actor, a political database as a governor, and in a bodybuilder database as Mr. Universe. In Freebase, however, there is only one topic for Arnold Schwarzenegger that brings all these facets together. The unified topic is a single reconciled identity, which makes it easier to find and contribute information about the linked world we live in.
Top/Computer_Science	Ranking by Stealing Human Cycles Ranking objects is a challenging task for machines. The main difficulty is that some characteristics of interest lack objective criteria. As the Internet becomes more widely used, it is possible to integrate the human capability of evaluating unmeasurable properties with the computational power of machines. A good example is the Internet voting for photos, foods and many others. In this talk, we propose a paired comparison framework, in which users are asked to show preferences in a pair of objects. Experiments on a photo ranking task show that the paired method outperforms the commonly used scoring method.
Top/Computer_Science	New methods for digital generation and postprocesing of true random numbers
Top/Computer_Science	My Turing Machine or Yours? The title of this talk is Your Turing Machine or Mine?. What I am alluding to with this title is the universality of a universal Turing machine, and at the same time, to the fact that there are many different universal Turing machines with somewhat different properties. Universal Turing machines are both universal and individual, in different senses. A universal Turing machine is one that can emulate (or imitate) any other Turing machine, and thus in a sense can undertake to compute any of a very large class of computable functions. But there are an indefinitely large number of universal Turing machines that can be defined.
Top/Computer_Science	The genesis of WiFi and its applications In 1985 changes to US regulations caused a paradigm shift by permitting the use of radio spectrum for devices that did not need to have an end-user license. After a few years, products appeared on the market and a group developed a standard for broadband wireless communications among computers. The presentation will explain how the standard developed and was adopted by the Wi-Fi Alliance, as well as the global harmonization and expansion of the available radio spectrum to over half a GHz. The success of Wi-Fi and user innovation and initiatives makes it a vehicle to bring broadband internet to rural areas both in developing as well as in developed countries. Vic Hayes is a Senior Research Fellow at the Delft University of Technology and is writing a book titled 'The genesis of Wi-Fi and the road toward global success'. He holds a BSEE and joined NCR in the Netherlands in 1974. He co-established and chaired the IEEE 802.11 Standards Working Group for Wireless Local Area Networks and became known as the 'Father of Wi-Fi'. After chairing the WG, he successfully mobilized the computer industry to support the agenda item for 455 MHz of spectrum at co-primary allocation on the agenda of the World Radio Conference 2003. In October 2003, Vic retired from Agere Systems. For his pioneering work on Wi-Fi, Vic is the recipient of the Innovation Award 2004 of 'The Economist', the Dutch Vosko Trophy, 2 Wi-Fi Alliance Leadership Awards, The IEEE Standards Medallion, the IEEE Leadership Award, the IEEE Hans Karlsson Award and the IEEE Steinmetz Award.
Top/Computer_Science	The Bill and Melinda Gates Foundation: pushing the boundaries of what is possible in public health - perspectives and challenges to the global community Introduction: In July 2008, Bill Gates will transition out of a day-to-day role at Microsoft, to spend more time on his philanthropic work with the Bill and Melinda Gates Foundation. As the worlds largest philanthropic organization, the Gates Foundation has set ambitious goals to tackle some of the worlds worst diseases. In this talk, Julie Jacobson will outline some of the objectives of the Gates Foundation and how it is impacting public health and challenging the global community. Speaker Bio: As Senior Programme Officer at the Bill and Melinda Gates Foundation, Julie Jacobson currently supports grants working toward the control of neglected tropical diseases and works with the development and implementation of new vaccines in the infectious disease group of Global Health. Previously Dr. Jacobson was Scientific Director of Immunization Solutions and Director of Japanese encephalitis (JE) project at PATH, an international non-profit organization. As director of the JE project, she managed a US$35 million grant to accelerate the control of JE in endemic countries by improving data on the distribution of JE, accelerating the development of an improved vaccine and diagnostic tests for JE, and helping countries integrate JE vaccine into immunization programs. In her role as scientific director she defined the direction and growth of immunization solutions work by increasing the availability of vaccines to the worlds most vulnerable populations. This ranged from work on clinical trials for specific vaccines to directly working with ministries of health and partners in decision-making on vaccine introduction and planning. Before that, Dr. Jacobson was responsible for prioritizing and designing field activities for PATHs Childrens Vaccine Project in the areas including yellow fever and rotavirus. Prior to joining PATH, Dr. Jacobson worked at the U.S. Centers for Disease Control and Prevention as an Epidemic Intelligence Officer. In this capacity, she worked in disaster epidemiology and conducted needs assessments for disaster victims, evaluated national surveillance systems, and evaluated the health impact of earthquakes on displaced persons. Dr. Jacobson is a physician with training in clinical tropical medicine and applied epidemiology.
Top/Computer_Science/Machine_Learning	Machine Learning for Games The course gives an introduction to the application of machine learning techniques to games. The course will consist of two parts, part I dealing with computer/video games, part II dealing with traditional board/strategy games. Alongside, I will introduce necessary background material including aspects of neural networks, reinforcement learning, and graphical models. 1. In recent years various aspects of computer games have been developed to near perfection. These include high-performance graphics, realistic surround sound, and detailed physical simulations. However, the control of non-player characters (NPCs), also known as game AI, has fallen behind to the point that the resulting gaming experience often suffers. Machine learning offers a framework for making NPCs adaptive to both the environment and the human player. This technology has therefore the potential to greatly enhance gaming experience. Furthermore, at development time machine learning techniques can be employed to automate the creation of (intelligent) NPC behavior, thereby replacing the current standard of scripting and trial-and-error. The examples presented include imitation learning for avatars and reinforcement learning in fighting games. 2. Classical board games such as Chess, Go, and Backgammon have been a traditional theme in artificial intelligence. While chess has essentially been solved by traditional AI approaches, world-class Backgammon engines could only be developed based on machine learning techniques, originally in the combination of neural networks and reinforcement learning. For the traditional board game Go, neither of the two approaches has been successful so far. In this part of the course I will explain and discuss the machine learning approach to Backgammon. I will then give an introduction to the game of Go and discuss what machine learning may be able to contribute to the field of computer Go with a particular focus on modeling the uncertainty that emerges from the game's overwhelming complexity.
Top/Computer_Science/Machine_Learning	Machine Learning Reductions There are several different classification problems commonly encountered in real world applications such as 'importance weighted classification', 'cost sensitive classification', 'reinforcement learning', 'regression' and others. Many of these problems can be related to each other by simple machines (reductions) that transform problems of one type into problems of another type. Finding a reduction from your problem to a more common problem allows the reuse of simple learning algorithms to solve relatively complex problems. It also induces an organization on learning problems problems that can be easily reduced to each other are 'nearby' and problems which can not be so reduced are not close.
Top/Computer_Science/Machine_Learning	Teaching Machine Learning from Examples
Top/Computer_Science/Machine_Learning	Tutorial on Machine Learning Reductions There are several different classification problems commonly encountered in real world applications such as 'importance weighted classification', 'cost sensitive classification', 'reinforcement learning', 'regression' and others. Many of these problems can be related to each other by simple machines (reductions) that transform problems of one type into problems of another type. Finding a reduction from your problem to a more common problem allows the reuse of simple learning algorithms to solve relatively complex problems. It also induces an organization on learning problems problems that can be easily reduced to each other are 'nearby' and problems which can not be so reduced are not close.
Top/Computer_Science/Machine_Learning	Object Correspondence as a Machine Learning Problem
Top/Computer_Science/Machine_Learning	Machine Learning, Probability and Graphical Models
Top/Computer_Science/Machine_Learning	Semisupervised Learning Approaches
Top/Computer_Science/Machine_Learning	Gaussian Process Basics How on earth can a plain old Gaussian distribution be useful for sophisticated regression and machine learning tasks?
Top/Computer_Science/Machine_Learning	Dirichlet Processes, Chinese Restaurant Processes, and all that Bayesian approaches to learning problems have many virtues, including their ability to make use of prior knowledge and their ability to link related sources of information, but they also have many vices, notably the strong parametric assumptions that are often invoked willy-nilly in practical Bayesian modeling. Nonparametric Bayesian methods offer a way to make use of the Bayesian calculus without the parametric handcuffs. In this talk I describe several recent explorations in nonparametric Bayesian modeling and inference, including various versions of 'Chinese restaurant process priors' that allow flexible structures to be learned and allow sharing of statistical strength among sets of related structures. I discuss applications to problems in bioinformatics and information retrieval.
Top/Computer_Science/Machine_Learning	Introduction to Machine Learning This course covers feature selection fundamentals and applications. The students will first be reminded of the basics of machine learning algorithms and the problem of overfitting avoidance. In the wrapper setting, feature selection will be introduced as a special case of the model selection problem. Methods to derive principled feature selection algorithms will be reviewed as well as heuristic method, which work well in practice. One class will be devoted to feature construction techniques. Finally, a lecture will be devoted to the connections between feature section and causal discovery. The class will be accompanied by several lab sessions. The course will be attractive to students who like playing with data and want to learn practical data analysis techniques. The instructor has ten years of experience with consulting for startup companies in the US in pattern recognition and machine learning. Datasets from a variety of application domains will be made available: handwriting recognition, medical diagnosis, drug discovery, text classification, ecology, marketing.
Top/Computer_Science/Machine_Learning	Statistical Learning Theory This course will give a detailed introduction to learning theory with a focus on the classification problem. It will be shown how to obtain (pobabilistic) bounds on the generalization error for certain types of algorithms. The main themes will be: * probabilistic inequalities and concentration inequalities * union bounds, chaining * measuring the size of a function class, Vapnik Chervonenkis dimension, shattering dimension and Rademacher averages * classification with real-valued functions Some knowledge of probability theory would be helpful but not required since the main tools will be introduced.
Top/Computer_Science/Machine_Learning	Embedded Methods This course covers feature selection fundamentals and applications. The students will first be reminded of the basics of machine learning algorithms and the problem of overfitting avoidance. In the wrapper setting, feature selection will be introduced as a special case of the model selection problem. Methods to derive principled feature selection algorithms will be reviewed as well as heuristic method, which work well in practice. One class will be devoted to feature construction techniques. Finally, a lecture will be devoted to the connections between feature section and causal discovery. The class will be accompanied by several lab sessions. The course will be attractive to students who like playing with data and want to learn practical data analysis techniques. The instructor has ten years of experience with consulting for startup companies in the US in pattern recognition and machine learning. Datasets from a variety of application domains will be made available: handwriting recognition, medical diagnosis, drug discovery, text classification, ecology, marketing.
Top/Computer_Science/Machine_Learning	Bayesian models of human inductive learning In everyday learning and reasoning, people routinely draw successful generalizations from very limited evidence. Even young children can infer the meanings of words, hidden properties of objects, or the existence of causal relations from just one or a few relevant observations -- far outstripping the capabilities of conventional learning machines. How do they do it? And how can we bring machines closer to these human-like learning abilities? I will argue that people's everyday inductive leaps can be understood as approximations to Bayesian computations operating over structured representations of the world, what cognitive scientists have called 'intuitive theories' or 'schemas'. For each of several everyday learning tasks, I will consider how appropriate knowledge representations are structured and used, and how these representations could themselves be learned via Bayesian methods. The key challenge is to balance the need for strongly constrained inductive biases -- critical for generalization from very few examples -- with the flexibility to learn about the structure of new domains, to learn new inductive biases suitable for environments which we could not have been pre-programmed to perform in. The models I discuss will connect to several directions in contemporary machine learning, such as semi-supervised learning, structure learning in graphical models, hierarchical Bayesian modeling, and nonparametric Bayes.
Top/Computer_Science/Machine_Learning	Some Mathematical Tools for Machine Learning These are lectures on some fundamental mathematics underlying many approaches and algorithms in machine learning. They are not about particular learning algorithms; they are about the basic concepts and tools upon which such algorithms are built. Often students feel intimidated by such material: there is a vast amount of 'classical mathematics', and it can be hard to find the wood for the trees. The main topics of these lectures are Lagrange multipliers, functional analysis, some notes on matrix analysis, and convex optimization. I've concentrated on things that are often not dwelt on in typical CS coursework. Lots of examples are given; if it's green, it's a puzzle for the student to think about. These lectures are far from complete: perhaps the most significant omissions are probability theory, statistics for learning, information theory, and graph theory. I hope eventually to turn all this into a series of short tutorials. Please let me know of any errors, etc. ; ://from Chris Burges homepage : [[http://research.microsoft.com/~cburges]] **Lecture contains:** Lagrange multipliers: * Lagrange the Mathematician * Lagrange multipliers: an indirect approach can be easier * Multiple Equality Constraints * Multiple Inequality Constraints * Two points on a d-sphere * The Largest Parallelogram * Resource allocation * A convex combination of numbers is maximized by choosing the largest * The Isoperimetric problem * For fixed mean and variance, which univariate distribution has maximum entropy? * An exact solution for an SVM living on a simplex Notes on some Basic Statistics * Probabilities can be Counter-Intuitive (Simpson's paradox; the Monty Hall puzzle) * IID-ness: Measurement Error decreases as 1/sqrt{n} * Correlation versus Independence * The Ubiquitous Gaussian: ** Product of Gaussians is Gaussian ** Convolution of two Gaussians is a Gaussian ** Projection of a Gaussian is a Gaussian ** Sum of Gaussian random variables is a Gaussian random variables ** Uncorrelated Gaussian variables are also independent ** Maximum Likelihood Estimates for mean and covariance (prove required matrix identities) ** Aside: For 1-dim Laplacian, max. likelihood gives the median * Using cumulative distributions to derive densities Principal Component Analysis and Generalizations * Ordering by Variance * Does Grouping Change Things? * PCA Decorrelates the Samples * PCA gives Reconstruction with Minimal Mean Squared Error * PCA preserves Mutual Information on Gaussian data * PCA directions lie in the span of the data * PCA: second order moments only * The Generalized Rayleigh Quotient ** Non-orthogonal principal directions ** OPCA ** Fisher Linear Discriminant ** Multiple Discriminant Analysis Elements of Functional Analysis * High Dimensional Spaces * Is Winning Transitive? * Most of the Volume is Near the Surface: Cubes * Spheres in n-dimensions * Banach Spaces, Hilbert Spaces, Compactness * Norms * Useful Inequalities (Minkowski and Holder) * Vector Norms * Matrix Norms * The Hamming Norm * L1, L2, L_infty norms - is L0 a norm? * Example: Using a Norm as a Constraint in Kernel Algorithms
Top/Computer_Science/Machine_Learning	Anti-Learning The Biological domain poses new challenges for statistical learning. In the talk we shall analyze and theoretically explain some counter-intuitive experimental and theoretical findings that systematic reversal of classifier decisions can occur when switching from training to independent test data (the phenomenon of anti-learning). We demonstrate this on both natural and synthetic data and show that it is distinct from overfitting. The natural datasets discussed will include: prediction of response to chemo-radio-therapy for esophageal cancer from gene expression (measured by cDNA-microarrays); prediction of genes affecting the aryl hydrocarbon receptor pathway in yeast. The main synthetic classification problem will be the approximation of samples drawn from high dimensional distributions, for which a theoretical explanation will be outlined.
Top/Computer_Science/Machine_Learning	Human Computation Tasks like image recognition are trivial for humans, but continue to challenge even the most sophisticated computer programs. This talk introduces a paradigm for utilizing human processing power to solve problems that computers cannot yet solve. Traditional approaches to solving such problems focus on improving software. I advocate a novel approach: constructively channel human brainpower using computer games. For example, the ESP Game, described in this talk, is an enjoyable online game -- many people play over 40 hours a week -- and when people play, they help label images on the Web with descriptive keywords. These keywords can be used to significantly improve the accuracy of image search. People play the game not because they want to help, but because they enjoy it. I describe other examples of 'games with a purpose': Peekaboom, which helps determine the location of objects in images, and Verbosity, which collects common-sense knowledge. I also explain a general approach for constructing games with a purpose.
Top/Computer_Science/Machine_Learning	Learning and Charting Chemical Space with Strings and Graphs: Challenges and Opportunities for AI and Machine Learning Informatics methods and computers have not yet become as pervasive in chemistry as they have in physics and biology. Drawing analogies from bioinformatics, key ingredients for progress in chemoinformatics are the availability of large, annotated databases of compounds and reactions, data structures and algorithms to efficiently search these databases, and computational methods to predict the physical, chemical, and biological properties of new compounds and reactions. We will describe how graph-based methods play a key role in the development of: (1) a large public database of compounds and reactions (ChemDB) and the underlying algorithms and representations; (2) machine learning kernel methods to predict molecular properties; and (3) the applications of these methods to drug screening/design problems and the identification of new drug leads against a major disease.
Top/Computer_Science/Machine_Learning	Online Learning
Top/Computer_Science/Machine_Learning	A simple feature extraction for high dimensional image representations We investigate a method to find local clusters in low dimensional subspaces of high dimensional data, e.g. in high dimensional image descriptions. Using cluster centers instead of the full set of data will speed up the performance of learning algorithms for object recognition, and will possibly also improve performance because overfitting might be avoided.
Top/Computer_Science/Machine_Learning	Hidden Topic Markov Models Algorithms such as Latent Dirichlet Allocation (LDA) have achieved significant progress in modeling word document relationships. These algorithms assume each word in the document was generated by a hidden topic and explicitly model the word distribution of each topic as well as the prior distribution over topics in the document. Given these parameters, the topics of all words in the same document are assumed to be independent. In this work, we propose modeling the topics of words in the document as a Markov chain. Specifically, we assume that all words in the same sentence have the same topic, and successive sentences are more likely to have the same topics. Since the topics are hidden, this leads to using the well-known tools of Hidden Markov Models for learning and inference. We show that incorporating this dependency allows us to learn better topics and to disambiguate words that can belong to different topics. Quantitatively, we show that we obtain better perplexity in modeling documents with only a modest increase in learning and inference complexity. //Joint work with Michal Rosen-Zvi and Yair Weiss.//
Top/Computer_Science/Machine_Learning	Machine learning and finance
Top/Computer_Science/Machine_Learning	Ten problems for the next 10 years
Top/Computer_Science/Machine_Learning	Visual Lexicons: The Quest for Data - Driven Decision Making The eternal AI quest - can machines think as well as man? - seems quaint today compared to the question of how can machines help man to think. True, Deep Blue can beat the world's best chess player, not by thinking, but by exhaustively examining all permutations and combinations in blinding time against a predetermined outcome set of rules. The questions for mankind, though, seem of the form where rules are imprecise at best, and essentially unknowable perhaps. If learnable and knowable even, many other constraints exist that mitigate against 'data-driven decision-making'. This presentation assesses some of these constraints, and offers some perspective on the value of using visual dynamics and analytics to help overcome such issues.
Top/Computer_Science/Machine_Learning	Declarative Vs. Procedural
Top/Computer_Science/Machine_Learning	Empirical Inference
Top/Computer_Science/Machine_Learning	Dirichlet Processes: Tutorial and Practical Course **The Bayesian approach** allows for a coherent framework for dealing with uncertainty in machine learning. By integrating out parameters, Bayesian models do not suffer from overfitting, thus it is conceivable to consider models with infinite numbers of parameters, aka Bayesian nonparametric models. An example of such models is the Gaussian process, which is a distribution over functions used in regression and classification problems. Another example is the Dirichlet process, which is a distribution over distributions. Dirichlet processes are used in density estimation, clustering, and nonparametric relaxations of parametric models. It has been gaining popularity in both the statistics and machine learning communities, due to its computational tractability and modelling flexibility. In the tutorial I shall introduce Dirichlet processes, and describe different representations of Dirichlet processes, including the Blackwell-MacQueen? urn scheme, Chinese restaurant processes, and the stick-breaking construction. I shall also go through various extensions of Dirichlet processes, and applications in machine learning, natural language processing, machine vision, computational biology and beyond. In the practical course I shall describe inference algorithms for Dirichlet processes based on Markov chain Monte Carlo sampling, and we shall implement a Dirichlet process mixture model, hopefully applying it to discovering clusters of NIPS papers and authors.
Top/Computer_Science/Machine_Learning	PANEL: Experiences in research, teaching, and applications of ML
Top/Computer_Science/Machine_Learning	Bias/variance analysis of relational domains
Top/Computer_Science/Machine_Learning	Opening of the 9th Machine Learning Summer School
Top/Computer_Science/Machine_Learning	When Training and Test Distributions are Different: Characterising Learning Transfer
Top/Computer_Science/Machine_Learning	A discussion about ML
Top/Computer_Science/Machine_Learning	Learning Similarity Metrics with Invariance Properties
Top/Computer_Science/Machine_Learning	Learning without overlearning This course covers feature selection fundamentals and applications. The students will first be reminded of the basics of machine learning algorithms and the problem of overfitting avoidance. In the wrapper setting, feature selection will be introduced as a special case of the model selection problem. Methods to derive principled feature selection algorithms will be reviewed as well as heuristic method, which work well in practice. One class will be devoted to feature construction techniques. Finally, a lecture will be devoted to the connections between feature section and causal discovery. The class will be accompanied by several lab sessions. The course will be attractive to students who like playing with data and want to learn practical data analysis techniques. The instructor has ten years of experience with consulting for startup companies in the US in pattern recognition and machine learning. Datasets from a variety of application domains will be made available: handwriting recognition, medical diagnosis, drug discovery, text classification, ecology, marketing.
Top/Computer_Science/Machine_Learning	Which Supervised Learning Method Works Best for What? An Empirical Comparison of Learning Methods and Metrics Decision trees are intelligible, but do they perform well enough that you should use them? Have SVMs replaced neural nets, or are neural nets still best for regression, and SVMs best for classification? Boosting maximizes margins similar to SVMs, but can boosting compete with SVMs? And if it does compete, is it better to boost weak models, as theory might suggest, or to boost stronger models? Bagging is simpler than boosting -- how well does bagging stack up against boosting? Breiman said Random Forests are better than bagging and as good as boosting. Was he right? And what about old friends like logistic regression, KNN, and naive bayes? Should they be relegated to the history books, or do they still fill important niches? In this talk we compare the performance of ten supervised learning methods on nine criteria: Accuracy, F-score, Lift, Precision/Recall Break-Even Point, Area under the ROC, Average Precision, Squared Error, Cross-Entropy, and Probability Calibration. The results show that no one learning method does it all, but some methods can be 'repaired' so that they do very well across all performance metrics. In particular, we show how to obtain the best probabilities from max margin methods such as SVMs and boosting via Platt's Method and isotonic regression. We then describe a new ensemble method that combines select models from these ten learning methods to yield much better performance. Although these ensembles perform extremely well, they are too complex for many applications. We'll describe what we're doing to try to fix that. Finally, if time permits, we'll discuss how the nine performance metrics relate to each other, and which of them you probably should (or shouldn't) use. During this talk I'll briefly describe the learning methods and performance metrics to help make the lecture accessible to non-specialists in machine learning.
Top/Computer_Science/Machine_Learning	Other ML/DM software (R, Weka, Yale)
Top/Computer_Science/Machine_Learning	Estimating the Joint AUC of Labelled and Unlabelled Data
Top/Computer_Science/Machine_Learning	Machine learning for access and retrieval I
Top/Computer_Science/Machine_Learning	Decision Maps
Top/Computer_Science/Machine_Learning	Overview of Environmental applications of Machine Learning
Top/Computer_Science/Machine_Learning	Trees for Regression and Classification Tree models are widely used for regression and classification problems, with interpretability and ease of implementation being among their chief attributes. Despite the widespread use tree models, a comprehensive theoretical analysis of their performance has only begun to emerge in recent years. This lecture provides an overview of tree modeling theory and methods, with an emphasis on risk bounds, oracle inequalities, approximation theory, and rates of convergence, in a variety of contexts. Special attention is devoted to decision trees and wavelet-based regression methods, two of the most well-known examples of tree models. The choice of loss function (squared error, absolute error, 0/1 error) plays a pivotal role in both theory and methods. In particular, optimal tree selection rules vary dramatically depending on the loss function employed. Despite these differences, suitable tree-based models coupled with appropriate selection rules can provide fast algorithms and near-minimax optimal performance in a very broad range of regression and classification problems. Examples from image reconstruction and pattern classification will demonstrate the effectiveness of trees in practice.
Top/Computer_Science/Machine_Learning	Two-eyed algorithms and problems - Best ECML paper
Top/Computer_Science/Machine_Learning	Universal Modeling: Introduction to modern MDL We give a tutorial introduction to the *modern* Minimum Description Length (MDL) Principle, taking into account the many refinements and developments that have taken place in the 1990s. These do not seem to be widely known outside the information theory community. We will especially emphasize the use of MDL in classification. We also consider the connections between MDL, Bayesian inference, maximum entropy inference and structural risk minimization.
Top/Computer_Science/Machine_Learning	The Machine Learning Approach to Brain-Computer Interfacing - Part 1
Top/Computer_Science/Machine_Learning	Learning Classifiers in Distribution and Cost-sensitive Environments
Top/Computer_Science/Machine_Learning	PAC-Bayes Analysis of Classification The lecture will introduce the PAC Bayes approach to the statistical analysis of learning. After some historical introduction, the key theorems will be covered. We will then consider some applications including for Support Vector Machines and novelty detection. A discussion of the status of the prior in the approach will lead to an investigation of how learning the prior can be used in practical applications. Discussions of further extensions of the approach will conclude the presentation.
Top/Computer_Science/Machine_Learning	Spooky Stuff in Metric Space Decision trees are intelligible, but do they perform well enough that you should use them? Have SVMs replaced neural nets, or are neural nets still best for regression, and SVMs best for classification? Boosting maximizes margins similar to SVMs, but can boosting compete with SVMs? And if it does compete, is it better to boost weak models, as theory might suggest, or to boost stronger models? Bagging is simpler than boosting -- how well does bagging stack up against boosting? Breiman said Random Forests are better than bagging and as good as boosting. Was he right? And what about old friends like logistic regression, KNN, and naive bayes? Should they be relegated to the history books, or do they still fill important niches? In this talk we compare the performance of ten supervised learning methods on nine criteria: Accuracy, F-score, Lift, Precision/Recall Break-Even Point, Area under the ROC, Average Precision, Squared Error, Cross-Entropy, and Probability Calibration. The results show that no one learning method does it all, but some methods can be 'repaired' so that they do very well across all performance metrics. In particular, we show how to obtain the best probabilities from max margin methods such as SVMs and boosting via Platt's Method and isotonic regression. We then describe a new ensemble method that combines select models from these ten learning methods to yield much better performance. Although these ensembles perform extremely well, they are too complex for many applications. We'll describe what we're doing to try to fix that. Finally, if time permits, we'll discuss how the nine performance metrics relate to each other, and which of them you probably should (or shouldn't) use. During this talk I'll briefly describe the learning methods and performance metrics to help make the lecture accessible to non-specialists in machine learning.
Top/Computer_Science/Machine_Learning	Introduction to Support Vector Machines This first presentation introduces support vector machines.
Top/Computer_Science/Machine_Learning	Monte Carlo Simulation for Statistical Inference, Model Selection and Decision Making **The first part** of his course will consist of two presentations. In the first presentation, he will introduce fundamentals of Monte Carlo simulation for statistical inference, with emphasis on algorithms such as importance sampling, particle filtering and smoothing for dynamic models, Markov chain Monte Carlo, Gibbs and Metropolis-Hastings, blocking and mixtures of MCMC kernels, Monte Carlo EM, sequential Monte Carlo for static models, auxiliary variable methods (Swedsen-Wang, hybrid Monte Carlo and slice sampling), and adaptive MCMC. The algorithms will be illustrated with several examples: image tracking, robotics, image annotation, probabilistic graphical models, and music analysis. **The second presentation** will target model selection and decision making problems. He will describe the reversible-jump MCMC algorithm and illustrate it with application to simple mixture models and nonlinear regression with an unknown number of basis functions. He will show how to apply this algorithm to general Markov decision processes (MDPs). The course will also cover other Monte Carlo simulation methods for partially observed Markov decision processes (POMDPs) using policy gradients, common random number generation, and active exploration with Gaussian processes. An outline to some applications of these methods to robotics and the design of computer game architectures will be given. The presentation will end with the problem of Monte Carlo simulation for Bayesian nonlinear experimental design, with application to financial modeling, robot exploration, drug treatments, dynamic sensor networks, optimal measurement and active vision.
Top/Computer_Science/Machine_Learning	Plenary session-ECML poster highlights
Top/Computer_Science/Machine_Learning	Determining significance in neuroimaging studies using covariate-modulated false discovery rate
Top/Computer_Science/Machine_Learning	Learning, Information Extraction and the Web
Top/Computer_Science/Machine_Learning	Empirical Comparisons of Learning Methods & Case Studies Decision trees may be intelligible, but can they cut the mustard? Have SVMs replaced neural nets, or are neural nets still best for regression, and SVMs best for classification? Boosting maximizes a margin much like SVMs, but can boosting compete with SVMs? And is it better to boost weak models, as theory suggests, or to boost stronger models? Bagging is much easier than boosting, so how well does bagging stack up against boosting? Bagging is supposed to be best with low bias high variance methods like decision trees, so if we bag lower variance models like neural nets are they as good as bagged trees? What happens if we do bagging with steroids, i.e. switch to random forests? And what about old friends like k-nearest neighbor should they just be put out to pasture? In this lecture I'll compare the performance of a variety of popular machine learning methods on nine performance criteria: Accuracy, F-score, Lift, Precision/Recall Break-Even? Point, Area under the ROC, Average Precision, Squared Error, Cross-Entropy?, and Probabilistic Calibration. I'll show that while no one learning method does it all, it is possible to 'repair' some of them so that they do well on all metrics. I'll then describe NACHOS, a new ensemble method that does even better by by building on top of these other learning methods. Finally, I'll discuss how the nine performance metrics relate to each other, and look at a few case-studies to show why it is important to use the right metric for each problem.
Top/Computer_Science/Machine_Learning	Machine Learning Summer Schools
Top/Computer_Science/Machine_Learning	The two faces of ML.
Top/Computer_Science/Machine_Learning	Machine learning open source software
Top/Computer_Science/Machine_Learning	What Semantic Web researchers need to know about Machine Learning? The tutorial will cover basic topics from the field of Machine Learning explained in an intuitive way relevant for Semantic Web researchers and practitioners. In the first part the topics will cover brief top level overview of the Machine Learning field, its algorithms, and data types being analyzed. In the second part we will cover relation to Semantic Web and Web2.0. In the last part we will perform hands-on exercise with some of the tools for modeling text semantics and social networks in analytical way.
Top/Computer_Science/Machine_Learning	Machine Learning, Market Design, and Advertising Given the complexity of preferences in markets such as key word advertising it is hard to believe that the de facto standard, decentralized, local, greedy algorithm (advertisers bid for clicks on keywords) is any where close to being optimal for any reasonable objective (welfare, profit, etc.). In this talk we consider the market design problem from a global perspective. We make connections between machine learning theory and market design theory, where machine learning design problems closely mirror game theoretic design problems. We reduce a general theoretical market design problem to a natural machine learning optimization problem. These theoretical results lead to a number of practical answers to advertising market design questions.
Top/Computer_Science/Machine_Learning	Experiment Databases for Machine Learning Experiment Databases for Machine Learning is a large public repository of machine learning experiments as well as a framework for producing similar databases for specic goals. This projects aims to bring the infor- mation contained in many machine learning experiments together and organize it a way that allows everyone to investigate how learning algorithms have performed in previous studies. To share such information with the world, a common language is proposed, dubbed ExpML, capturing the basic structure of a large range of machine learning experiments while remaining open for future extensions. This language also enforces reproducibility by requiring links to the used datasets and algorithms and by storing all details of the ex- periment setup. All stored information can then be accessed by querying the database, creating a powerful way to collect and reorganize the data, thus warranting a very thorough examination of the stored results. The current publicly available database contains over 500,000 classication and regression experiments, and has both an online interface, at http://expdb.cs.kuleuven.be, as well as a stand-alone explorer tool oering various visualization techniques. This framework can also be integrated in machine learning toolboxes to automatically stream results to a global (or local) experiment database, or to download experiments that have been run before.
Top/Computer_Science/Machine_Learning	Should all Machine Learning be Bayesian? Should all Bayesian models be non-parametric? I'll present some thoughts and research directions in Bayesian machine learning. I'll contrast black-box approaches to machine learning with model-based Bayesian statistics. Can we meaningfully create Bayesian black-boxes? If so what should the prior be? Is non-parametrics the only way to go? Since we often can't control the effect of using approximate inference, are coherence arguments meaningless? How can we convert the pagan majority of ML researchers to Bayesianism? If the audience gets bored of these philosophical musings, I will switch to talking about our latest technical work on Indian buffet processes.
Top/Computer_Science/Machine_Learning	Some Challenging Machine Learning Problems in Computational Biology: Time-Varying Networks Inference and Sparse Structured Input-Out Learning Recent advances in high-throughput technologies such as microarrays and genome-wide sequencing have led to an avalanche of new biological data that are dynamic, noisy, heterogeneous, and high-dimensional. They have raised unprecedented challenges in machine learning and high-dimensional statistical analysis; and their close relevance to human health and social welfare has often created unique demands on performance metric different from standard data mining or pattern recognition problems. In this talk, I will discuss two of such problems. First, I will present a new statistical formalism for modeling network evolution over time, and several new algorithms based on temporal extensions of the sparse graphical logistic regression, for parsimonious reverse-engineering the latent time varying networks. I will show some promising results on recovering the latent sequence of temporally rewiring gene networks over more than 4000 genes during the life cycle of Drosophila melanogaster from microarray time course, at a time resolution only limited by sample frequency. Second, I will present a family of sparse structured regression models in the context of uncovering true associations between linked genetic variations (inputs) in the genome and networks of human traits (outputs) in the phenome. If time allows, I will also present another class of new models known as the maximum entropy discrimination Markov networks, which address the same problem in the maximum margin paradigm, but using a entropic regularizer that lead to a distribution of structured prediction functions that are simultaneously primal and dual sparse (i.e., with few support vectors, and of low effective feature dimension). Joint work with Amr Ahmed, Seyoung Kim, Mladen Kolar, Le Song and Jun Zhu.
Top/Computer_Science/Machine_Learning	VideoLectures.net case study The task in the VideoLectures case study is to develop a software component that will aid the VideoLectures editors in categorizing recorded lectures (i.e. ontology population). This functionality is required due to the rapid growth of the number of hosted lectures as well as due to the fact that the categorization taxonomy is rather fine-grained (200 categories and growing). In addition to aiding the categorization of new lectures, the software will also be used for re-categorization and additional categorization of lectures already categorized. We will show that we were successful in our task as the categorizer is highly accurate it achieves accuracies that stretch 1220% above the baseline and highly robust in terms of missing data. The latter means that a lecture might be missing textual annotations (such as the description and slide titles) but is still categorized correctly. Furthermore, the categorizer has been successfully integrated into the VideoLectures Web site. Categorization suggestions (termed 'quick links') are provided to the author in the categorization panel.
Top/Computer_Science/Machine_Learning/Kernel_Methods	Kernel Tricks, Means and Ends I will present my thoughts on what made kernel machines popular and what may or may not keep them going. I will also discuss applications in different domains, including computer graphics.
Top/Computer_Science/Machine_Learning/Kernel_Methods	Introduction to Kernel Methods
Top/Computer_Science/Machine_Learning/Kernel_Methods	Kernel Methods In this short course I will discuss exponential families, density estimation, and conditional estimators such as Gaussian Process classification, regression, and conditional random fields. The key point is that I will be providing a unified view of these estimation methods. In the second part I will discuss how moment matching techniques in Hilbert space can be used to design two-sample tests and independence tests in statistics. I will describe the basic principles and show how they can be used to correct covariate shift, select features, or merge databases.
Top/Computer_Science/Machine_Learning/Kernel_Methods	Kernel Methods in Computational Biology Many problems in computational biology and chemistry can be formalized as classical statistical problems, e.g., pattern recognition, regression or dimension reduction, with the caveat that the data are often not vectors. Indeed objects such as gene sequences, small molecules, protein 3D structures or phylogenetic trees, to name just a few, have particular structures which contain relevant information for the statistical problem but can hardly be encoded into finite-dimensional vector representations. Kernel methods are a class of algorithms well suited for such problems. Indeed they extend the applicability of many statistical methods initially designed for vectors to virtually any type of data, without the need for explicit vectorization of the data. The price to pay for this extension to non-vectors is the need to define a positive definite kernel between the objects, formally equivalent to an implicit vectorization of the data.
Top/Computer_Science/Machine_Learning/Kernel_Methods	The Pyramid Match Kernel: Efficient Learning with Sets of Features
Top/Computer_Science/Machine_Learning/Kernel_Methods	Kernels in Bioinformatics
Top/Computer_Science/Machine_Learning/Kernel_Methods	Semi-supervised Graph Clustering: A Kernel Approach
Top/Computer_Science/Machine_Learning/Kernel_Methods	Introduction to Kernel Methods
Top/Computer_Science/Machine_Learning/Kernel_Methods	Kernel Methods in Statistical Learning
Top/Computer_Science/Machine_Learning/Kernel_Methods	Unsupervised Learning with Kernels
Top/Computer_Science/Machine_Learning/Kernel_Methods	Introduction to Kernel Methods The course will cover the basics of Support Vector Machines and related kerne methods: 1. Kernels and Feature Spaces 2. Large Margin Classification 3. Basic Ideas of Learning Theory 4. Support Vector Machines 5. Examples of Other Kernel Algorithms
Top/Computer_Science/Machine_Learning/Kernel_Methods	Adaptive Modelling via Pattern Analysis and the Kernel Methods approach There is a dramatic growth in the availability of complex data from a wide range of different applications. The challenge of the data analyzer is to extract knowledge from the raw data by identifying the useful patterns and structures that underlie it. This module introduces adaptive and probabilistic approaches to modeling such complex data. We first consider finding structure in high-dimensional data. The kernel methods approach to identifying non-linear patterns in introduced while addressing the issues of statistical reliability of inferences made from limited data. Subspace identification is considered and correlations across different data modalities are shown to provide a useful approach to eliciting semantic representations. The final section of the course will introduce learning probabilistic models, (e.g. in biological sequence data), fusing prior knowledge and data, complex and approximate inference.
Top/Computer_Science/Machine_Learning/Kernel_Methods	Bayesian Kernel Methods
Top/Computer_Science/Machine_Learning/Kernel_Methods	Invariance in kernel methods - distance and integration kernels
Top/Computer_Science/Machine_Learning/Kernel_Methods	Kernel Methods
Top/Computer_Science/Machine_Learning/Kernel_Methods	Kernel Methods for Higher Order Image Statistics The conditions under which natural vision systems evolved show statistical regularities determined both by the environment and by the actions of the organism. Many aspects of biological vision can be understood as evolutionary adaptations to these regularities. This is demonstrated by the recent sucess in explaining properties of retinal and cortical neurons from the statistics of natural images. At the same time, we observe an increasing interest in statistical modeling techniques in the computer vision community. Here, the motivation comes from the need for powerful image models in image processing tasks such as super-resolution or denoising. In the literature, the statistical analysis of natural images has mainly been done with linear techniques such as Principal Component Analysis (PCA) or Fourier analysis. These techniques capture only the second-order statistics of an image ensemble. A large part of the interesting image structure, however, is contained in the higher-order statistics. Unfortunately, the estimation of these statistics involves a huge number of terms which makes their explicit computation for images infeasible in practice. Kernel methods provide an implicit access to higher-order statistics that avoids this combinatorial explosion. In the course, we start with an overview of existing approaches to image statistics. The need to go beyond the usual linear, second-order techniques will lead us to the classical higher-order statistics such as Wiener series, higher-order cumulants and spectra. We will see that the exponential number of terms involved in these statistics prevents them from being applied to images. This motivates the introduction of kernel techniques. Here, we will discuss two approaches: 1. The Wiener series can be estimated implicitly via polynomial kernel regression. We will use this technique to decompose an image into components that are characterized by pixel interactions of a given order. 2. Kernel PCA of image patches provides a powerful image model that takes higher-order statistics into account. We will show applications of this model to various image processing tasks.
Top/Computer_Science/Machine_Learning/Kernel_Methods	Kernel Methods in Statistical Learning
Top/Computer_Science/Machine_Learning/Kernel_Methods	Learning multiple tasks with kernel methods
Top/Computer_Science/Machine_Learning/Kernel_Methods	Optimization for Kernel Methods Optimization methods play a crucial role in kernel methods such as Support Vector Machines and Kernel Logistic Regression. In a variety of scenarios, different optimization algorithms are better suited than others. The aims of the six lectures in this topic are: (1) to introduce a range of optimization problems that arise in the solution of classification problems by kernel methods; (2) to briefly review the relevant optimization algorithms; and (3) to point out in some detail as to which optimization methods are suited for these varied problems.
Top/Computer_Science/Machine_Learning/Kernel_Methods	Regularization of Kernel Methods by Decreasing the Bandwidth of the Gaussian Kernel
Top/Computer_Science/Machine_Learning/Kernel_Methods	Support Vector and Kernel Methods The lectures will introduce the kernel methods approach to pattern analysis through the particular example of support vector machines for classification. The presentation touches on: generalization, optimization, dual representation, kernel design and algorithmic implementations. We then broaden the discussion to consider general kernel methods by introducing different kernels, different learning tasks, and subspace methods such as kernel PCA. The aim is to give a view of the subject that will enable a newcomer to the field to gain his bearings so that they can move to apply or develop the techniques for their particular application.
Top/Computer_Science/Machine_Learning/Kernel_Methods	Conformal Multi-Instance Kernels In the multiple instance learning setting, each observation is a bag of feature vectors of which one or more vectors indicates membership in a class. The primary task is to identify if any vectors in the bag indicate class membership while ignoring vectors that do not. We describe here a kernel-based technique that defines a parametric family of kernels via conformal transformations and jointly learns a discriminant function over bags together with the optimal parameter settings of the kernel. Learning a conformal transformation effectively amounts to weighting regions in the feature space according to their contribution to classification accuracy; regions that are discriminative will be weighted higher than regions that are not. This allows the classifier to focus on regions contributing to classification accuracy while ignoring regions that correspond to vectors found both in positive and in negative bags. We show how parameters of this transformation can be learned for support vector machines by posing the problem as a multiple kernel learning problem. The resulting multiple instance classifier gives competitive accuracy for several multi-instance benchmark datasets from different domains.
Top/Computer_Science/Machine_Learning/Kernel_Methods	Kernels on histograms through the transportation polytope For two integral histograms and of equal sum, the Monge-Kantorovich distance MK(r,c) between r and c parameterized by a d d cost matrix T is the minimum of all costs <F,T> taken over matrices F of the transportation polytope U(r,c). Recent results suggest that this distance is not negative definite, and hence, through Schoenberg's well-known result, may not be a positive definite kernel for all t > 0. Rather than using directly MK to define a similarity between r and c, we present in this talk kernels on r and c based on the whole transportation polytope U(r,c). We prove that when r and c have binary counts, which is equivalent to stating that r and c depict clouds of points of equal size, the permanent of their Gram matrix induced by the cost matrix T is a positive definite kernel under favorable conditions on T. We also show that the volume of the polytope U(r,c), that is the number of integral transportation plans, is a positive definite quantity in r and c through the Robinson-Schensted-Knuth correspondence between transportation matrices and Young Tableaux.
Top/Computer_Science/Machine_Learning/Kernel_Methods	Learning and Regularization Using non Positive Kernels
Top/Computer_Science/Machine_Learning/Kernel_Methods	Learning from Network Traffic: Computing Kernels over Connection Content
Top/Computer_Science/Machine_Learning/Kernel_Methods	Learning RoboCup-Keepaway with Kernels We give another success story of using kernel-based methods to solve a dificult reinforcement learning problem, namely that of 3vs2 keepaway in RoboCup simulated soccer. Key challenges in keepaway are the high-dimensionality of the state space (rendering conventional grid-based function approximation like tilecoding infeasable) and the stochasticity due to noise and multiple learning agents needing to co- operate. We use approximate policy iteration with sparsified regular- ization networks to carry out policy evaluation. Preliminary results indicate that the behavior learned through our approach clearly out- performs the best results obtained with tilecoding by Stone et al.
Top/Computer_Science/Machine_Learning/Kernel_Methods	Learning with Kernels The Course on Learning with Kernels covers * Elements of Statistical Learning Theory * Kernels and feature spaces * Support vector algorithms and other kernel methods * Applications
Top/Computer_Science/Machine_Learning/Kernel_Methods	Online Learning with Kernels Online learning is concerned with the task of making decisions on-the-fly as observations are received. We describe and analyze several online learning tasks through the same algorithmic prism. We start with online binary classification and show how to build simple yet efficient and effective online algorithms that incorporate kernel functions. We describe how to analyze the algorithms in the mistake bound model for both separable and inseparable settings. We then describe numerous generalizations of online learning with kernels to other, often more complex, problems. Specifically, we discuss learning algorithms for uniclass prediction, regression, multiclass problems, and sequence prediction. We conclude with discussion on implications to batch learning and generalization. Based on joint works with Koby Crammer, Ofer Dekel, Vineet Gupta, Joseph Keshet, Andrew Ng, Shai Shalev-Shwartz?, Lavi Shpigelman.
Top/Computer_Science/Machine_Learning/Kernel_Methods	Random projection, margins, kernels, and feature-selection Random projection is a simple technique that can often provide insight into questions such as 'why is it good to have a large margin?' or 'what are kernels really doing and how are they similar to feature selection?' In this talk I will describe some simple learning algorithms using random projection. I will then discuss how, given a kernel as a black-box function, we can use various forms of random projection to extract an explicit small feature space that captures much of the power of the given kernel function.
Top/Computer_Science/Machine_Learning/Kernel_Methods	Statistical Translation, Heat Kernels, and Expected Distances High dimensional structured data such as text and images is often poorly understood and misrepresented in statistical modeling. The standard histogram representation suffers from high variance and performs poorly in general. We explore novel connections between statistical translation, heat kernels on manifolds and graphs, and expected distances. These connections provide a new framework for unsupervised metric learning for text documents. Experiments indicate that the resulting distances are generally superior to their more standard counterparts.
Top/Computer_Science/Machine_Learning/Kernel_Methods	Using Belief Networks and Fisher Kernels for Structured Document Classification
Top/Computer_Science/Machine_Learning/Kernel_Methods	Learning from Interpretations: A Rooted Kernel for Ordered Hypergraphs The paper presents a kernel for learning from ordered hypergraphs, a formalization that captures relational data as used in Inductive Logic Programming (ILP). The kernel generalizes previous approaches to graph kernels in calculating similarity based on walks in the hypergraph. Experiments on challenging chemical datasets demonstrate that the kernel outperforms existing ILP methods, and is competitive with state-of-the-art graph kernels. The experiments also demonstrate that the encoding of graph data can affect performance dramatically, a fact that can be useful beyond kernel methods.
Top/Computer_Science/Machine_Learning/Kernel_Methods	Measures of Statistical Dependence A number of important problems in signal processing depend on measures of statistical dependence. For instance, this dependence is minimised in the context of instantaneous ICA, in which linearly mixed signals are separated using their (assumed) pairwise independence from each other. A number of methods have been proposed to measure this dependence, however they generally assume a particular parametric model for the densities generating the observations. Recent work suggests that kernel methods may be used to find estimates that adapt according to the signals they compare. These methods are currently being refined, both to yeild greater accuracy, and to permit the use of the signal properties over time in improving signal separability. In addition, these methods can be applied in cases where the statistical dependence between observations must be maximised, which is true for certain classes of clustering algorithms.
Top/Computer_Science/Machine_Learning/Kernel_Methods	Kernels and Gaussian Processes
Top/Computer_Science/Machine_Learning/Kernel_Methods	Theory and Applications of Kernel Space ;Basics of kernel definitions and theory are first given. Then 3 algorithms are described with an explicit reference to the representer theorem: : Support vector Mahcines, : Support Vector Regression and : Kernel Principal Components Analysis. The last course is devoted to examples of kernel design (Mahalanobis kernles and Fisher kernels)
Top/Computer_Science/Machine_Learning/Kernel_Methods	Output kernel tree In this paper, we generalize tree-based methods to the prediction in structured output space. The extension is based on a kernelization of the algorithm that allows one to grow trees as soon as a kernel can be defined on the output space. The resulting algorithm, called output kernel trees (OK3), generalizes classification and regression trees in a principled way.
Top/Computer_Science/Machine_Learning/Kernel_Methods	Introduction to kernel methods This lecture given by Mr. Smola is combined with Mr. Bernhard Schlkopf and will encopass Part 1, Part 5, Part 6 of the complete lecture. Part 2, 3 and 4 of this lecture can be found here [[mlss07_scholkopf_intkmet|//at Bernhard Schlkopf's// '%title']]
Top/Computer_Science/Machine_Learning/Kernel_Methods	Introduction to kernel methods This lecture given by Mr. Bernhard Schlkop is combined with Mr. Smola and will encompass Part 2, Part 3, Part 4 of the complete lecture. Part 1 , 5, 6 of this lecture can be found here at [[mlss07_smola_intkmet|//Alex Smola's// '%title']]
Top/Computer_Science/Machine_Learning/Kernel_Methods	Kernel Methods for Dependence and Causality
Top/Computer_Science/Machine_Learning/Kernel_Methods	Learning and Charting Chemical Space with Strings and Graphs: Challenges and Opportunities for AI and Machine Learning Informatics methods and computers have not yet become as pervasive in chemistry as they have in physics and biology. Drawing analogies from bioinformatics, key ingredients for progress in chemoinformatics are the availability of large, annotated databases of compounds and reactions, data structures and algorithms to efficiently search these databases, and computational methods to predict the physical, chemical, and biological properties of new compounds and reactions. We will describe how graph-based methods play a key role in the development of: (1) a large public database of compounds and reactions (ChemDB) and the underlying algorithms and representations; (2) machine learning kernel methods to predict molecular properties; and (3) the applications of these methods to drug screening/design problems and the identification of new drug leads against a major disease.
Top/Computer_Science/Machine_Learning/Kernel_Methods	A theory of similarity functions for learning and clustering Kernel methods have proven to be very powerful tools in machine learning. In addition, there is a well-developed theory of sufficient conditions for a kernel to be useful for a given learning problem. However, while a kernel function can be thought of as just a pairwise similarity function that satisfies additional mathematical properties, this theory requires viewing kernels as implicit (and often difficult to characterize) maps into high-dimensional spaces. In this talk I will describe a more general theory that applies to more general similarity functions (not just legal kernels) and furthermore describes the usefulness of a given similarity function in terms of more intuitive, direct properties of the induced weighted graph. An interesting feature of the proposed framework is that it can also be applied to learning from purely unlabeled data, i.e., clustering. In particular, one can ask how much stronger the properties of a similarity function should be (in terms of its relation to the unknown desired clustering) so that it can be used to *cluster* well. Investigating this question leads to a number of interesting graph-theoretic properties, and their analysis in the inductive setting uses regularity-lemma type results of [FK99,AFKK03]. This work is joint with Maria-Florina Balcan and Santosh Vempala.
Top/Computer_Science/Machine_Learning/Kernel_Methods	Random walk graph kernels and rational kernels Random walk graph kernels (Gartner et al., 2003 [5]; Borgwardt et al., 2005 [1]) count matching random walks, and are defined using the tensor product graph. Loosely speaking, rational kernels (Cortes et al., 2004, 2003, 2002 [4, 3, 2]) use the weight assigned by a transducer to define a kernel. The kernel is shown to be positive semi-definite when the transducer can be written as a composition of two identical transducers. In our talk we will establish explicit connections between random walk graph kernels and rational kernels. More concretely, we show that composition of transducers is analogous to computing product graphs, and that rational kernels on weighted transducers may be viewed as generalizations of random walk kernels to weighted automata. In order to make these connections explicit we adapt slightly non-standard notation for weighted transducers, extensively using matrices and tensors wherever possible. We prove that under certain conditions rational kernels are positive semi-definite. Our proof only uses basic linear algebra and is simpler than the one presented in Cortes et al., 2004[4].
Top/Computer_Science/Machine_Learning/Kernel_Methods	ML in Bioinformatics After a brief introduction to the use of machine learning in computational biology, we focus on the problem of biological networks inference. We define the problem as a problem of kernel learning using prediction in kernelized output spaces. Methods based on Output kernel Tree are presented to solve the problem. Results on two benchmarks are shown.
Top/Computer_Science/Machine_Learning/Kernel_Methods	Large-scale RLSC Learning Without Agony The advances in kernel-based learning necessitate the study on solving a large-scale non-sparse positive definite linear system. To provide a deterministic approach, recent researches focus on designing fast matrixvector multiplication techniques coupled with a conjugate gradient method. Instead of using the conjugate gradient method, our paper proposes to use a domain decomposition approach in solving such a linear system. Its convergence property and speed can be understood within von Neumann's alternating pro jection framework. We will report significant and consistent improvements in convergence speed over the conjugate gradient method when the approach is applied to recent machine learning problems.
Top/Computer_Science/Machine_Learning/Kernel_Methods	Graph kernels and applications in chemoinformatics Several problems in chemistry can be formulated as classification or regression problems over molecules which, when represented by their planar structure, can be seen as labeled graphs. Several approaches have been proposed recently to define positive definite kernels over labeled graphs, paving the way to the use of powerful kernel methods in chemoinformatics. In this talk I will review some of these approaches and present relevant applications in computational chemistry.
Top/Computer_Science/Machine_Learning/Kernel_Methods	Completion of biological networks : the output kernel trees approach Elucidating biological networks appears nowadays as one of the most important challenge in systems biology. Due to the availability of various sources of data, machine learning has to play a major role regarding this issue, given its large spectrum of tools ranging from generative models to concept learning methods. In this work the focus is narrowed on the completion of biological interactions networks for which some of the interactions between variables (usually genes or proteins) are already known.
Top/Computer_Science/Machine_Learning/Kernel_Methods	An SMO-like algorithm for Kernel Conditional Random Fields
Top/Computer_Science/Machine_Learning/Kernel_Methods	A Universal Kernel for Learning Regular Languages We give a universal kernel that renders all the regular languages linearly separable. We are not able to compute this kernel efficiently and conjecture that it is intractable, but we do have an efficient -approximation
Top/Computer_Science/Machine_Learning/Kernel_Methods	Color Image Segmentation: Kernel Do the Feature Space
Top/Computer_Science/Machine_Learning/Kernel_Methods	The use of machine translation tools for cross-lingual text-mining
Top/Computer_Science/Machine_Learning/Kernel_Methods	Molecular Graph Kernels for Drug Discovery
Top/Computer_Science/Machine_Learning/Kernel_Methods	Fast Clustering based on Kernel Density Estimation The Denclue algorithm employs a cluster model based on kernel density estimation. A cluster is defined by a local maximum of the estimated density function. Data points are assigned to clusters by hill climbing, i.e. points going to the same local maximum are put into the same cluster. A disadvantage of Denclue 1.0 is, that the used hill climbing may make unnecessary small steps in the beginning and never converges exactly to the maximum, it just comes close. We introduce a new hill climbing procedure for Gaussian kernels, which adjusts the step size automatically at no extra costs. We prove that the procedure converges exactly towards a local maximum by reducing it to a special case of the expectation maximization algorithm. We show experimentally that the new procedure needs much less iterations and can be accelerated by sampling based methods with sacrificing only a small amount of accuracy.
Top/Computer_Science/Machine_Learning/Kernel_Methods	Kernel Based Methods This second presentation covers more general kernel methods including training, model selection and practical aspects.
Top/Computer_Science/Machine_Learning/Kernel_Methods	Kernel methods and Support Vector Machines The tutorial will introduce the main ideas of statistical learning theory, support vector machines, and kernel feature spaces. This includes a derivation of the support vector optimization problem for classification and regression, the v-trick, various kernels and an overview over applications of kernel methods.
Top/Computer_Science/Machine_Learning/Kernel_Methods	Support Vector Machines and Kernel Methods
Top/Computer_Science/Machine_Learning/Kernel_Methods	The Sample Complexity of Learning the Kernel The success of kernel based learning algorithms depends upon the suitability of the kernel to the learning task. Ideally, the choice of a kernel should based on prior information of the learner about the task at hand. However, in practice, kernel parameters are being tuned based on available training data. I will discuss the sample complexity overhead associated with such learning the kernel scenarios. I will address the setting in which the training data for the kernel selection is target labeled examples, as well as settings in which this training is based on dierent types of data, such as unlabeled examples and examples labeled by a dierent (but related) tasks. Part of this work is joint with Nati Srebro.
Top/Computer_Science/Machine_Learning/Kernel_Methods/Support_Vector_Machines	Integrating two features or kernels within one SVM classifier
Top/Computer_Science/Machine_Learning/Kernel_Methods/Support_Vector_Machines	Support Vector Machines and Kernels
Top/Computer_Science/Machine_Learning/Kernel_Methods/Support_Vector_Machines	Support vector machines loss with l1 penalty We consider an i.i.d. sample from (X,Y), where X is a feature and Y a binary label, say with values +1 or -1. We use a high-dimensional linear approximation of the regression of Y on X and support vector machine loss with l1 penalty on the regression coefficients. This procedure does not depend on the (unknown) noise level or on the (unknown) sparseness of approximations of Bayes rule, but nevertheless its prediction error is smaller for smaller noise levels and/or sparser approximations. Thus, it adapts to unknown properties of the underlying distribution. In an example, we show that up to terms logarithmic in the sample size, the procedure yields minimax rates for the excess risk.
Top/Computer_Science/Machine_Learning/Kernel_Methods/Support_Vector_Machines	Large Scale Genomic Sequence Support Vector Machines
Top/Computer_Science/Machine_Learning/Kernel_Methods/Support_Vector_Machines	Large-scale parallel implementations of SVMs
Top/Computer_Science/Machine_Learning/Kernel_Methods/Support_Vector_Machines	Working Set Selection Using the Second Order Information for SVMs
Top/Computer_Science/Machine_Learning/Kernel_Methods/Support_Vector_Machines	Object categorization with SVM: kernels for local features We will focus on object categorization. The basic idea is to combine the nice invariance propreties of local features with the robustness of SVM's and the ability to control generalization in this framework.
Top/Computer_Science/Machine_Learning/Kernel_Methods/Support_Vector_Machines	Fast SVM Approximations for Object Detection state-of-the-art accuracies in object detection. However, for real time applications, standard SVMs are usually too slow. In this work, we propose a method for approximating an SVM detector in terms of a small number of separable nonlinear filters. We are building on work of Romdhani et al. (ICCV 2001), where an SVM face detector was approximated using the so-called reduced set algorithm and evaluated in a cascade. However, when using plain gray values as features, we found it more effective to reduce the high computational cost for the pixel-wise comparisons, rather than focusing on sparsity of the detectors alone. In our approach, we constrain the reduced set optimization to a class of nonlinear convolution filters which can be evaluated more efficiently (i.e. O(w+h) instead of O(wh), where w and h are the patch dimensions, respectively). We demonstrate a prototype of our system which runs in real time on a standard PC.
Top/Computer_Science/Machine_Learning/Kernel_Methods/Support_Vector_Machines	Some Aspects of Learning Rates for SVMs We present some learning rates for support vector machine classification. In particular we discuss a recently proposed geometric noise assumption which allows to bound the approximation error for Gaussian RKHSs. Furthermore we show how a noise assumption proposed by Tsybakov can be used to obtain learning rates between 1/sqrt(n) and 1/n. Finally, we describe the influence of the approximation error on the overall learning rate.
Top/Computer_Science/Machine_Learning/Kernel_Methods/Support_Vector_Machines	Support Vector Machines Support vector machines (SVM) and kernel methods are important machine learning techniques. In this short course, we will introduce their basic concepts. We then focus on the training and optimization procedures of SVM. Examples demonstrating the practical use of SVM will also be discussed. Basically we focus on classification. If time is allowed, we will also touch SVM regression.
Top/Computer_Science/Machine_Learning/Kernel_Methods/Support_Vector_Machines	Supervised Clustering with Support Vector Machines Supervised clustering is the problem of training a clustering algorithm to produce desirable clusterings: given sets of items and complete clusterings over these sets, we learn how to cluster future sets of items. Example applications include noun-phrase coreference clustering, and clustering news articles by whether they refer to the same topic. In this paper we present an SVM algorithm that trains a clustering algorithm by adapting the item-pair similarity measure. The algorithm may optimize a variety of different clustering functions to a variety of clustering performance measures. We empirically evaluate the algorithm for noun-phrase and news article clustering.
Top/Computer_Science/Machine_Learning/Kernel_Methods/Support_Vector_Machines	Analysis of Support Vector Machine Classification
Top/Computer_Science/Machine_Learning/Kernel_Methods/Support_Vector_Machines	A Support Vector Method for Multivariate Performance Measures We examine the relationship between the predictions made by different learning algorithms and true posterior probabilities. We show that maximum margin methods such as boosted trees and boosted stumps push probability mass away from 0 and 1 yielding a characteristic sigmoid shaped distortion in the predicted probabilities. Models such as Naive Bayes, which make unrealistic independence assumptions, push probabilities toward 0 and 1. Other models such as neural nets and bagged trees do not have these biases and predict well calibrated probabilities. We experiment with two ways of correcting the biased probabilities predicted by some learning methods: Platt Scaling and Isotonic Regression. We qualitatively examine what kinds of distortions these calibration methods are suitable for and quantitatively examine how much data they need to be effective. The empirical results show that after calibration boosted trees, random forests, and SVMs predict the best probabilities.
Top/Computer_Science/Machine_Learning/Kernel_Methods/Support_Vector_Machines	Automatic Dominance Detection In Meetings Using Support Vector Machines We show that, using a Support Vector Machine classifier, it is possible to determine with a 75% success rate who dominated a particular meeting on the basis of a few basic features. We discuss the corpus we have used, the way we had people judge dominance and the details of the classifier and features that were used.
Top/Computer_Science/Machine_Learning/Kernel_Methods/Support_Vector_Machines	Data analysis and support vector machines in recognition of sleep stages
Top/Computer_Science/Machine_Learning/Kernel_Methods/Support_Vector_Machines	Fast Learning Rates for Support Vector Machines We establish learning rates to the Bayes risk for support vector machines with hinge loss (L1-SVM's). Since a theorem of Devroye states that no learning algorithm can learn with a uniform rate to the Bayes risk for all probability distributions we have to restrict the class of considered distributions: in order to obtain fast rates we assume a noise condition recently proposed by Tsybakov and an approximation condition in terms of the distribution and the reproducing kernel Hilbert space used by the L1-SVM. For Gaussian RBF kernels with varying widths we propose a geometric noise assumption on the distribution which ensures the approximation condition. This geometric assumption is not in terms of smoothness but describes the concentration of the marginal distribution near the decision boundary. In particular we are able to describe nontrivial classes of distributions for which L1-SVM's using a Gaussian kernel can learn with almost linear rate.
Top/Computer_Science/Machine_Learning/Kernel_Methods/Support_Vector_Machines	How to Teach Support Vector Machine to Learn Vector Outputs
Top/Computer_Science/Machine_Learning/Kernel_Methods/Support_Vector_Machines	Learning structured data via flow represented actions of support vector machines
Top/Computer_Science/Machine_Learning/Kernel_Methods/Support_Vector_Machines	Optimal Support Vector Selection for Kernel Perceptrons
Top/Computer_Science/Machine_Learning/Kernel_Methods/Support_Vector_Machines	Robustness properties of support vector machines and related methods The talk brings together methods from two disciplines: machine learning theory and robust statistics. We argue that robustness is an important aspect and we show that many existing machine learning methods based on convex risk minimization have - besides other good properties - also the advantage of being robust if the kernel and the loss function are chosen appropriately. Our results cover classification and regression problems. Assumptions are given for the existence of the influence function and for bounds on the influence function. Kernel logistic regression, support vector machines, least squares and the AdaBoost loss function are treated as special cases. We also consider Robust Learning from Bites, a simple method to make some methods from convex risk minimization applicable for huge data sets for which currently available algorithms are much to slow. As an example we use a data set from 15 German insurance companies.
Top/Computer_Science/Machine_Learning/Kernel_Methods/Support_Vector_Machines	Support Vector and Kernel Methods The lectures will introduce the kernel methods approach to pattern analysis through the particular example of support vector machines for classification. The presentation touches on: generalization, optimization, dual representation, kernel design and algorithmic implementations. We then broaden the discussion to consider general kernel methods by introducing different kernels, different learning tasks, and subspace methods such as kernel PCA. The aim is to give a view of the subject that will enable a newcomer to the field to gain his bearings so that they can move to apply or develop the techniques for their particular application.
Top/Computer_Science/Machine_Learning/Kernel_Methods/Support_Vector_Machines	Context changes detection by one-class svms For a system that aims at taking into account the user, we need to consider that there are many different behaviors as well as many different users. Hence we need adaptative, unsupervised (or semi-supervised) learning methods. Our idea is to take advantage of wearable computers and wearable sensors (indeed their use is realistic at least for certain categories of people, such as pilots) to retrieve the current context of the user. Wearable sensors can be physiological (EMG, ECG, blood volume pressure...) or physical (accelerometers, microphone...). Contexts are depending on the application using the system and can be behaviors, affective states, combinations of these. Since this problem of context retrieval is very complex, we choose to detect changes at first place instead of labeling directly. Indeed this way we can apply unsupervised and fast methods which saves time for labeling (the labeling task is then applied only when changes are detected). Our interest lies in low level treatments and we present a non parametric change detection algorithm. This algorithm is meant to provide sequences of unlabeled contexts to be analyzed to higher level applications. Detection is made from signals given by non invasive sensors the user is wearing. Note that the methods presented here could as well be adapted to external sensors.
Top/Computer_Science/Machine_Learning/Kernel_Methods/Support_Vector_Machines	Implementing SVM in an RDBMS: Improved Scalability and Usability
Top/Computer_Science/Machine_Learning/Kernel_Methods/Support_Vector_Machines	Improving SVM Text Classification Performance through Threshold Adjustment
Top/Computer_Science/Machine_Learning/Kernel_Methods/Support_Vector_Machines	Learning interpretable SVMs for biological sequence classification
Top/Computer_Science/Machine_Learning/Kernel_Methods/Support_Vector_Machines	Linear SVM Classification of Visual Objects using a Dense Image Representation
Top/Computer_Science/Machine_Learning/Kernel_Methods/Support_Vector_Machines	Mixture of SVMs for Face Class Modeling We present a method for face detection which uses a new {SVM} structure trained in an expert manner in the eigenface space. This robust method has been introduced as a post processing step in a real-time face detection system. The principle is to train several parallel {SVMs} on subsets of some initial training set and then train a second layer {SVM} on the margins of the first layer of {SVMa}. This approach presents a number of advantages over the classical {SVM}: firstly the training time is considerably reduced and secondly the classification performance is improved, we will present some comparisions with the single {SVM} approach for the case of human face class modeling.
Top/Computer_Science/Machine_Learning/Kernel_Methods/Support_Vector_Machines	Multi-Classification by Using Tri-class SVM
Top/Computer_Science/Machine_Learning/Kernel_Methods/Support_Vector_Machines	Textual Entailment as Syntactic Graph Distance: a rule based and a SVM based approach
Top/Computer_Science/Machine_Learning/Kernel_Methods/Support_Vector_Machines	The Limit of One-Class SVM In this talk, I will present an analysis of the asymptotic behaviour of the One-Class support vector machine (SVM), a popular algorithm for outlier detection. I will show that One-Class SVM asymptotically estimates a truncated version of the density of the distribution generating the data, in the case where the Gaussian kernel is used with a well-calibrated decreasing bandwidth parameter, and the regularization parameter involved in the algorithm is held fixed as the training sample size goes to infinity.A long version of this work can be found at www.lri.fr/~vert/Publi/regularizeGaussianKernel.ps , in which extensions to the 2-class case and to more general convex loss functions are considered.
Top/Computer_Science/Machine_Learning/Kernel_Methods/Support_Vector_Machines	Tricks of the trade for training SVMs
Top/Computer_Science/Machine_Learning/Kernel_Methods/Support_Vector_Machines	Learning with Kernels The course will cover the basics of Support Vector Machines and related kernel methods. # Kernel and Feature Spaces # Large Margin Classification # Basic Ideas of Learning Theory # Support Vector Machines # Other Kernel Algorithms
Top/Computer_Science/Machine_Learning/Kernel_Methods/Support_Vector_Machines	Optimal Dimensionality of Metric Space for Classification For large-scale classification problems, the training samples can be clustered beforehand as a downsampling pre-process, and then only the obtained clusters are used for training. Motivated by such assumption, we proposed a classification algorithm, Support Cluster Machine (SCM), within the learning framework introduced by Vapnik. For the SCM, a compatible kernel is adopted such that a similarity measure can be handled not only between clusters in the training phase but also between a cluster and a vector in the testing phase. We also proved that the SCM is a general extension of the SVM with the RBF kernel. The experimental results confirm that the SCM is very effective for largescale classification problems due to significantly reduced computational costs for both training and testing and comparable classification accuracies. As a by-product, it provides a promising approach to dealing with privacy-preserving data mining problems.
Top/Computer_Science/Machine_Learning/Kernel_Methods/Support_Vector_Machines	Support Feature Machine for Classification of Abnormal Brain Activity In this study, a novel multidimensional time series classification technique, namely support feature machine (SFM), is proposed. SFM is inspired by the optimization model of support vector machine and the nearest neighbor rule to incorporate both spatial and temporal of the multi-dimensional time series data. This paper also describes an application of SFM for detecting abnormal brain activity. Epilepsy is a case in point in this study. In epilepsy studies, electroencephalograms (EEGs), acquired in multidimensional time series format, have been traditionally used as a gold-standard tool for capturing the electrical changes in the brain. From multi-dimensional EEG time series data, SFM was used to identify seizure pre-cursors and detect seizure susceptibility (pre-seizure) periods. The empirical results showed that SFM achieved over 80% correct classification of per-seizure EEG on average in 10 patients using 5-fold cross validation. The proposed optimization model of SFM is very compact and scalable, and can be implemented as an online algorithm. The outcome of this study suggests that it is possible to construct a computerized algorithm used to detect seizure pre-cursors and warn of impending seizures through EEG classification.
Top/Computer_Science/Machine_Learning/Kernel_Methods/Support_Vector_Machines	Who is Afraid of Non-Convex Loss Functions? The NIPS community has suffered of an acute convexivitis epidemic: - ML applications seem to have trouble moving beyond logistic regression, SVMs, and exponential-family graphical models; - For a new ML model, convexity is viewed as a virtue; - Convexity is sometimes a virtue; - But it is often a limitation. ML theory has essentially never moved beyond convex models - the same way control theory has not really moved beyond linear systems.
Top/Computer_Science/Machine_Learning/Kernel_Methods/Support_Vector_Machines	Theory and Applications of Kernel Space ;Basics of kernel definitions and theory are first given. Then 3 algorithms are described with an explicit reference to the representer theorem: : Support vector Mahcines, : Support Vector Regression and : Kernel Principal Components Analysis. The last course is devoted to examples of kernel design (Mahalanobis kernles and Fisher kernels)
Top/Computer_Science/Machine_Learning/Kernel_Methods/Support_Vector_Machines	Introduction to Support Vector Machines This first presentation introduces support vector machines.
Top/Computer_Science/Machine_Learning/Kernel_Methods/Support_Vector_Machines	Fast Support Vector Machine Training and Classification on Graphics Processors Recent developments in programmable, highly parallel Graphics Processing Units (GPUs) have enabled high performance implementations of machine learning algorithms. We describe a solver for Support Vector Machine training, using Platt's Sequential Minimal Optimization algorithm and an adaptive first and second order working set selection heuristic, which achieves speedups of 9-35x over LIBSVM running on a traditional processor. We also present a GPU-based system for SVM classification which achieves speedups of 81-138x over LibSVM (5-24x over our own CPU-based SVM classifier).
Top/Computer_Science/Machine_Learning/Kernel_Methods/Support_Vector_Machines	Predicting Diverse Subsets Using Structural SVMs In many retrieval tasks, one important goal involves retrieving a diverse set of results (e.g., documents covering a wide range of topics for a search query). First of all, this reduces redundancy, effectively presenting more information with the presented results. Secondly, search queries are often ambiguous at some level. For example, the query Jaguar can refer to many different topics (such as the car or the feline). A set of documents with high topic diversity ensures that fewer users abandon the query because none of the results are relevant to them. Unlike existing approaches to learning retrieval functions, we present a method that explicitly trains to diversify results. In particular, we formulate the learning problem of predicting a diverse subset and derive a training algorithm based on structural SVMs.
Top/Computer_Science/Machine_Learning/Kernel_Methods/Support_Vector_Machines	Structured Output Prediction with Structural SVMs This talk explores large-margin approaches to predicting graph-based objects like trees, clusterings, or alignments. Such problems arise, for example, when a natural language parser needs to predict the correct parse tree for a given sentence, when one needs to determine the co-reference relationships of noun-phrases in a document, or when predicting the alignment between two proteins. In particular, the talk will show how structural SVMs can learn such complex prediction rules, using the problems of supervised clustering, protein sequence alignment, and diversification in search engines as application examples. Furthermore, the talk will present new cutting-plane algorithms that allows training of structural SVMs in time linear in the number of training examples.
Top/Computer_Science/Machine_Learning/Kernel_Methods/Support_Vector_Machines	Splice form prediction using Machine Learning Accurate ab initio gene finding is still a major challenge in computational biology. We employ state-of-the-art machine learning techniques based on Hidden Semi-Markov-SVMs to assay and improve the accuracy of genome annotations. We applied our system, called mSplicer, on the Caenorhabditis elegans genome and were able to drastically improve its annotation.
Top/Computer_Science/Machine_Learning/Kernel_Methods/Support_Vector_Machines	Polyp Detection in Endoscopic Video using SVMs
Top/Computer_Science/Machine_Learning/Kernel_Methods/Support_Vector_Machines	Pascal Challenge: Linear Support Vector Machines We participate in the linear SVM Track of the Pascal Large Scale Learning Challenge at ICML 2008. We consider the LIBLINEAR package, which can handle L1- and L2-loss linear SVMs. The L1-SVM solver implemented in LIBLINEAR employes a coordinate descent method to solve the dual problem. This method is very useful for large sparse data with a huge number of instances and features. However, most data sets of this challenges have a quite small number of features. To work on the competition data, we slightly modify LIBLINEAR.
Top/Computer_Science/Bioinformatics	Kernels in Bioinformatics
Top/Computer_Science/Bioinformatics	Kernel Methods in Computational Biology Many problems in computational biology and chemistry can be formalized as classical statistical problems, e.g., pattern recognition, regression or dimension reduction, with the caveat that the data are often not vectors. Indeed objects such as gene sequences, small molecules, protein 3D structures or phylogenetic trees, to name just a few, have particular structures which contain relevant information for the statistical problem but can hardly be encoded into finite-dimensional vector representations. Kernel methods are a class of algorithms well suited for such problems. Indeed they extend the applicability of many statistical methods initially designed for vectors to virtually any type of data, without the need for explicit vectorization of the data. The price to pay for this extension to non-vectors is the need to define a positive definite kernel between the objects, formally equivalent to an implicit vectorization of the data.
Top/Computer_Science/Bioinformatics	Brain Computer Interfaces Brain Computer Interfacing (BCI) aims at making use of brain signals for e.g. the control of objects, spelling, gaming and so on. This tutorial will first provide a brief overview of the current BCI research activities and provide details in recent developments on both invasive and non-invasive BCI systems. In a second part -- taking a physiologist point of view -- the necessary neurological/neurophysical background is provided and medical applications are discussed. The third part -- now from a machine learning and signal processing perspective -- shows the wealth, the complexity and the difficulties of the data available, a truely enormous challenge. In real-time a multi-variate very noise contaminated data stream is to be processed and classified. Main emphasis of this part of the tutorial is placed on feature extraction/selection and preprocessing which includes among other techniques CSP and also ICA methods. Finally, I report in more detail about the Berlin Brain Computer (BBCI) Interface that is based on EEG signals and take the audience all the way from the measured signal, the preprocessing and filtering, the classification to the respective application. BCI communication is discussed in a clincial setting and for gaming.
Top/Computer_Science/Bioinformatics	Machine Learning in Bioinformatics
Top/Computer_Science/Bioinformatics	Bioinformatics
Top/Computer_Science/Bioinformatics	Bioinformatics Challenge: Learning in Very High Dimensions with Very Few Samples Dedicated machine learning procedures have already become an integral part of modern genomics and proteomics. However, these very high dimensional and low learning sample tasks often stretch these procedures well beyond natural boundaries of their applicability. A few such challenges will be a subject of this series of lectures. We will start with a brief overview of classification of genomics (microarray) data. In particular we shall discuss, in some detail, examples of applications to cancer genomics and proteomics. Then we concentrate on a phenomenon of anti-learning, a case of supervised classification where standard supervised learning techniques systematically produce classifiers perfect on learning sample but with independent test error rates higher than that of the default (random) classification rule. The examples of natural and synthetic anti-learning data will be given and analysed from the stand point of implications to practical supervised and unsupervised classification. A series of practical tutorials will be organized in parallel. Participants will be exposed to classification of microarray data including first-hand experience with anti-learning.
Top/Computer_Science/Bioinformatics	Bioinformatic, Structural Biology and Structure Based Ligand Design in Drug discovery During the lecture we will be shown the principles on which the modern development and drug discovery with emphasis on optimization of compounds with the use of three dimensional structures of the receptor. Also the use of bioinformatics will be shown and computer supported planning of ligands together with the implications of the understanding of detailed interactions between the protein and the ligand in the transfiguration of the enzyme inhibitors into medicines in the context of modern challenges.
Top/Computer_Science/Bioinformatics	Bioinformatic, Structural Biology and Structure Based Ligand Design in Drug discovery During the lecture we will be shown the principles on which the modern development and drug discovery with emphasis on optimization of compounds with the use of three dimensional structures of the receptor. Also the use of bioinformatics will be shown and computer supported planning of ligands together with the implications of the understanding of detailed interactions between the protein and the ligand in the transfiguration of the enzyme inhibitors into medicines in the context of modern challenges.
Top/Computer_Science/Bioinformatics	Evidence Integration in Bioinformatics Biologists frequently use databases; for example, when a biologist encounters some unfamiliar proteins, s/he will use databases to get a preliminary idea of what is known about them. The databases can be often interpreted as lists of assertions. An example is a protein-protein interaction database: each entry is a pair of proteins that are asserted to interact, along with the supporting evidence. Often a candidate for inclusion in such a database can be supported in a variety of fundamentally different ways. A methodological challenge is how to effectively combine these different sources of evidence to make accurate aggregate predictions. Ideas from machine learning are useful for this. I will describe some of the special properties of problems like this, and relevant methods from machine learning, including algorithms based on bayesian networks, boosting and SVMs.
Top/Computer_Science/Bioinformatics	Mutual Cuts in Graphs: Learning in Bioinformatics
Top/Computer_Science/Bioinformatics	Industry 2: From the Bench to the Bedside: The role of Semantics in enabling the vision of Translational Medicine Biomedical research and healthcare clinical transactions are generating huge volumes of information. Biomedical research literature doubles every 19 years and AIDS literature in particular doubles every 22 months. Biomedical research is now an information-based science marked by factory-scale sequencing generating huge amounts of data. A clinician, on the other hand, needs approximately 2 million facts to practice. There is a critical need to speed 'translation' of genomic research insights into clinical research and practice and vice versa. In this talk, we will discuss the challenges faced by a healthcare enterprise in realizing the vision of Translational Medicine. In this talk, we will discuss some of these challenges, such as: * The need to create structured and semantic representations of genotypic and phenotypic data such as orders, observations, molecular diagnostic test results, etc. * The need for as-needed data and information integration achieved in an incremental and cost-efficient manner. * The need for actionable decision support for suggesting molecular diagnostic tests in response to phenotypic information and therapies in response to genotypic test results. * The need for knowledge update, propagation and consistency to keep abreast of the rapid pace of knowledge discovery being witnessed in the life sciences, a crucial pre-requisite to reduce the cost of knowledge acquisition and maintenance. There is a need and applicability of semantic web-based specifications and technologies to address the above challenges. We will present semantics-based approaches to address the challenges enumerated above. The role and applicability of various semantic web standards (such as RDF and OWL) in the proposed solutions will also be discussed .
Top/Computer_Science/Bioinformatics	Introduction to bioinformatics I will start by giving a general introduction into Bioinformatics, including basic biology, typical data types (sequences, structures, expression data and networks) and established analysis tasks. In the second part, I will discuss the problem of predictive sequence analysis with Support Vector Machines (SVMs). I will introduce a series of kernels suitable for different analysis tasks. Furthermore I will discuss the basic data structures needed for large scale learning and how to combine kernels for heterogeneous data. In the third part, I will focus on Hidden Markov models and discriminative alternatives like Conditional Random Fields and Hidden Markov SVMs suitable for segmentation tasks frequently appearing in Bioinformatics. In the last part I will present three applications in greater detail: A large margin alignment algorithm, computational gene finding and the identification of polymorphisms from resequencing arrays.
Top/Computer_Science/Bioinformatics	Lost in Translation -- Solving biological problems with machine learning We demonstrate the application of machine learning methods to problems from biology, chemistry, and pharmacy, nameley the prediction of protein subcellular localization, prediction of chromatiographic separation of oligo nucleotides, and the prediction of percutaneous drug absorption. For these examples, we show how translating the primary data into problem-specific features is essential for solving classification and regression problems.
Top/Computer_Science/Bioinformatics	LungCAD: A Clinically Approved, Machine Learning System for Lung Cancer Detection We present LungCAD, a computer aided diagnosis (CAD) system that employs a classification algorithm for detecting solid pulmonary nodules from CT thorax studies. We briefly describe some of the machine learning techniques developed to overcome the real world challenges in this medical domain. The most significant hurdle in transitioning from a machine learning research prototype that performs well on an in-house dataset into a clinically deployable system, is the requirement that the CAD system be tested in a clinical trial. We describe the clinical trial in which LungCAD was tested: a large scale multi-reader, multi-case (MRMC) retrospective observational study to evaluate the effect of CAD in clinical practice for detecting solid pulmonary nodules from CT thorax studies. The clinical trial demonstrates that every radiologist that participated in the trial had a significantly greater accuracy with LungCAD, both for detecting nodules and identifying potentially actionable nodules; this, along with other findings from the trial, has resulted in FDA approval for LungCAD in late 2006.
Top/Computer_Science/Bioinformatics	The Knowledge Revolution in Healthcare
Top/Computer_Science/Bioinformatics	ML in Bioinformatics After a brief introduction to the use of machine learning in computational biology, we focus on the problem of biological networks inference. We define the problem as a problem of kernel learning using prediction in kernelized output spaces. Methods based on Output kernel Tree are presented to solve the problem. Results on two benchmarks are shown.
Top/Computer_Science/Bioinformatics	Maximum Likelihood Estimation for a Gene Regulatory Network Defined by Differential Equations Gene regulation may be described by a set of deterministic differential equations describing the time rate evolution of the gene product concentrations, and containing parameters accounting for the regulatory relationships occurring in the gene network. We will present maximum likelihood based estimators of the parameters arising in this formalism and we will prove that they have desirable properties. Our results may be applied to a gene regulation model yielding the early Drosophila segments formation relying on a statistical modelling of gene expression data obtained by confocal laser scanning microscopy. The proposed statistical model accounts for the uncertainty in the measurement of gene expression and the uncertainty in the time at which the measurements are performed.
Top/Computer_Science/Bioinformatics	Identifiability of Delay Parameters for Nonlinear Time-delay Systems with Applications in Systems Biology The concept of parameter identifiability will be introduced briefly, followed by a short description of how this property can be tested for ODE-systems in general by rank calculations. Then, the extension of this analysis to delay systems, recently developed by Xia et al. [1] and Zhang et al. [2] will be reviewed. In these works, the authors use the framework of modules over non-commutative rings to formulate an analogous rank test for parameter identifiability of delay systems with known time-delays. Our ongoing work will then be motivated by a model of cellular signal transduction by Timmer et al. [3], where the sojourn time of STAT-5 in the nucleus is modelled by an unknown time delay which is estimated numerically by Timmer et al.. We will show how the identifiability of the time delay parameter is determined by the form of the external input-output representation of the system. Working in the mathematical framework of [1] and [2], we formulate explicit criteria based on rank calculations for the space spanned by the gradients of the output derivatives. Finally, several examples of biological systems from the literature will be discussed. Joint work with Bernt Wennberg.
Top/Computer_Science/Bioinformatics	Estimating Parameters and Hidden Variables in a Non-linear State-space Model of Regulatory Networks Understanding and identifying biological complex systems at work in the cell requires to develop models able to capture the stochastic nature of biological processes as well as their dynamics. Focusing on gene regulatory networks, we propose a new quantitative model in the form of a dynamical Bayesian network that allows to represent both genes and proteins in the same framework. We start from the nonlinear differential equations of Michaelis-Menten which are the gold-standard to represent biochemical interactions and develop a discrete-time and probabilistic model from these equations. Compared to previous works such as Nachman et al [1], our model takes into account the dependency between the regulatory proteins and the genes that code for them as well as protein-protein interactions and protein degradations. In the resulting nonlinear dynamical system, the proteins concentrations are hidden while gene expressions are observed. In order to learn the model's parameters, we first construct a discrete-time probabilistic model corresponding to our continuous-time state-space model and then derive a Kalman smoother algorithm based on the unscented transformation [2] to recursively estimate the parameters and unobserved protein activities. The generality of the learning method opens the door to various adaptations of the model if required by the biology. Numerical results on parameter and state estimation for the repressilator [3] and other several small networks are presented and show the relevance of the model.
Top/Computer_Science/Bioinformatics	Recovering Temporally Rewiring Networks: A model-based approach A plausible representation of relational information among entities in dynamic systems such as a living cell or a social community is a stochastic network which is topologically rewiring and semantically evolving over time. While there is a rich literature on modeling static or temporally invariant networks, much less has been done toward modeling the dynamic processes underlying rewiring networks, and on recovering such networks when they are not observable. We present a class of hidden temporal exponential random graph models (htERGMs) to study the yet unexplored topic of modeling and recovering temporally rewiring networks from time series of node attributes such as activities of social actors or expression levels of genes. We show that one can reliably infer the latent timespecific topologies of the evolving networks from the observation. We report empirical results on both synthetic data and a Drosophila lifecycle gene expression data set, in comparison with a static counterpart of htERGM.
Top/Computer_Science/Bioinformatics	Benchmarking parameter estimation and reverse engineering strategies Parameter estimation has become a central problem in systems biology, both in the form of calibration of bottom-up models or as a component of reverse engineering algorithms. With a proliferation of algorithms proposed for these purposes it has become important to compare them in objective ways. I will argue that in silico biochemical network models are extremely useful for this purpose. Several networks will be presented that are challenging tests for parameter estimation and network inference. An issue that arises from the use of in silico networks, though, is whether they can provide realistic data. The application of this benchmarking methodology will be illustrated with a comparison of four reverse engineering methods. //Joint work with Diogo Camacho, Paola Vera Licona, and Reinhard Laubenbacher//
Top/Computer_Science/Bioinformatics	Model Reduction for Parameter Estimation Estimating parameters in biochemical network models is a central but often difficult problem. A general approach that may be worth developing further is first to seek simplified or 'reduced' models with fewer dynamical degrees of freedom, estimate parameters for the reduced models, and then use that information to constrain the corresponding parameters in the full model. This approach can leverage appropriate human expertise and could in principle be applied recursively. The choice of variables to eliminate during model reduction could also be made by clustering or other machine learning methods. Some relevant model reductions already exist for quasi-equilibrium models of transcriptional regulation networks, which could provide a starting point for this strategy.
Top/Computer_Science/Bioinformatics	System Identification of Enzymatic Control Processes Using Population Monte Carlo Methods We demonstrate the superiority of Population Monte Carlo techniques over standard Metropolis Markov Chain Monte Carlo (MCMC) methods for inferring optimal parameters for a particular mechanistic model of a biological process given noisy experimental data. As our understanding of biological processes increases, the proposed models to describe them become more complex. With such potentially large numbers of equations and parameters, it is no longer feasible to hand-pick parameter values and be sure that the most appropriate values have been chosen. Monte Carlo methods are becoming more widely used for estimating parameter values, however we show that the standard Metropolis MCMC approach fails to converge on optimal values for even relatively simple models and that a more sophisticated method, in the form of non-Markovian Population Monte Carlo, may be successfully employed to produce consistent and accurate results. We illustrate the basic problem using the minimal model for the circadian genetic network in Arabidopsis thaliana, which consists of 3 linked differential equations containing a total of 6 parameters, with an additional noise parameter incorporated to estimate the variance of noise in the data. Joint work with Mark Girolami.
Top/Computer_Science/Bioinformatics	Learning to align: a statistical approach We present a new machine learning approach to the inverse parametric sequence alignment problem: given as training examples a set of correct pairwise global alignments, find the parameter values that make these alignments optimal.We consider the distribution of the scores of all incorrect alignments, then we search for those parameters for which the score of the given alignments is as far as possible from this mean, measured in number of standard deviations. This normalized distance is called the Z-score in statistics. We show that the Z-score is a function of the parameters and can be computed with efficient dynamic programs similar to the Needleman-Wunsch algorithm.We also show that maximizing the Z-score boils down to a simple quadratic program. Experimental results demonstrate the effectiveness of the proposed approach.
Top/Computer_Science/Bioinformatics	Game theoretic models in molecular biology There are many challenges in computational modeling of biological processes. Few processes such as signaling pathways operate in- dependently of others but rather involve substantial coordination and shared resources. The level of abstraction appropriate for understand- ing different processes, e.g, viewing a pathway as a filter or a molecular cascade, varies by context and the type of predictions sought.
Top/Computer_Science/Bioinformatics	Protein Subcellular Localization Prediction Based on Compartment-Specific Biological Features Prediction of subcellular localization of proteins is important for genome annotation, protein function prediction, and drug discovery. We present a prediction method for Gram-negative bacteria that uses ten one-versus-one support vector machine (SVM) classifiers, where compartment-specific biological features are selected as input to each SVM classifier. The final prediction of localization sites is determined by inte-grating the results from ten binary classifiers using a combination of majority votes and a probabilistic method. The overall accuracy reaches 91.4%, which is 1.6% better than the state-of-the-art system, in a ten-fold cross-validation evaluation on a bench-mark data set. We demonstrate that feature selection guided by biological knowledge and insights in one-versus-one SVM classifiers can lead to a significant improvement in the prediction performance. Our model is also used to produce highly accurate prediction of 92.8% overall accuracy for proteins of dual localizations.
Top/Computer_Science/Bioinformatics	Dynamic Modelling of Microarray Data We recently released rHVDM (Hidden Variable Dynamic Modelling), an R/Bioconductor package that predicts targets of a known transcription factor using time course microarray data. The key feature behind the algorithm is a simple ODE model of mRNA concentration. In the first stage of rHVDM, transcription factor activity (the hidden variable) is deduced from the expression time profile of a small number of known targets. This information is then used to screen other genes for dependency on that transcription factor. The accuracy of the technique has been demonstrated with Affymetrix microarray time course data and verified experimentally using siRNA knockdown of a targeted transcription factor (p53). While implementing the rHVDM algorithm and refining it for release we encountered a number of problems. These included parameter identifiability, parameter count reduction, algorithmic speed, parameter domain restriction, confidence interval estimation, and measurement noise. I will discuss each of these issues individually, along with the techniques we used to address them.
Top/Computer_Science/Bioinformatics	Reaction and Diffusion on Fractal Sets Systems biologists are interested in modelling chemical reactions in the intracellular environment, and to date much of what is done is based on the use of mass action kinetics to construct models of elementary reactions. Mass action kinetic models are based on a number assumptions which are not obviously valid in the intracellular environment. The cytoplasm is far from an ideal, isotropic wellmixed solution and often the concentrations of important chemical species are very small. Molecular crowding can have significant thermodynamic effects, but also must play an important dynamical role. An interesting approach that has been adopted to this has its roots in fractal geometry - a given molecule, depending upon its size and shape and the sizes and shapes of the molecules which surround it will find itself able to move in an environment of restricted dimension (see for example[1, 2]). Simple ideas have been suggested which give spatially homogeneous rate-like equations which attempt to account for this. It has been suggested, for example, that rate laws which depend on non-integer powers of the concentration of species might be used, and alternatively that the rate constants for elementary reactions which involve the encounter of different species (as opposed to spontaneous decomposition of individual molecules) should be time-dependent[1]. In this case the rates decay in time - the suggested form is the Zipf-Mandlebrot law which tends to a power law decay at long times, it is suggested that this power law characterises the dimension of the restricted environment of each chemical species[2]. Both of these approaches suffer from shortcomings. The use of non-integer powers of concentrations can only be justified in very limited circumstances, and has been shown to be inferior to the time-dependent rate parameter when describing certain lattice gas computer simulations of chemical reactions. However, the latter is clearly not invariant to time translation - the origin of time has a particular significance, and it is not clear as a general principle what the correct choice of time origin should be. Moreover, experimental techniques are being refined to the extent that spatio-temporal resolution of the species within a single cell is becoming possible. We might, therefore, aspire to constructing theories which describe the dynamics for spatially non-uniform distributions of active species. We have recently been working on a class of simple models of this type. These are spatio-temporal dynamical systems which model reaction and diffusion on a certain class of fractal sets. It has been known for some time now that it is possible to define random walks, and hence diffusion, on a certain class of fractals (indeed, it was this observation that motivated the work described above[1]). A simple example if this class is the Sierpinsky Gasket which has constrictions to the diffusion process in the sense that it can be disconnected by the removal of a finite set of points. The talk will focus mainly on this example, but we shall also suggest ways which could lead to more general models. Supported by the Manchester Institute for Mathematical Science (MIMS).
Top/Computer_Science/Bioinformatics	Spatiotemporal Modelling of Intracellular Signalling in Bacterial Chemotaxis Whilst theoretical models have been used to understand aspects of bacterial chemotaxis systems for the past thirty or so years, little work has focused on the importance that spatial localisation of proteins within the cytoplasm of the cell has on the overall functionality of the intracellular network. In this talk we will examine spatio-temporal models of signal transduction developed to describe the phosphotransfer pathway within E. coli. This model framework will then be extended to examine the importance of protein localisation within R. sphaeroides, a species which contains considerably more phosphotransfer proteins than E. coli and the spatial localisation of which plays a particularly important role in activating certain elements of the phosphotransfer network. The difficulties encountered in obtaining robust parameter estimates for reaction rates within the R. sphaeroides will also be detailed. Joint work with S. L. Porter, P. K. Maini and J. P. Armitage.
Top/Computer_Science/Bioinformatics	Model based identification of transcription factor activity from microarray data With the increase in volume of gene expression data available from high throughput microarray experiments, much research interest has been directed at building mathematical models of the process of gene regulation. Such models have primarily been used for the so called reverse engineering of regulatory networks; inferring possible regulatory interactions directly from microarray data, for example [14]. By using microarray data, all of these techniques make the implicit assumption that there is a direct relationship between the level of mRNA of genes coding for transcription factors (TFs) and the mRNA levels of their gene-targets.
Top/Computer_Science/Bioinformatics	Conditions for validity of re-sampling based
Top/Computer_Science/Bioinformatics	Conservation Laws and Identifiability of Models for Cellular Metabolism New experimental techniques in the biosciences provide us with high-quality data allowing quantitative mathematical modeling. When fitting model parameters to experimental data, it is important to know whether all parameters can be uniquely estimated from available data. In this paper we discuss a class of models for metabolism, where the introduction of conserved moieties may cause an otherwise identifiable model to be unidentifiable. A general method for reparametrization to identifiable rate expressions is presented, and the general results are exemplified by three well-cited models for yeast metabolism. Joint work with Milena Anguelova, Gunnar Cedersuna, Carl Johan Franzen, Mikael Johansson
Top/Computer_Science/Bioinformatics	Completion of biological networks : the output kernel trees approach Elucidating biological networks appears nowadays as one of the most important challenge in systems biology. Due to the availability of various sources of data, machine learning has to play a major role regarding this issue, given its large spectrum of tools ranging from generative models to concept learning methods. In this work the focus is narrowed on the completion of biological interactions networks for which some of the interactions between variables (usually genes or proteins) are already known.
Top/Computer_Science/Bioinformatics	Joint Mining of Biological Text and Images: Case Studies
Top/Computer_Science/Bioinformatics	Gene-based bin-analysis of genome-wide association studied With the improvement of genotyping technologies and the exponentially growing number of available markers, case-control genome-wide association studies promise to be a key tool for investigation of complex diseases. However new analytical methods have to be developed to face the problems induced by this data scale-up, such as statistical multiple testing, data quality control, biological interpretation and computational tractability. We present a novel method to analyze genome-wide association studies results. The algorithm is based on a Bayesian model that integrates genotyping errors and genomic structure dependencies. Probability values are assigned to genomic regions termed bins, which are defined from a gene-biased partitioning of the genome, and the false-discovery rate is estimated. We have applied this algorithm to data coming from three genome-wide association studies of Multiple Sclerosis. The method practically overcomes the scale-up problems and permits to identify new putative regions statistically associated with the disease.
Top/Computer_Science/Bioinformatics	Inverse Problems in Biology
Top/Computer_Science/Bioinformatics	Molecular Graph Kernels for Drug Discovery
Top/Computer_Science/Bioinformatics	Soft Topographic Map for Clustering and Classification of Bacteria In this work a new method for clustering and building a topographic representation of a bacteria taxonomy is presented. The method is based on the analysis of stable parts of the genome, the so-called housekeeping genes. The proposed method generates topographic maps of the bacteria taxonomy, where relations among different type strains can be visually inspected and verified. Two well known DNA alignement algorithms are applied to the genomic sequences. Topographic maps are optimized to represent the similarity among the sequences according to their evolutionary distances. The experimental analysis is carried out on 147 type strains of the Gammaprotebacteria class by means of the 16S rRNA housekeeping gene. Complete sequences of the gene have been retrieved from the NCBI public database. In the experimental tests the maps show clusters of homologous type strains and presents some singular cases potentially due to incorrect classification or erroneous annotations in the database.
Top/Computer_Science/Bioinformatics	Mathematical Modeling of Cell Signalling Pathways In recent years, the analysis of cell signalling systems through data-based models in ordinary differential equations (ODE) or other paradigms (e.g. stochastic models) has emerged as an invaluable tool to understand the underlying complexity of the protein interactions happening in cellular signal transduction. Compared with other biochemical systems, the modelling of cell signalling systems faces additional difficulties related to the challenges quantifying protein-protein processes but also to the lack of complete information about the topology of the considered network interactions. Since in most of the metabolic systems the complete network of interactions is (virtually) perfectly established, in cell signalling systems the real structure of the pathways is an open question to be elucidated either in parallel or through mathematical modelling based analysis. In this talk we discuss the use of power-law models (advantages and challenges) in biochemical systems. We also show how pre-existent biological knowledge and quantitative data can be integrated through mathematical modelling to validate hypothesis about the structure of signalling pathways.
Top/Computer_Science/Bioinformatics	Reverse engineering gene and protein regulatory networks using graphical models: A comparative evaluation study One of the major goals in systems biology is to infer the architecture of biochemical pathways and regulatory networks from postgenomic data, such as microarray gene expression and cytometric protein expression data. Various reverse engineering Machine Learning methods have been proposed in the literature, and it is important to understand their relative merits and shortcomings. In the talk the learning performances of three different graphical models machine learning methods, namely Relevance networks, Gaussian Graphical Models, and Bayesian networks, are cross-compared on real cytometric protein data and simulated data from the RAF signalling pathway. Relevance networks are based on pairwise association scores and straightforward to implement. But the inference is not done in the context of the whole system and there is no possibility to distinguished between direct and indirect associations. Both shortcomings are addressed by Gaussian graphical models, where the partial correlation between two variables, conditional on all the other domain variables, is employed as association score. Bayesian networks are more flexible probabilistic graphical models for conditional dependence and independence relations. Bayesian networks are based on directed acyclic graphs and can be exploited to analyse interventional data for identifying putative causal interactions. The empirical results were obtained by applying the shrinkage estimator of Schaefer and Strimmer (2005) to compute the inverse covariance matrix for Gaussian Graphical Models, and Bayesian network inference was done by sampling BNs from the posterior distribution with order Markov chain Monte Carlo (MCMC), as proposed by Friedman and Koller (2003). The experimental results were obtained by analysing data from the RAF protein signalling network reported in Sachs et al. (2005); which describes the interaction of eleven phosphorylated proteins and phospholipids in human immune system cells. Thereby it was distinguished between real cytometric protein activity measurements reported in Sachs et al. (2005) and synthetically generated data as well as between pure observational and interventional data. Observational data are obtained by passively monitoring the system without any interference while interventional data are obtained by actively manipulating variables, e.g. using gene knock-out experiments. Detailed results of this empirical study have been published in Werhli et al. (2006) and Grzegorczyk (2007). The three main findings can be summarized as follows. First, exclusively on Gaussian observational data, Bayesian networks and Gaussian graphical models were found to outperform Relevance networks. Second, for observational data no significant difference between Bayesian networks and Gaussian Graphical models was observed. Third, only for interventional data Bayesian networks clearly performed superior to the other two approaches.
Top/Computer_Science/Bioinformatics	Estimation of human endogeneous retrovirus activities from expressed sequence databases Human endogenous retroviruses (HERVs) are remnants of ancient retrovirus infections and now reside within the human DNA. Recently HERV expression has been detected in both normal tissues and diseased patients. However, the activities (expression levels) of individual HERV sequences are mostly unknown.
Top/Computer_Science/Bioinformatics	Relationship between structure and dynamics of gene regulatory networks Gene regulatory networks in living cells are comprised of recurring network motifs, which perform key functions to control cellular responses. Mathematical models of network motifs are already developed and experimentally validated. We analyze the stability of previously validated mathematical models of network motifs against the fluctuations in their associated biochemical reaction rates and find that all of these elementary functional modules are stable against any perturbation in their rates of reaction. In gene regulatory networks the motifs are connected to each other in a directed acyclic manner. Any large assembly of stable and robust motifs connected with each other in a directed acyclic manner exhibits stable and robust dynamics. All the motifs except those having feedback loops in their structure preserve certain dynamic properties when embedded within large operational networks. Our study also suggests that evolutionary mechanisms selected stable and robust functional modules with which to build regulatory networks in order to ensure stability and reliability on a larger scale instead of finding the simplest canonical representation with which to serve the same purpose.
Top/Computer_Science/Bioinformatics	Need of Systems Approach for Biological Explanation of Anti-Learnable Signatures
Top/Computer_Science/Bioinformatics	Data variability could be your friend Deterministic modeling, in the form of ordinary differential equations (ODE), is the dominant paradigm in systems biology. This stems partially from the type of data that is available. Input data (e.g. gene expression data, protein concentrations) for these models is normally derived from whole cell populations. Consequently, what is modeled is the behaviour of one average cell rather than a multitude of individual cells. Variability within the data originates mainly from the measurement apparatus (technical error) or from difficult-to-control environmental conditions that precede the measurement (biological variability) and can constitute an impediment to clear cut conclusions. For example, kinetic parameters cannot be known with absolute precision and have to be accompanied with confidence intervals that are generally commensurate with the rather high variability attached to biological data. Data variability can also put obstacles in the way of decisive model selection. Measurement techniques are, however, increasingly being applied to individual cells. It is possible to average the individual cell observations, estimate the dispersion of this synthetic measurement, and use these data along with the modeling paradigms outlined above. However, inter-cell variability can be the result of intrinsic system noise. In particular, this is the case if molecular species involved exist in very low concentrations, such as in signaling networks. We argue that because this variability is in part intrinsic, it can be harnessed rather than tolerated, so that it provides novel insights into the mechanisms governing the system under study. This requires a paradigm shift from deterministic to stochastic modeling- even though ODEs are still central in the latter. To illustrate this, the example system we use is DNA Double Strand Break repair dynamics in irradiated human cells. Recent assaying techniques allow the quantification of DNA Double Strand Break (DSB) at the individual cell level. Repeated measurements in time form a dynamic image of the DSB decay process of cells after they have been exposed to a pulse of ionising irradiation. Crucially, individual cell measurements allow the monitoring of distributional features of the DSB count in a population. Existing deterministic models correctly mimic global features in this system. In particular, they can fit very well different decay regimes that are being observed when one focuses on the average DSB count in the population. We show however that these models, when translated into the stochastic realm, provide a poor data fit when one considers distributional features, such as the variance of the DSB count. Furthermore, using simple stochastic models that are partly amenable to analytical manipulation, we show that enriching the existing models with extra feedback loops produces an outcome more in tune with observations. Three independent data sets are used. Possible biological consequences are briefly discussed.
Top/Computer_Science/Machine_Learning/Boosting	Boosting
Top/Computer_Science/Machine_Learning/Boosting	Boosting
Top/Computer_Science/Machine_Learning/Boosting	Boosting Boosting is a general method for producing a very accurate classification rule by combining rough and moderately inaccurate 'rules of thumb.' While rooted in a theoretical framework of machine learning, boosting has been found to perform quite well empirically. This tutorial will introduce the boosting algorithm AdaBoost?, and explain the underlying theory of boosting, including explanations that have been given as to why boosting often does not suffer from overfitting, as well as some of the myriad other theoretical points of view that have been taken on this algorithm. Some recent applications and extensions of boosting will also be described.
Top/Computer_Science/Machine_Learning/Boosting	Introduction to Boosting This course provides an introduction to theoretical and practical aspects of Boosting and Ensemble Learning. I will begin with a short description of the learning theoretical foundations of weak learners and their linear combination. Then we point out the useful connection between Boosting and the Theory of Optimization, which facilitates the understanding of Boosting and later on enables us to move on to new Boosting algorithms, applicable to a broader spectrum of problems. In the course we will discuss 'tricks of the trade', algorithmic issues, experimental results and a few applications.
Top/Computer_Science/Machine_Learning/Boosting	The Dynamics of AdaBoost One of the most successful and popular learning algorithms is AdaBoost, which is a classification algorithm designed to construct a 'strong' classifier from a 'weak' learning algorithm. Just after the development of AdaBoost nine years ago, scientists derived margin- based generalization bounds to explain AdaBoost's unexpectedly good performance. Their result predicts that AdaBoost yields the best possible performance if it always achieves a 'maximum margin' solution. Yet, does AdaBoost achieve a maximum margin solution? Empirical and theoretical studies conducted within this period conjecture the answer to be 'yes'. In order to answer this question, we look toward AdaBoost's dynamics. We simplify AdaBoost to reveal a nonlinear iterated map. We then analyze the convergence of AdaBoost for cases where cyclic behavior is found; this cyclic behavior provides the key to answering the question of whether AdaBoost always maximizes the margin. As it turns out, the answer to this question turns out to be the opposite of what was thought to be true! In this talk, I will introduce AdaBoost, describe our analysis of AdaBoost when viewed as a dynamical system, briefly mention a new boosting algorithm which always maximizes the margin with a fast convergence rate, and if time permits, I will reveal a surprising new result about AdaBoost and the problem of bipartite ranking.
Top/Computer_Science/Machine_Learning/Boosting	Predicting Electricity Distribution Feeder Failures Using Boosting and Online Learning
Top/Computer_Science/Machine_Learning/Boosting	BoostCluster: Boosting Clustering by Pairwise Constraints Data clustering is an important task in many disciplines. A large number of studies have attempted to improve clustering by using the side information that is often encoded as pairwise constraints. However, these studies focus on designing special clustering algorithms that can effectively exploit the pairwise constraints. We present a boosting framework for data clustering, termed as BoostCluster, that is able to iteratively improve the accuracy of any given clustering algorithm by exploiting the pairwise constraints. The key challenge in designing a boosting framework for data clustering is how to influence an arbitrary clustering algorithm with the side information since clustering algorithms by definition are unsupervised. The proposed framework addresses this problem by dynamically generating new data representations at each iteration that are, on the one hand, adapted to the clustering results at previous iterations by the given algorithm, and on the other hand consistent with the given side information. Our empirical study shows that the proposed boosting framework is effective in improving the performance of a number of popular clustering&#160; algorithms (Kmeans, partitional SingleLink, spectral clustering), and its performance is comparable to the state-of-the-art algorithms for data clustering with side information.
Top/Computer_Science/Machine_Learning/Boosting	An Introduction to Ensemble and Boosting
Top/Computer_Science/Fuzzy_Logic	Fuzzy Logic The tutorial will introduce the basics of fuzzy logic for data analysis. Fuzzy Logic can be used to model and deal with imprecise information, such as inexact measurements or available expert knowledge in the form of verbal descriptions. We will first introduce the concepts of fuzzy sets, degrees of membership and fuzzy set operators. After discussions on fuzzy numbers and arithmetic operations using them, the focus will shift to fuzzy rules and how such systems of rules can be derived from available data.
Top/Computer_Science/Fuzzy_Logic	Fuzzy Logic Based Gait Classification for Hemiplegic Patients In this study a fuzzy logic classification system was used first to discriminate healthy subjects from patients rather than classifying those using Brunnstrom stages. Decision making was performed in two stages: feature extraction of gait signals and the fuzzy logic classification system which is used Tsukamato-type inference method. According to our signal feature extraction studies, we focused on temporal events and symetrical features of gait signal. Developed system has six inputs while four of them for temporal features evaluation rule block and two of them symmetrical features evaluation rule block. Our simulation test results showed that proposed system classify correctly 100% of subjects as patient and healthy elderly. The correlation coefficient was found 0.85 for classification to subjects to correct Brunnstrom stages. The results show that classifying patients becomes increasingly difficult linearly according to hemiplegias severity.
Top/Computer_Science/Fuzzy_Logic	Evolving Systems One of the important research challenges today is to develop new theoretical methods, algorithms, and implementations of systems with a higher level of flexibility and autonomy, we can say with higher level of intelligence. These systems have to be able to evolve their structure and knowledge on the environment and ultimately evolve their intelligence. To address the problems of modelling, control, prediction, classification and data processing in a dynamically changing and evolving environment, a system must be able to fully adapt its structure and adjust its parameters, rather than use a pre-trained and a fixed structure. That is, the system must be able to evolve, to self-develop, to self-organize, to self-evaluate and to self-improve. The talk will concentrate on the problems and results the author encountered during last several years of research in this emerging area as well as on the approach to on-line identification of a particular type of fuzzy models so called Takagi-Sugeno fuzzy models including some applications, in particular to mobile robots, mobile communications, process modelling and control, on-line evolving classification intelligent (inferential) sensors.
Top/Computer_Science/Fuzzy_Logic	Real Time GPU-Based Fuzzy ART Skin Recognition
Top/Computers/Programming	Reverse engineering techniques to find security bugs: A case study of the ANI Alex Sotirov is a vulnerability engineer at determina. He will discuss some latest techniques in reverse engineering software to find vulnerabilities. Particularly, he'll discuss his technique that lead him to find the ANI bug (a critical new bug in WinXP and Vista). Alex will describe the tools he uses for reverse engineering and show how he reverse engineered ANI Bug. He will continue to discussed Windows security mechanisms (ASLR, /GS) and describe how ANI exploit bypasses them.
Top/Computers/Programming	Faith, Evolution, and Programming Languages Faith and evolution provide complementary--and sometimes conflicting--models of the world, and they also can model the adoption of programming languages. Adherents of competing paradigms, such as functional and object-oriented programming, often appear motivated by faith. Families of related languages, such as C, C++, Java, and C#, may arise from pressures of evolution. As designers of languages, adoption rates provide us with scientific data, but the belief that elegant designs are better is a matter of faith. This talk traces one concept, second-order quantification, from its inception in the symbolic logic of Frege through to the generic features introduced in Java 5, touching on features of faith and evolution. The remarkable correspondence between natural deduction and functional programming informed the design of type classes in Haskell. Generics in Java evolved directly from Haskell type classes, and are designed to support evolution from legacy code to generic code. Links, a successor to Haskell aimed at AJAX-style three-tier web applications, aims to reconcile some of the conflict between dynamic and static approaches to typing.
Top/Computers/Programming	Three Beautiful Quicksorts This talk describes three of the most beautiful pieces of code that I have ever written: three different implementations of Hoare's classic Quicksort algorithm. # The first implementation is a bare-bones function in about a dozen lines of C. # The second implementation starts by instrumenting the first program to measure its run time; a dozen systematic code transformations proceed to make it more and more powerful yet more and more simple, until it finally disappears in a puff of mathematical smoke. It therefore becomes the most beautiful program I never wrote. # The third program is an industrial-strength C library Qsort function that I built with Doug McIlroy. A theme running through all three implementations is the power of elegance and simplicity. (This talk expands my Chapter 3 in Beautiful Code, edited by Oram and Wilson and published by O'Reilly in July, 2007.)
Top/Computers/Programming	The Graphing Calculator Story It's midnight. I've been working sixteen hours a day, seven days a week. I'm not being paid. In fact, my project was canceled six months ago, so I'm evading security, sneaking into Apple Computer's main offices in the heart of Silicon Valley, doing clandestine volunteer work for an eight-billion-dollar corporation. [[http://www.pacifict.com/Story/|...more at www.pacifict.com/Story/]]
Top/Computers/Programming	Basics of algorithmics, computation models, formal languages Between the many theoretical computer science issues that one should be aware of when working in Machine learning, we visit, in this series of lectures, two. The first corresponds to strings, and through the study of strings, the questions about more complex structures like trees and graphs. We describe the main algorithmic and combinatorial questions about substrings and subsequences, and concentrate our attention to the topological questions: ordering strings and computing distances and kernels. The second is complexity. Not only should we be aware (and have a reasonable control of the techniques involved) of the usual barriers, but we should know something about classes for randomized algorithms. We also show some examples concerning Las Vegas and Monte Carlo techniques.
Top/Computers/Programming	Advanced Topics in Programming Languages Series: Python Design Patterns
Top/Mathematics	Social Constructivism as a Philosophy of Mathematics
Top/Mathematics	Symmetry in the works of M. C. Escher Introductory References about Tessellations (tilings) and Sources for Illustrations: [[http://www.geom.uiuc.edu/software/tilings/TilingBibliography.html|link]]
Top/Mathematics	Visualizing Cauchys Interlacing Property for Line Distance Matrices In the paper it is proven that line distance matrices of size n have one positive and n ! 1 negative eigenvalues. Visual representation of Cauchy's interlacing property for line distance matrices is considered.
Top/Mathematics	Famous and lesser known problems in elementary combinatorial geometry and number theory Which problems attain great notoriety and which are delegated to collect dust on a shelf? Elementary problems tend to attract attention because they are very easy to understand and look solvable. It is a mystery to me why some attract a lot of attention while others lie hibernating waiting for some new fresh ideas. In their recent interesting book Research Problems in Discrete Geometry (Springer, New York 2005) P. Brass, W. Moser, J. Pach wrote: Although Discrete Geometry has a rich history extending more than 150 years, it abounds in open problems that even a high-school student can understand and appreciate. Some of these problems are notoriously difficult and are intimately related to deep questions in other fields of mathematics. But many problems, even old ones, can be solved by a clever undergraduate or a high- school student equipped with an ingenious idea and the kinds of skills used in a mathematical olympiad.
Top/Mathematics	Estimation of the Solution of a Differential Equation: an Inverse Problem
Top/Mathematics	Reaction and Diffusion on Fractal Sets Systems biologists are interested in modelling chemical reactions in the intracellular environment, and to date much of what is done is based on the use of mass action kinetics to construct models of elementary reactions. Mass action kinetic models are based on a number assumptions which are not obviously valid in the intracellular environment. The cytoplasm is far from an ideal, isotropic wellmixed solution and often the concentrations of important chemical species are very small. Molecular crowding can have significant thermodynamic effects, but also must play an important dynamical role. An interesting approach that has been adopted to this has its roots in fractal geometry - a given molecule, depending upon its size and shape and the sizes and shapes of the molecules which surround it will find itself able to move in an environment of restricted dimension (see for example[1, 2]). Simple ideas have been suggested which give spatially homogeneous rate-like equations which attempt to account for this. It has been suggested, for example, that rate laws which depend on non-integer powers of the concentration of species might be used, and alternatively that the rate constants for elementary reactions which involve the encounter of different species (as opposed to spontaneous decomposition of individual molecules) should be time-dependent[1]. In this case the rates decay in time - the suggested form is the Zipf-Mandlebrot law which tends to a power law decay at long times, it is suggested that this power law characterises the dimension of the restricted environment of each chemical species[2]. Both of these approaches suffer from shortcomings. The use of non-integer powers of concentrations can only be justified in very limited circumstances, and has been shown to be inferior to the time-dependent rate parameter when describing certain lattice gas computer simulations of chemical reactions. However, the latter is clearly not invariant to time translation - the origin of time has a particular significance, and it is not clear as a general principle what the correct choice of time origin should be. Moreover, experimental techniques are being refined to the extent that spatio-temporal resolution of the species within a single cell is becoming possible. We might, therefore, aspire to constructing theories which describe the dynamics for spatially non-uniform distributions of active species. We have recently been working on a class of simple models of this type. These are spatio-temporal dynamical systems which model reaction and diffusion on a certain class of fractal sets. It has been known for some time now that it is possible to define random walks, and hence diffusion, on a certain class of fractals (indeed, it was this observation that motivated the work described above[1]). A simple example if this class is the Sierpinsky Gasket which has constrictions to the diffusion process in the sense that it can be disconnected by the removal of a finite set of points. The talk will focus mainly on this example, but we shall also suggest ways which could lead to more general models. Supported by the Manchester Institute for Mathematical Science (MIMS).
Top/Mathematics	Some Operations on the Lattice of Equivalence Relations Equivalence relations have played a fundamental role through the History of Mathematics. These special relations are so omnipresent in everyday life that we often forget about their proactive existence. Though, according to some authors, still much is unknown about them. In this talk, we will studdy independence and permutability, on the context of a equivalence relations lattice of a given set, refering to the interpretation of these mathematical concepts in the Information Theory point of view. We, then, present of equivalence pairs of finite type, due to B.Jnsson, and introduce the operation star defined within equivalences, rewrithing M.-L. Dubreil important theorem, describing permutable equivalences.
Top/Mathematics	The Mathematics of Emergence and Flocking Stephen J. Smale, a professor emeritus of mathematics at the University of California, Berkeley, who has contributed to a broad range of mathematical fields, has been named a recipient of the 2007 Wolf Foundation Prize in Mathematics, one of an array of prestigious prizes awarded yearly by the Israeli foundation. Though retired from UC Berkeley since 1994, Smale continues to explore new fields, such as learning theory - the mathematical description of nerve connections in the brain that give rise to intelligence and learning; flocking, the tendency of group behavior to look coordinated, as with a flock of birds or a school of fish; and the mathematics of data mining. (UC Newsroom, University of Califonia, 2007-01- 19)
Top/Mathematics	A Century of Controversy over the Foundations of Mathematics I'll tell the dramatic story of the recent disputes over the foundations of mathematics. I'll start with the problems in Cantor's theory of infinite sets and then discuss the work of Bertrand Russell, David Hilbert, Kurt Godel and Alan Turing, and finally my own work using complexity. This complexity-based analysis of the foundations of mathematics suggests to me that perhaps mathematics is more similar to physics and to biology than is commonly believed, and should sometimes be carried out quasi-empirically, that is, more in the spirit of an experimental science.
Top/Computer_Science/Machine_Learning/Reinforcement_Learning	Model-based Bayesian RL Although Bayesian methods for Reinforcement Learning can be traced back to the 1960s (Howard's work in Operations Research), Bayesian methods have only been used sporadically in modern Reinforcement Learning. This is in part because non-Bayesian approaches tend to be much simpler to work with. However, recent advances have shown that Bayesian approaches do not need to be as complex as initially thought and offer several theoretical advantages. For instance, by keeping track of full distributions (instead of point estimates) over the unknowns, Bayesian approaches permit a more comprehensive quantification of the uncertainty regarding the transition probabilities, the rewards, the value function parameters and the policy parameters. Such distributional information can be used to optimize (in a principled way) the classic exploration/exploitation tradeoff, which can speed up the learning process. Similarly, active learning for reinforcement learning can be naturally optimized. The estimation of gradient performance with respect to value function or and/or policy parameters can also be done more accurately while using less data. Bayesian approaches also facilitate the encoding of prior knowledge and the explicit formulation of domain assumptions. The primary goal of this tutorial is to raise the awareness of the research community with regard to Bayesian methods, their properties and potential benefits for the advancement of Reinforcement Learning. An introduction to Bayesian learning will be given, followed by a historical account of Bayesian Reinforcement Learning and a description of existing Bayesian methods for Reinforcement Learning. The properties and benefits of Bayesian techniques for Reinforcement Learning will be discussed, analyzed and illustrated with case studies.
Top/Computer_Science/Machine_Learning/Reinforcement_Learning	Reinforcement Learning
Top/Computer_Science/Machine_Learning/Reinforcement_Learning	Reinforcement Learning Reinforcement learning is about learning good control policies given only weak performance feedback: occasional scalar rewards that might be delayed from the events that led to good performance. Reinforcement learning inherently deals with feedback systems rather than (data, class) data samples, providing a more flexible control-like framework than many standard machine algorithms. These lectures will summarise reinforcement learning along 3 axes: # Learning with or without knowledge of the system dynamics. # Using state values as an intermediate solution, or learning a policy directly. # Learning with or without fully observable system states.
Top/Computer_Science/Machine_Learning/Reinforcement_Learning	Reinforcement Learning Theory The tutorial is on several new pieces of Reinforcement learning theory developed in the last 7 years. This includes: 1. Sample based analysis of RL including E3 and sparse sampling. 2. Generalization based analysis of RL including conservative policy iteration and RL-to-Classification reductions. For each of these forms of theory, we cover the basic results and cover the weaknesses and strengths of the approach in context.
Top/Computer_Science/Machine_Learning/Reinforcement_Learning	Gaussian Process Temporal Difference Although Bayesian methods for Reinforcement Learning can be traced back to the 1960s (Howard's work in Operations Research), Bayesian methods have only been used sporadically in modern Reinforcement Learning. This is in part because non-Bayesian approaches tend to be much simpler to work with. However, recent advances have shown that Bayesian approaches do not need to be as complex as initially thought and offer several theoretical advantages. For instance, by keeping track of full distributions (instead of point estimates) over the unknowns, Bayesian approaches permit a more comprehensive quantification of the uncertainty regarding the transition probabilities, the rewards, the value function parameters and the policy parameters. Such distributional information can be used to optimize (in a principled way) the classic exploration/exploitation tradeoff, which can speed up the learning process. Similarly, active learning for reinforcement learning can be naturally optimized. The estimation of gradient performance with respect to value function or and/or policy parameters can also be done more accurately while using less data. Bayesian approaches also facilitate the encoding of prior knowledge and the explicit formulation of domain assumptions. The primary goal of this tutorial is to raise the awareness of the research community with regard to Bayesian methods, their properties and potential benefits for the advancement of Reinforcement Learning. An introduction to Bayesian learning will be given, followed by a historical account of Bayesian Reinforcement Learning and a description of existing Bayesian methods for Reinforcement Learning. The properties and benefits of Bayesian techniques for Reinforcement Learning will be discussed, analyzed and illustrated with case studies.
Top/Computer_Science/Machine_Learning/Reinforcement_Learning	Introduction to Reinforcement Learning and Bayesian learning Although Bayesian methods for Reinforcement Learning can be traced back to the 1960s (Howard's work in Operations Research), Bayesian methods have only been used sporadically in modern Reinforcement Learning. This is in part because non-Bayesian approaches tend to be much simpler to work with. However, recent advances have shown that Bayesian approaches do not need to be as complex as initially thought and offer several theoretical advantages. For instance, by keeping track of full distributions (instead of point estimates) over the unknowns, Bayesian approaches permit a more comprehensive quantification of the uncertainty regarding the transition probabilities, the rewards, the value function parameters and the policy parameters. Such distributional information can be used to optimize (in a principled way) the classic exploration/exploitation tradeoff, which can speed up the learning process. Similarly, active learning for reinforcement learning can be naturally optimized. The estimation of gradient performance with respect to value function or and/or policy parameters can also be done more accurately while using less data. Bayesian approaches also facilitate the encoding of prior knowledge and the explicit formulation of domain assumptions. The primary goal of this tutorial is to raise the awareness of the research community with regard to Bayesian methods, their properties and potential benefits for the advancement of Reinforcement Learning. An introduction to Bayesian learning will be given, followed by a historical account of Bayesian Reinforcement Learning and a description of existing Bayesian methods for Reinforcement Learning. The properties and benefits of Bayesian techniques for Reinforcement Learning will be discussed, analyzed and illustrated with case studies.
Top/Computer_Science/Machine_Learning/Reinforcement_Learning	Effects of Stress and Genotype on Exploration-Exploitation Dynamics in Reinforcement Learning Stress and genetic background regulate different aspects of behavioral learning through the action of stress hormones and neuromodulators. Similarly, in reinforcement learning (RL) models, exploitation-exploration factor and other meta-parameters control learning dynamics and performance. We found that many different measures of animal learning and performance can be reproduced by simple RL models using dynamic control of the meta-parameters. To study the effects of stress and genotype, we carried out 5-hole-box light conditioning and Morris water maze experiments with 2 different genetic strains of mice, exposing them to different stressors. Then, we used RL models to simulate their behavior. For each experimental session, we estimated a set of model meta-parameters that produced the best fit between the model and the animal performance. Exploration-exploitation factors had similar characteristic dynamics for the two simulated experiments, and there were statistically significant differences between different genetic strains and stress conditions.
Top/Computer_Science/Machine_Learning/Reinforcement_Learning	Policy-gradient Reinforcement Learning
Top/Computer_Science/Machine_Learning/Reinforcement_Learning	Reinforcement Learning MDPs/VI, Q learning (w/ proof), TD(lambda), Function approximation, options, PSRs
Top/Computer_Science/Machine_Learning/Reinforcement_Learning	Welcome Although Bayesian methods for Reinforcement Learning can be traced back to the 1960s (Howard's work in Operations Research), Bayesian methods have only been used sporadically in modern Reinforcement Learning. This is in part because non-Bayesian approaches tend to be much simpler to work with. However, recent advances have shown that Bayesian approaches do not need to be as complex as initially thought and offer several theoretical advantages. For instance, by keeping track of full distributions (instead of point estimates) over the unknowns, Bayesian approaches permit a more comprehensive quantification of the uncertainty regarding the transition probabilities, the rewards, the value function parameters and the policy parameters. Such distributional information can be used to optimize (in a principled way) the classic exploration/exploitation tradeoff, which can speed up the learning process. Similarly, active learning for reinforcement learning can be naturally optimized. The estimation of gradient performance with respect to value function or and/or policy parameters can also be done more accurately while using less data. Bayesian approaches also facilitate the encoding of prior knowledge and the explicit formulation of domain assumptions. The primary goal of this tutorial is to raise the awareness of the research community with regard to Bayesian methods, their properties and potential benefits for the advancement of Reinforcement Learning. An introduction to Bayesian learning will be given, followed by a historical account of Bayesian Reinforcement Learning and a description of existing Bayesian methods for Reinforcement Learning. The properties and benefits of Bayesian techniques for Reinforcement Learning will be discussed, analyzed and illustrated with case studies.
Top/Computer_Science/Machine_Learning/Reinforcement_Learning	Artificial intelligence: An instance of Aibo ingenuity This describes research related to using RL for, among other tasks, learning behaviors for an Aibo robot.
Top/Computer_Science/Machine_Learning/Reinforcement_Learning	A path integral approach to stochastic optimal control Many problems in machine learning use a probabilistic description. Examples are pattern recognition methods and graphical models. As a consequence of this uniform description, one can apply generic approximation methods such as mean field theory and sampling methods. Another important class of machine learning problems are the reinforcement learning problems, aka optimal control problems. Here, also a probabilistic description is used, but up to now efficient mean field approximations have not been obtained. In this presentation, I consider linear-quadratic control of an arbitrary dynamical system and show, that for this class of stochastic control problems the non-linear Hamilton-Jacobi-Bellman equation can be transformed into a linear equation. The transformation is similar to the transformation used to relate the Schrdinger equation to the Hamilton-Jacobi formalism. The computation can be performed efficiently by means of a forward diffusion process that can be computed by stochastic integration or that can be described by a path integral. For this path integral it is expected that a variational mean field approximation could be derived.
Top/Computer_Science/Machine_Learning/Reinforcement_Learning	Introduction to Reinforcement Learning The tutorial will introduce Reinforcement Learning, that is, learning what actions to take, and when to take them, so as to optimize long-term performance. This may involve sacrificing immediate reward to obtain greater reward in the long-term or just to obtain more information about the environment. The first part of the tutorial will cover the basics, such as Markov decision processes, dynamic programming, temporal-difference learning, Monte Carlo methods, eligibility traces, the role of function approximation. In the second part we cover some recent developments, namely policy gradient and second order methods, such as LSPI and the modified Bellman residual minimization algorithm.
Top/Computer_Science/Machine_Learning/Reinforcement_Learning	Learning for Control from Multiple Demonstrations We consider the problem of learning to follow a desired trajectory when given a small number of demonstrations from a sub-optimal expert. We present an algorithm that (i) extracts the---initially unknown---desired trajectory from the sub-optimal expert's demonstrations and (ii) learns a local model suitable for control along the learned trajectory. We apply our algorithm to the problem of autonomous helicopter flight. In all cases, the autonomous helicopter's performance exceeds that of our expert helicopter pilot's demonstrations. Even stronger, our results significantly extend the state-of-the-art in autonomous helicopter aerobatics. In particular, our results include the first autonomous tic-tocs, loops and hurricane, vastly superior performance on previously performed aerobatic maneuvers (such as in-place flips and rolls), and a complete airshow, which requires autonomous transitions between these and various other maneuvers.
Top/Arts/Music	Ethnomusicology of the Nineties: Perspectives of the History of Research Bruno Nettl, rojen v Pragi leta 1930, je 'profesor emeritus glasbe in antropologije' na University of Illinois, Urbana-Champaign, ZDA. Doktorat je pridobil na Indiana University. eprav je njegov domicil e desetletja University of Illinois, je kot gostujoi profesor deloval na tevilnih univerzah v ZDA in drugje po svetu. Prejel je vrhunske nagrade za doseke s podroij etnomuzikologije, muzikologije, antropologije in folkloristike Med njegove najpogosteje citirane knjige sodijo: North American Indian Musical Styles (1954); An Introduction to Folk Music in the United States (1960); Theory and Method in Ethnomusicology (1964); Folk and Traditional Music of the Western Continents (1965); Eight Urban Musical Cultures [ed.] (1978); The Study of Ethnomusicology: 29 Issues and Concepts (1983); The Western Impact on World Music (1985); The Radif of Persian Music: Studies of Structure and Cultural Context (1987); Blackfoot Musical Thought: Comparative Perspectives (1989); Comparative Musicology and Anthropology of Music: Essays on the History of Ethnomusicology [co-ed.] (1991); Excursions in World Music [co-ed.] (1992); New Perspectives on Improvisation [ed.] (1992); Heartland Excursions: Ethnomusicological Reflections on Schools of Music (1995); Encounters in Ethnomusicology, a Memoir (2002) and The Study of Ethnomusicology: 31 Issues and Concepts (2005). Marsikatera med njimi je bila prevedena v druge jezike.
Top/Arts/Music	Abiding Issues in the Study of North American Indian Music
Top/Arts/Music	A Concert of Carnatic Music
Top/Arts/Music	Interview with David Hardoon
Top/Arts/Music	Book-Adaptive and Book-Dependent Models to Accelerate Digitization of Early Music Optical music recognition (OMR) enables early music collections to be digitized on a large scale. The workflow for such digitisation projects also includes scanning and preprocessing, but the cost of expert human labour to correct automatic recognition errors dominates the cost of these other two steps. To reduce the number of recognition errors in the OMR process, we present an innovative application of maximum a posteriori (MAP) adaptation for hidden Markov models (HMMs) to build book-adaptive models, taking advantage of the new learning data generated from human editing work, which is part of any music digitization project. We also experimented with using the generated data to build book-dependent models from scratch, which sometimes outperform the book-adaptive models after enough corrected data is available. Our experiments show that these approaches can reduce human editing costs by more than half and that they are especially well suited to highly variable sources like early or degraded documents.
Top/Arts/Music	The Mental Representation of Music: A Neural Darwinist Perspective In this presentation, I review the perceptual research related to auditory representation for music. The research suggests that multiple representations exist concurrently in the auditory system, and that the dominant representation is shaped by the specific auditory environment. I note that the research is consistent with theories of competitive representations, such as Edelman's neural Darwinist approach. I propose that the difference in predictive accuracy for different representations provides the feedback mechanism by which competing representations are selected. Repercussions for cognitive modeling of music are discussed.
Top/Computer_Science/Semantic_Web	Where the Social Web Meets the Semantic Web The Semantic Web is an ecosystem of interaction among computer systems. The social web is an ecosystem of conversation among people. Both are enabled by conventions for layered services and data exchange. Both are driven by human-generated content and made scalable by machine-readable data. Yet there is a popular misconception that the two worlds are alternative, opposing ideologies about how the web ought to be. Folksonomy vs. ontology. Practical vs. formalistic. Humans vs. machines. This is nonsense, and it is time to embrace a unified view. I subscribe to the vision of the Semantic Web as a substrate for collective intelligence. The best shot we have of collective intelligence in our lifetimes is large, distributed human-computer systems. The best way to get there is to harness the 'people power' of the Web with the techniques of the Semantic Web. In this presentation I will show several ways that this can be, and is, happening.
Top/Computer_Science/Semantic_Web	Semantic text features from small world graphs We present a set of methods for creating a semantic representation from a collection of textual documents. Given a document collection we use a simple algorithm to connect the documents into a tree or a graph. Using the imposed topology we define a feature and document similarity measures. We use the kernel alignment to compare the quality of various similarity measures. Results show that the document similarity defined over the topology gives better alignment than standard cosine similarity measure on a bag of words document representation.
Top/Computer_Science/Semantic_Web	A short Tutorial on Semantic Web
Top/Computer_Science/Semantic_Web	Semantic Web Usage Mining Overview and Case Studies In this tutorial we will review fundamentals of web usage mining - theory, case studies and related topics. Web usage mining is a topic which became in the late 90ties one of the first profitable areas of data mining and which was necessity for the succesful e-commerce companies to understand better their customers, their behaviour and to optimize the e-services accordingly. In this tutorial lecture we will show several case studies which show approaches, techniques and results coming out of this area.
Top/Computer_Science/Semantic_Web	The Role of Semantic Web in Web 2.0: Partner or Follower? Currently, the web phenomenon that is driving the best developers and captivating the best entrepreneurs is Web 2.0. Web 2.0 encompasses some of today's most exciting web-based applications: mashups, blogs/wikis/feeds, interface remixes, and social networking/tagging systems. Although most Web 2.0 applications rely on an implicit, lightweight, shared semantics in order to deliver user value, by several metrics (number of startups funded, number of 'hype' articles in the trade press, number of conferences), Web 2.0 technologies are significantly outdistancing semweb technologies in both implementation and mindshare. Hackers are staying up late building mashups with AJAX and REST and microformats, and only rarely including RDF and OWL. This panel will consider whether semantic web technology has a role in Web 2.0 applications, in at least the context of the following areas: 1. Web 2.0 and Semantics: What unique value can semantic web technologies supply to Web 2.0 application areas? How do semantic web technologies match up with the semantic demands of Web 2.0 applications? 2. Semantics and Web 'Ecosystems': Web 2.0 applications often strive to build participatory ecosystems of content that is supplied and curated by their users. Can these users effectively create, maintain, map between, and use RDF/OWL content in a way that reinforces the ecosystem? 3. Semantic Web in Practice: Does semantic web technology enable the cost-effective creation of Web 2.0 applications that are simple, scalable, and compelling for a targeted user community? Can semantic web technology genuinely strengthen Web 2.0 applications, or will it just be a footnote to the Web 2.0 wave?
Top/Computer_Science/Semantic_Web	Data Mining Vs. Semantic Web This tutorial covers the field of datamining in general, talks about its possible applications (special case studies can be added on request), and elaborates on the issue of hardware accelerators for datamining. The introduction gives a formal and an informal definition (through an example), plus it points to possible missunderstandings typical of the topic. The part on methods and algorithms covers a number of different approaches, each one presented thru animation, using the examples that are both colourfull and unusual, but excellent for pointing into the essence. The part on tools lists about a dozen different tools, and selects one for a detailed case study. The part on applications includes examples from a variety of different fields (engineering, science, medicine, psychiatry, etc...) The part on hardware accelerators is available on special request. This tutorial was presented so far many times for industry and academia in the USA and Europe, and received the best tutorial award at several conferences.
Top/Computer_Science/Semantic_Web	A short Tutorial on Semantic Web The availability of electronically stored information increased drastically through the development of the World Wide Web. Currently the WWW contains more than a billion documents, but support for accessing and precessing information is limited. Most information is only presentable but not understandable by computers. Tim Berners-Lee envisioned the Semantic Web that aims at providing automated access to information due to machine-processable semantics of data. Ontologies formalize a shared understanding of a domain and therefore play a crucial role for communication among human beings and software agents. We will present the underlying ideas of the Semantic Web and will shortly introduce ontologies as the backbone of the Semantic Web. Further we will show how much effort is necessary to setup the Semantic Web and how tools can support this process. Additionally Web and Data Mining techniques can be used to bootstrap the Semantic Web. The idea of Semantic Web Mining is to improve the results of Web Mining by exploiting the new semantic structures in the web.
Top/Computer_Science/Semantic_Web	Industry 1: Semantic Solutions: Generating Business Value from Semantic Web Technologies Thanks to the efforts of many researchers over the past five years, Semantic Web technologies have reached the point where they are now enabling new types of business solutions. In this presentation, we will show how IBM Research is using Semantic Web technologies and Semantic Super Computing to generate new insight from the Web, intranets and large document repositories. It includes an introduction and overview of the use of Semantic Super Computing to automatically identify, index and augment semantic information, covers relevant foundational technologies and concludes with a case study of one emerging application of the technology that we refer to as the Wikification of Corporate Corpora
Top/Computer_Science/Semantic_Web	Usability and the Semantic Web In addition to its technical implications, the semantic web vision gives rise to some challenges concerning usability and interface design. What difficulties can arise when persons with little or no relevant training try to * formulate knowledge (e.g., with ontology editors or annotation tools) in such a way that it can be exploited by semantic web technologies; * leverage semantic information while querying or browsing? What strategies have been applied in an effort to overcome these difficulties, and what are the main open issues that remain? This talk will address these questions, referring to examples and results from a variety of research efforts, including the project SemIPort, which concerns semantic methods and tools for information portals, and Halo 2, in which tools have been developed and evaluated that enable scientists to formalize and query college-level scientific knowledge.
Top/Computer_Science/Semantic_Web	Where Does It Break? Or: Why the Semantic Web is Not Just 'Research as Usual' Work on the Semantic Web is all to often phrased as a technological challenge: how to improve the precision of search engines, how to personalise web-sites, how to integrate weakly-structured data-sources, etc. This suggests that we will be able to realise the Semantic Web by merely applying (and at most refining) the results that are already available from many branches of Computer Science. I will argue in this talk that instead of (just) a technological challenge, the Semantic Web forces us to rethink the foundations of many subfields of Computer Science. This is certainly true for my own field (Knowledge Representation), where the challenge of the Semantic Web continues to break many often silently held and shared assumptions underlying decades of research. With some caution, I claim that this is also true for other fields, such as Machine Learning, Natural Language Processing, Databases, and others. For each of these fields, I will try to identify silently held assumptions which are no longer true on the Semantic Web, prompting a radical rethink of many past results from these fields.
Top/Computer_Science/Semantic_Web	The Semantic Web and Networked Governance: Promise and Challenges The virtual state is a metaphor meant to draw attention to the structures and processes of the state that are becoming increasingly aligned with the structures and processes of the semantic web. Semantic Web researchers understand the potential for information sharing, enhanced search, improved collaboration, innovation, and other direct implications of contemporary informatics. Yet many of the broader democratic and governmental implications of increasingly networked governance remain elusive, even in the world of public policy and politics.
Top/Computer_Science/Semantic_Web	The Semantic Web: Suppliers and Customers The notion of the Semantic Web can be coined as a Web of data when bringing database content to the Web or as a Web of enriched human-readable content when encoding the semantics of web-resources in a machine-interpretable form. It has been clear from the beginning that realizing the Semantic Web vision will require interdisciplinary research. At this the fifth ISWC, it is time to re-examine the extent to which interdisciplinary work has played and can play a role in Semantic Web research, and even how Semantic Web research can contribute to other disciplines. Core Semantic Web research has drawn from various disciplines, such as knowledge representation and formal ontologies, reusing and further developing their techniques in a new context.
Top/Computer_Science/Semantic_Web	Annotation for the Semantic Web
Top/Computer_Science/Semantic_Web	Contextual Intelligence for Mobile Services through Semantic Web Technology
Top/Computer_Science/Semantic_Web	Dinamic Assembly of personalised Learning Content on the Semantic web
Top/Computer_Science/Semantic_Web	Dynamic Assembly of Personalized Learning Content on the Semantic Web
Top/Computer_Science/Semantic_Web	First order logic for learning user models in the semantic web: why do we need it? In this paper we claim that learning in a first order logic framework is crucial for the future of user modeling applications in the context of the Semantic Web (SW in the remaining of the paper). We first present some works that have currently been done for designing first order logic based languages, for reasoning in the SW. In the context of user modeling in the SW, it would then be relevant to use such languages to model users behaviors and preferences. We show that discovering knowledge in the context of such languages could be done using multi-relational data mining that has already provided efficient prototypes. Nevertheless, some work remains to be done in order to use them in that context and we give some directions for that purpose.
Top/Computer_Science/Semantic_Web	Funding the Semantic Web : A cross-continental assessment and outlook In the recent years semantic technologies have demonstrated their usefulness and applicability in a variety of domains, the Semantic Web being the most prominent one. The Semantic Web has started to move from academic research to deployed business-critical and scientific applications, with support from recommendations (standards) developed under W3C governance and a growing list of commercial technologies and products is being developed. These developments seem to be early but firm steps in establishing semantics as a core column of computer science and application development. The outreach of this development can only be assessed to limited degree at the moment, but most likely will affect key aspects of society and the way we communicate. This high potential was recognized early by funding agencies all over the world. However, after the first strong funding in US by DARPA, subsequent research funding seems to be limited. Europe seems to have seem more substantial and sustained funding, at least during last few years. Now may be a good time to assess what has been achieved so far and how funding agencies see future research directions, funding opportunities and funding environments, i.e., what are the planned strategies and instruments of funding agencies to maximize the impact of future research in semantics. We consider it specifically interesting to the research community to hear the opinions and plans of the major funding bodies around the world and to learn about their view on future issues/requirements/applications/challenges related to semantics and Semantic Web-- and by extension their opinion on the needs of industry, government and education for research in the Semantic Web and related areas.
Top/Computer_Science/Semantic_Web	Human Language technology for the Semantic Web In this talk I will present an overview of Human Language Technology (HLT) and its use in Semantic Web development. HLT is concerned with automatic linguistic processing towards the semantic analysis and extraction of information from textual data. In the context of the Semantic Web the use of HLT is in knowledge markup of web documents for ontology population and text mining for ontology evolution (extension and modification of ontology models). The talk will include examples of both as currently developed in the context of the SmartWeb project on 'Mobile Broadband Access to the Semantic Web' - http://www.smartweb-projekt.de/
Top/Computer_Science/Semantic_Web	Human Language Technology for the Semantic Web
Top/Computer_Science/Semantic_Web	Human Language Technology for the Semantic Web
Top/Computer_Science/Semantic_Web	Industry 3: Knowledge Representation in Practice: Project Halo and the Semantic Web Vulcan's Project Halo is an ambitious, multiyear research program to develop a detailed scientific knowledge base that can answer AP-level questions and provide explanations in a user-appropriate manner. It is one of the largest AI research programs in the US today. Halo's current focus is building AI tools that allow graduate students in chemistry, biology, and physics to author scientific knowledge adequate to answer sophisticated natural-language questions without relying on trained knowledge engineers. Halo researchers have been working to link Semantic Web technology with the other knowledge representations in the system. This talk will lay out Halo's technologies and results to date, and describe the technical and UI issues we have faced in getting users to author scientific conceptual knowledge.
Top/Computer_Science/Semantic_Web	Industry 3: Semantic Web @ W3C: Activities, Recommendations and State of Adoption The presentation presents the status of the Semantic Web from W3C's perspective, referring to the finished and active works in terms of W3C groups, view of available tools, and ideas floating around for possible future work. References and examples of real Semantic Web applications will also show the widening adoption of the technology by the industrial community.
Top/Computer_Science/Semantic_Web	In-Use 1: NEWS: bringing Semantic Web Technologies into News Agencies
Top/Computer_Science/Semantic_Web	In-Use 1: Semantic web technology for expert knowledge sharing and discovery
Top/Computer_Science/Semantic_Web	In-Use 2: Information Integration via an End-to-End Distributed Semantic Web System
Top/Computer_Science/Semantic_Web	In-Use 4: A Mixed Initiative Semantic Web Framework for Process Composition
Top/Computer_Science/Semantic_Web	Is Semantic Web technology ready for Healthcare?
Top/Computer_Science/Semantic_Web	Lexical Semantics in the Age of the Semantic Web
Top/Computer_Science/Semantic_Web	Making Semantic Web Real
Top/Computer_Science/Semantic_Web	Metadata Extraction: Human Language technology and the Semantic Web
Top/Computer_Science/Semantic_Web	Metadata Extraction: Human Language technology and the Semantic Web - Part 1
Top/Computer_Science/Semantic_Web	Metadata Extraction: Human Language technology and the Semantic Web - Part 3
Top/Computer_Science/Semantic_Web	Metadata Extraction: Human Language technology and the Semantic Web - Part 4
Top/Computer_Science/Semantic_Web	Metadata Extraction: Human Language technology and the Semantic Web - Part 5
Top/Computer_Science/Semantic_Web	Metadata Extraction: Human Language technology and the Semantic Web - Part 6
Top/Computer_Science/Semantic_Web	Metadata Extraction: Human Language technology and the Semantic Web part 7
Top/Computer_Science/Semantic_Web	Research 11: PowerMap: Mapping the Real Semantic Web on the Fly
Top/Computer_Science/Semantic_Web	Research 12: A Software Engineering Approach to Design and Development of Semantic Web Service Applications
Top/Computer_Science/Semantic_Web	Research 12: IRS-III: A Broker for Semantic Web Services based Applications
Top/Computer_Science/Semantic_Web	Research 12: RS2D: Fast Adaptive Search for Semantic Web - Services in Unstructured P2P Networks
Top/Computer_Science/Semantic_Web	Research 13: Crawling and Indexing Semantic Web Data
Top/Computer_Science/Semantic_Web	Research 8: A Browser for Heterogeneous Semantic Web Repositories
Top/Computer_Science/Semantic_Web	Semantic Desktop and Social Semantic Collaboration: Open Constitution Based Knowledge Communities in the Semantic Web
Top/Computer_Science/Semantic_Web	Semantic Web: Next Generation Web Services and Knowledge Management
Top/Computer_Science/Semantic_Web	Semantic Web(s) and Language Technology
Top/Computer_Science/Semantic_Web	Semantic Web Service Systems - Part 1 The proposed tutorial presents the Web Service Execution Environment WSMX and the Internet Reasoning Server IRS as an integrated environment for development and execution of Semantic Web Services on basis of the Web Service Modeling Ontology WSMO.
Top/Computer_Science/Semantic_Web	Semantic Web Service Systems - Part 2 The proposed tutorial presents the Web Service Execution Environment WSMX and the Internet Reasoning Server IRS as an integrated environment for development and execution of Semantic Web Services on basis of the Web Service Modeling Ontology WSMO.
Top/Computer_Science/Semantic_Web	Semantic web technology Roadmap
Top/Computer_Science/Semantic_Web	Workshop: Spatial Integration of Semantic Web Services: the e-Merges Approach
Top/Computer_Science/Semantic_Web	Semantic Web Rules with Ontologies, and their E-Services Applications Rules are a main emerging area of the Semantic Web. There has been significant progress in just the last three years in several aspects of Semantic Web rules. This includes exciting developments in the underlying knowledge representation formalisms as well as advances in integration of rules with ontologies; translations between heterogeneous commercial rule engines; development of open-source tools for inferencing and interoperability; standards proposals and efforts (including RuleML, SWRL, Semantic Web Service Framework, and recently W3C Rule Interchange Format); proposals for rule-based semantic Web services; and pilot applications in the emerging area of e-services. This tutorial will provide an introduction to these developments and will explore techniques, applications, and challenges. We will also touch upon the issues of business value, adoption, investment, and strategy considerations.
Top/Computer_Science/Semantic_Web	Semantic Overlay Networks for P2P Web Search
Top/Computer_Science/Semantic_Web	WWW Challenges - Publishing, Searching, and Browsing on the Web Web is a highly distributed and dynamic environment that poses a number of challenges to designers and users of Web services and applications. In our research we promote a user centric approach to studying Web related issues and designing solutions that would improve the user's experience. We show how an exploratory study of users' behaviour led to new opportunities for modifying and improving Web publishing practices and browser features. We present the findings from quantitative and qualitative analyses of the users' logs and the impact they had on the prototype design. We point to the depth of problem understanding that has been achieved through the combined approach of statistical analysis of user logs and user interviews.
Top/Computer_Science/Semantic_Web	Text Mining and Link Analysis for Web and Semantic Web The tutorial on Text Mining and Link Analysis for Web Data will focus on two main analytical approaches when analyzing web data: text mining and link analysis for the purpose of analyzing web documents and their linkage. First, the tutorial will cover some basic steps and problems when dealing with the textual and network (graph) data showing what is possible to achieve without very sophisticated technology. The idea of this first part is to present the nature of un-structured and semi-structured data. Next, in the second part, more sophisticated methods for solving more difficult and challenging problems will be shown. In the last part, some of the current open research issues will be presented and some practical pointers on the available tolls for solving previously mentioned problems will be provided.
Top/Computer_Science/Semantic_Web	From Mining the Web to Inventing the New Sciences Underlying the Internet As the Internet continues to change the way we live, find information, communicate, and do business, it has also been taking on a dramatically increasing role in marketing and advertising. Unlike any prior mass medium, the Internet is a unique medium when it comes to interactivity and offers ability to target and program messaging at the individual level. Coupled with its uniqueness in the richness of the data that is available for measurability, in the variety of ways to utilize the data, and in the great dependence of effective marketing on applications that are heavily data-driven, makes data mining and statistical data analysis, modeling, and reporting an essential mission-critical part of running the on-line business.
Top/Computer_Science/Semantic_Web	Web User Behavior Characterization: Techniques, Applications And Research Directions
Top/Computer_Science/Semantic_Web	Research 7: Web Service Composition via Generic Procedures & Customizing User Preferences
Top/Computer_Science/Semantic_Web	Context Sensitivity in Knowledge Rich Systems The main goal of this tutorial is to provide an extensive survey of the past and current work in the area of context related topics. This includes analysis of the past work: (1) defining the notion of context, (2) present logic-based formalisms for dealing with contexts, (3) present probabilistic/fuzzy approaches to model context, (4) demonstrate modelling the context and reasoning with contexts in real-life applications. In addition, the presented work we will provide a synthesis of the past work in the light of a unified categorization of context-related approaches along several dimensions which appear as relevant from theoretical and practical point of view.
Top/Computer_Science/Semantic_Web	Automatic Techniques for Extracting Semantic Data (from text and media) Using the results of a number of large European projects Fabio will outline the main issues for acquiring ontological knowledge from text and media at large scale. A lot of the work described here is based on collaborations with the jet engine company Rolls Royce. This session will give attendees insights into how information extraction techniques can support the population of ontologies with data.
Top/Computer_Science/Semantic_Web	Learning to Distinguish Valid Textual Entailments This paper proposes a new architecture for textual inference in which finding a good alignment is separated from evaluating entailment. Current approaches to semantic inference in question answering and textual entailment have approximated the entailment problem as that of computing the best alignment of the hypothesis to the text, using a locally decomposable matching score. While this formulation is adequate for representing local (word-level) phenomena such as synonymy, it is incapable of representing global interactions, such as that between verb negation and the addition/removal of qualifiers, which are often critical for determining entailment.
Top/Computer_Science/Semantic_Web	Qualitative Spatial Relationships for Image Interpretation by using Semantic Graph In this paper, a new way to express complex spatial relations is proposed in order to integrate them in a Constraint Satisfaction Problem with bilevel constraints. These constraints allow to build semantic graphs, which can describe more precisely the spatial relations between subparts of a composite object that we look for in an image. For example, it allows to express complex spatial relations such as is surrounded by. This approach can be applied to image interpretation and some examples on real images are presented.
Top/Computer_Science/Semantic_Web	Context Sensitivity in Knowledge Rich Systems The main goal of this tutorial is to provide an extensive survey of the past and current work in the area of context related topics. This includes analysis of the past work: (1) defining the notion of context, (2) present logic-based formalisms for dealing with contexts, (3) present probabilistic/fuzzy approaches to model context, (4) demonstrate modelling the context and reasoning with contexts in real-life applications. In addition, the presented work we will provide a synthesis of the past work in the light of a unified categorization of context-related approaches along several dimensions which appear as relevant from theoretical and practical point of view.
Top/Computer_Science/Semantic_Web	Context Sensitivity in Knowledge Rich Systems The main goal of this tutorial is to provide an extensive survey of the past and current work in the area of context related topics. This includes analysis of the past work: (1) defining the notion of context, (2) present logic-based formalisms for dealing with contexts, (3) present probabilistic/fuzzy approaches to model context, (4) demonstrate modelling the context and reasoning with contexts in real-life applications. In addition, the presented work we will provide a synthesis of the past work in the light of a unified categorization of context-related approaches along several dimensions which appear as relevant from theoretical and practical point of view.
Top/Computer_Science/Semantic_Web	In-Use 2: Semantic Desktop 2.0: The Gnowsis Experience
Top/Computer_Science/Semantic_Web	Knowledge Management in the Petroleum Industry
Top/Computer_Science/Semantic_Web	How I was right even when I was wrong For the past several years I have warned people not to ask me to predict the future, because my predictions are usually wrong. Undaunted by failure, in this talk I will try to predict the future of the semantic web based on a very personal view of its history, the history of the internet, web, semantic web, and AI, and the mistakes I've made predicting where and how they would be valuable.
Top/Computer_Science/Semantic_Web	(Semantic) Structure in Structured Document Retrieval
Top/Computer_Science/Semantic_Web	Industry 1: Managing Richly Connected Information We examine research issues that arise when most information items in an enterprise can be linked to each other via short paths, implicit or explicit. In such high-recall settings, the treatment of metadata management, indexing and ranking needs new attention. Additional issues arise as to the best way to handle updates to the connections, whether on or off the transaction path. Even traditional techniques, such as classification and clustering of documents, which stand to benefit from the extra information provided by the so-called network of meaning, need to be reexamined for how best to exploit the extra information. The talk ends with an examination of some promising avenues for using high recall as a driver for the next wave of business process automation
Top/Computer_Science/Semantic_Web	Data Integration using Semantic Technology: a Use Case
Top/Computer_Science/Semantic_Web	Building semantic applications
Top/Computer_Science/Semantic_Web	Research 15: Automatic Annotation of Web Services based on Workflow Definitions
Top/Computer_Science/Semantic_Web	POWERSET - Natural Language and the Semantic Web The **Semantic Web** promises to revolutionize access to information by adding machine-readable semantic information to content which is normally interpretable only by people. In addition, it will also revolutionize access to services by adding semantic information to create machine-readable service descriptions. This ambitious vision has been slow to take off because of a chicken and egg problem. Markup is required before people will build applications, applications are required before it is worth the hard work of doing markup. **Natural language processing (NLP)** has advanced to the point where it can break the impasse and open up the possibilities of the Semantic Web. First, NLP systems can now automatically create annotations from unstructured text. This provides the data that semantic web applications require. Second, NLP systems are themselves consumers of semantic web information and thus provide economic motivation for people to create and maintain such information. For example, a new generation of natural language search systems, as illustrated by Powerset, can take advantage of semantic web markup and ontologies to augment their interpretation of underlying textual content. They can also expose semantic web services directly in response to natural language queries.
Top/Computer_Science/Semantic_Web	Universal Access to Human Knowledge (Or Public Access to Digital Materials) The goal of universal access to our cultural heritage is within our grasp. With current digital technology we can build comprehensive collections, and with digital networks we can make these available to students and scholars all over the world. The current challenge is establishing the roles, rights, and responsibilities of our libraries and archives in providing public access to this information. With these roles defined, our institutions will help fulfill this epic opportunity of our digital age.
Top/Computer_Science/Semantic_Web	Extracting Semantic Relations from Query Logs In this paper we study a large query log of more than twenty million queries with the goal of extracting the semantic relations that are implicitly captured in the actions of users submitting queries and clicking answers. Previous query log analyses were mostly done with just the queries and not the actions that followed after them. We first propose a novel way to represent queries in a vector space based on a graph derived from the query-click bipartite graph. We then analyze the graph produced by our query log, showing that it is less sparse than previous results suggested, and that almost all the measures of these graphs follow power laws, shedding some light on the searching user behavior as well as on the distribution of topics that people want in the Web. The representation we introduce allows to infer interesting semantic relationships between queries. Second, we provide an experimental analysis on the quality of these relations, showing that most of them are relevant. Finally we sketch an application that detects multitopical URLs.
Top/Computer_Science/Semantic_Web	A Self-Organizing Map for Relation Extraction from Wikipedia using Structured Data Representations
Top/Computer_Science/Semantic_Web	Efficient Lazy Algorithms for Minimal-Interval Semantics
Top/Computer_Science/Semantic_Web	Ground Facts, Rules and Probabilistic Inference for Cyc One aspect of Cyc is a very large, logic-based knowledge base that includes, inter-alia, large amounts of background knowledge over a wide variety of domains, but it is more than that; the Cyc project is an attempt to move towards general artificial intelligence by supporting automated reasoning about a very wide variety of real-world concerns. To support that goal, Cyc also encompasses, obviously enough, and inference engine able to reason over a large, contextual, knowledge base, but it also includes components for interpreting and producing natural language, acquiring knowledge and responding to user queries, and for interfacing with other software. Applying logic to representation of general knowledge, /at scale/, and using it in the production of intelligent behaviors has been difficult enough; unfortunately it is becoming clear that doing so using traditional logics is probably not sufficient, either for satisfying a long term goal of supporting general intelligence, or even for shorter term goals, like recognizing, interpreting, and elaborating descriptions of piracy events. In this talk, I'll briefly describe what Cyc is, and has been, and how it is growing, touch on an early approach to abductive reasoning and classification in a traditional logical framework, and some difficulties with that approach, and then describe recent, very initial work training the Markov Logic networks based on ground facts and rules within the millions of axioms of the Cyc KB. Finally I'll sketch a vision for a system that truly integrates both sound, deductive reasoning, and the bounded unsoundness of probabilistic classification, induction, abduction and deduction.
Top/Computer_Science/Semantic_Web	A Concept-based Model for Enhancing Text Categorization Most of text categorization techniques are based on word and/or phrase analysis of the text. Statistical analysis of a term frequency captures the importance of the term within a document only. However, two terms can have the same frequency in their documents, but one term contributes more to the meaning of its sentences than the other term. Thus, the underlying model should indicate terms that capture the semantics of text. In this case, the model can capture terms that present the concepts of the sentence, which leads to discover the topic of the document. A new concept-based model that analyzes terms on the sentence and document levels rather than the traditional analysis of document only is introduced. The concept-based model can effectively discriminate between non-important terms with respect to sentence semantics and terms which hold the concepts that represent the sentence meaning. The proposed model consists of concept-based statistical analyzer, conceptual ontological graph representation, and concept extractor. The term which contributes to the sentence semantics is assigned two different weights by the concept-based statistical analyzer and the conceptual ontological graph representation. These two weights are combined into a new weight. The concepts that have maximum combined weights are selected by the concept extractor. A set of experiments using the proposed concept-based model on different datasets in text categorization is conducted. The experiments demonstrate the comparison between traditional weighting and the concept-based weighting obtained by the combined approach of the concept-based statistical analyzer and the conceptual ontological graph. The evaluation of results is relied on two quality measures, the Macro-averaged F1 and the Error rate. These quality measures are improved when the newly developed concept-based model is used to enhance the quality of the text categorization
Top/Computer_Science/Semantic_Web	Industry 3: How Co-Occurrence can Complement Semantics? Analysis of texts is an obvious way for semantic annotation and extraction of structured knowledge. A basic task is the recognition of references to entities (people, locations, organizations, etc). A next step is relation extraction, e.g. identifying that an organization is located in a particular city. Automatic extraction of such relations is a tough linguistic problem - the solutions are either very partial, expensive to implement, or slow. On the other hand, relationships are crucial for the usability of the extracted knowledge for navigation and search purposes. We demonstrate how efficient co-occurrence analysis, performed on top of semantic annotation, can be used for several purposes: relation extraction, faceted search, and popularity timelines. The faceted search interface allows an easy way for augmenting full-text search by means of entity references, derived through co-occurrence profiling and semantic relationships. Although this sort of analytics can be used in virtually any domain, their development within the KIM platform was driven by the requirements for news analysis and research. We demonstrate the usage of these interfaces on top of 1 million news articles - a corpus of the major international news for the last five years. This sort of co-occurrence analysis has the potential of aiding identity resolution, which is recognized to be a crucial problem for several tasks: cross-document co-reference resolution, record linkage, object linking, and data integration.
Top/Computer_Science/Semantic_Web	In-Use 3: Towards Semantic Interoperability in a Clinical Trials Management System
Top/Computer_Science/Semantic_Web	Semantic Desktop and Social Semantic Collaboration: SemDAV: A File Exchange Protocol for the Semantic Desktop
Top/Computer_Science/Semantic_Web	Research 1: Extracting Relations in Social Networks from Web using Similarity between Collective Contexts
Top/Computer_Science/Semantic_Web	Research 15: Provenance Explorer Tailored Provenance Views Using Semantic Inferencing
Top/Computer_Science/Semantic_Web	Idea Navigation: Structured Browsing for Unstructured Text Dont search for keywords! Search for ideas! Our system extracts subject-verb-object triples from unstructured text, groups them into hierarchies, and allows iterative refinement to findexactly what you want.
Top/Computer_Science/Semantic_Web	Making Value Out of Semantic Web Data
Top/Computer_Science/Semantic_Web	Approaching Textual Entailment with LFG and FrameNet Frames We present a baseline system for modeling textual entailment that combines deep syntactic analysis with structured lexical meaning descriptions in the FrameNet paradigm. Textual entailment is approximated by degrees of structural and semantic overlap of text and hypothesis, which we measure in a match graph. The encoded measures of similarity are processed in a machine learning setting.
Top/Computer_Science/Semantic_Web	Ontologies and Machine Learninig We address the problem of constructing light-weight ontology from social network data. As an example we use social network of a mid size research institution obtained based on e-mail communication. The main contribution is an architecture consisting from five major steps that enable transformation of the data from a given e-mail transactions recordings to an ontology estimating the structure of the organization. Once having a set of sparse vectors, we apply an approach to semi-automated ontology construction as implemented in the OntoGen tool. The experiments and illustrative evaluation show that our approach is useful and applicable in real life situations where the goal is to model social structures based on communication records.
Top/Computer_Science/Semantic_Web	Architecture Proposal
Top/Computer_Science/Semantic_Web	Ontology-based Information Extraction for Business Intelligence Business Intelligence (BI) requires the acquisition and aggregation of key pieces of knowledge from multiple sources in order to provide valuable information to customers or feed statistical BI models and tools. The massive amount of information available to business analysts makes information extraction and other natural language processing tools key enablers for the acquisition and use of that semantic information. We describe the application of ontology-based extraction and merging in the context of a practical e-business application for the EU MUSING Project where the goal is to gather international company intelligence and country/region information. The results of our experiments so far are very promising and we are now in the process of building a complete end-to-end solution.
Top/Computer_Science/Semantic_Web	Free Semantic Content: Using OpenCyc in Semantic Web Applications OpenCyc will be more accessible and Semantic Web interoperability will be enhanced if users are able to access just the parts of OpenCyc they need. The tutorial will describe how Semantic Web researchers and practitioners can benefit from integrating their representations with the extensive upper and middle level ontological content of the free and unrestricted OpenCyc knowledge base, and other integrative vocabularies like Okkam. The syntax of OpenCyc will be described both in raw form, and as mapped onto Semantic Web standard languages, and the content of the knowledge base will be described in overview. Based on that, well show how to extend the OpenCyc KB for user applications, and how to make use of it in a web-services environment to support knowledge integration, and simple machine learning applications. Finally, well demonstrate the use of the OpenCyc vocabulary to support a broad-applicability knowledge capture application, illustrative of the transition from Web2.0 to Web3.0. Hands on exercises will be used to illustrate knowledge use and construction, use of OpenCyc with inference, and use for semantic search over text in a web services environment.
Top/Computer_Science/Semantic_Web	Introduction to the Semantic Web
Top/Computer_Science/Semantic_Web	Recipes for Semantic Web Dog Food - The ESWC and ISWC Metadata Projects Semantic Web conferences such as ESWC and ISWC offer prime opportunities to test and showcase semantic technologies. Conference metadata about people, papers and talks is diverse in nature and neither too small to be uninteresting or too big to be unmanageable. Many metadata-related challenges that may arise in the Semantic Web at large are also present here. Metadata must be generated from sources which are often unstructured and hard to process, and may originate from many different players, therefore suitable workflows must be established. Moreover, the generated metadata must use appropriate formats and vocabularies, and be served in a way that is consistent with the principles of linked data. This paper reports on the metadata efforts from ESWC and ISWC, identifies specific issues and barriers encountered during the projects, and discusses how these were approached. Recommendations are made as to how these may be addressed in the future, and we discuss how these solutions may generalize to metadata production for the Semantic Web at large.
Top/Computer_Science/Semantic_Web	What Semantic Web researchers need to know about Machine Learning? The tutorial will cover basic topics from the field of Machine Learning explained in an intuitive way relevant for Semantic Web researchers and practitioners. In the first part the topics will cover brief top level overview of the Machine Learning field, its algorithms, and data types being analyzed. In the second part we will cover relation to Semantic Web and Web2.0. In the last part we will perform hands-on exercise with some of the tools for modeling text semantics and social networks in analytical way.
Top/Computer_Science/Semantic_Web	Text mining for semantically enabled social browsing
Top/Computer_Science/Semantic_Web	Term Dependence on the Semantic Web A large amount of terms (classes and properties) have been published on the Semantic Web by various parties, to be shared for describing resources. Terms are defined based on other terms, and thus a directed dependence relation is formed. The study of term dependence is a foundation work and is important for many other tasks, such as ontology maintenance, integration, and distributed reasoning on the Web scale. In this paper, we analyze the complex network characteristics of the term dependence graph and the induced vocabulary dependence graph. The graphs analyzed in the experiments are constructed from a large data set that contains 1,278,233 terms in 3,039 vocabularies. The results characterize the current status of schemas on the Semantic Web in many aspects, including degree distributions, reachability, and connectivity.
Top/Computer_Science/Semantic_Web	Semantic Web-based E-commerce
Top/Computer_Science/Semantic_Web	Message in a Bottle or: How can the Semantic Web Community be more convincing? Enormous resources are poured into projects like the Large Hadron Collider, the Hubble space telescope, or the Iter fusion reactor. Computer science resources pale in comparison the European Semantic Web effort is tiny compared to those projects. Why is this the case? Does the Semantic Web (or computer science in general) promise less impact or relevance than those Physics projects? In my talk I will argue that the Physicists are much better in formulating an engaging mission and message. Especially the Semantic Web community has not been very good in coming up with a convincing mission directed to the public. We need to and we can do better. I will formulate requirements and a starting point for such a message and investigate ongoing seemingly unrelated research areas and trends on the Semantic Web like Semantic Sensor Networks, Social Semantic Desktop and Semantic Publishing and how they contributes to a better conveyable mission.
Top/Computer_Science/Semantic_Web	Freebase: An Open, Writable Database of the Worlds Information Freebase is an open database of the worlds information, built by a global community and free for anyone to query, contribute to, and build applications on. Drawing from large open data sets like Wikipedia, MusicBrainz, GNIS, EDGAR etc., Freebase is curated by a passionate community of users and contains structured information on millions of topics such as people, places, music, film, food, science, historical events, and more. Part of what makes this open database unique is that it spans domains, but requires that a particular topic exist only once in Freebase. Thus freebase is an identity database with a user contributed schema which spans multiple domains. For example, Arnold Schwarzenegger may appear in a movie database as an actor, a political database as a governor, and in a bodybuilder database as Mr. Universe. In Freebase, however, there is only one topic for Arnold Schwarzenegger that brings all these facets together. The unified topic is a single reconciled identity, which makes it easier to find and contribute information about the linked world we live in.
Top/Computer_Science/Semantic_Web	Network Structure of Folksonomies Folksonomies can be viewed as three mode graphs or as graphs made up of nodes (tags, users, resources) connected by hyper-edges. I shall report on some network statistical properties of a folksonomy graph based on data collected for the del.icio.us system. Moreover, by introducing a suitable distance between resources based on tag co-occurrence, I shall show that folksonomies embed a meaningful semantic clusterization of resources.
Top/Computer_Science/Semantic_Web	Beyond the Semantic Web There are multiple sources of power available for forming and propelling automobiles; analogously, there are several sources of power for forming and propelling thoughts. Besides the neural ones you're most familiar with, and the Semantic Web ones that have received the lion's share of hype in recent years, there are some additional ones that we are tapping into with some success. These deep semantic representations and operations are able to produce useful and in cases even novel conclusions requiring induction, abduction, and analogy, as well as deductive reasoning. I will illustrate this with case examples from recent Cyc applications, including terrorism scenario generation for intelligence analysts and ad hoc clinical trial question answering for medical researchers.
Top/Computer_Science/Semantic_Web	Web mining for natural language engineering tasks
Top/Computer_Science/Semantic_Web	OntoGame: Weaving the Semantic Web by Online Games
Top/Computer_Science/Semantic_Web	Using the Web to Reduce Data Sparseness in Pattern-based Information
Top/Computer_Science/Semantic_Web	2014: Semantic Technologies in Large Distributed Organisations
Top/Computer_Science/Semantic_Web	Collective Semantics: Collective Intelligence & the Semantic Web - Semantically enriching folksonomies with flor Web 2.0 has introduced new style of information sharing featuring mass user participation, social networking, heterogeneity of data sources, and a huge scale of information and knowledge, posing difficulties in discovering relevant information. The Semantic Web may contribute by providing a language basis and ontologies to support structuring, or introducing new ways to explore the information space. This may be achieved by combining semantics from semantic web resources with structure of the sharing platforms (tags, social acquaintances etc.) and automatic content analysis tools. This workshop targets integration arising from the mining of Web 2.0 information, multimedia content and knowledge with help of the Semantic Web.
Top/Computer_Science/Network_Analysis	Modeling real-world networks using Kronecker multiplication Given a large, real graph, how can we generate a synthetic graph that matches its properties, i.e., it has similar degree distribution, similar (small) diameter, similar spectrum, etc? First, we propose a graph generator that is mathematically tractable and generates realistic graphs. The main idea is to use a non-standard matrix operation, the Kronecker product, to generate graphs that we refer to as ''Kronecker graphs''. We show that Kronecker graphs naturally obey all the above properties; in fact, we can rigorously prove that they do so. Once we have the model, we fit it to real graph to generate a synthetic graph that matches its properties, i.e., it has similar degree distribution, similar (small) diameter, similar spectrum, etc? We present a fast and scalable algorithm for fitting the Kronecker graph generation model to real networks. A naive approach to fitting would take super-exponential time. In contrast, our algorithm takes linear time, by exploiting the structure of Kronecker matrix multiplication and by using sampling. Experiments on large real and synthetic graphs show that our approach recovers the true parameters and indeed mimics very well the patterns found in the target graphs. Once fitted, the model parameters and the resulting synthetic graphs can be used for anonymization, extrapolations, and graph summarization. //The presentation starts in Slovenian language and switches to English a few minutes into the lecture. Another lecture on the same topic can be found at [[icml07_leskovec_smrg]].//
Top/Computer_Science/Network_Analysis	Scalable Modeling of Real Graphs using Kronecker Multiplication Given a large, real graph, how can we generate a synthetic graph that matches its properties, i.e., it has similar degree distribution, similar (small) diameter, similar spectrum, etc? We propose to use 'Kronecker graphs', which naturally obey all of the above properties, and we present KronFit, a fast and scalable algorithm for fitting the Kronecker graph generation model to real networks. A naive approach to fitting would take super-exponential time. In contrast, KronFit takes linear time, by exploiting the structure of Kronecker product and by using sampling. Experiments on large real and synthetic graphs show that KronFit indeed mimics very well the patterns found in the target graphs. Once fitted, the model parameters and the resulting synthetic graphs can be used for anonymization, extrapolations, and graph summarization.
Top/Computer_Science/Network_Analysis	Learning from Network Traffic: Computing Kernels over Connection Content
Top/Computer_Science/Network_Analysis	Unsupervised Prediction of Citation Influences Publication repositories contain an abundance of information about the evolution of scientific research areas. We address the problem of creating a visualization of a research area that describes the flow of topics between papers, quantifies the impact that papers have on each other, and helps to identify key contributions. To this end, we devise a probabilistic topic model that explains the generation of documents; the model incorporates the aspects of topical innovation and topical inheritance via citations. We evaluate the model's ability to predict the strength of influence of citations against manually rated citations.
Top/Computer_Science/Network_Analysis	Recovering Temporally Rewiring Networks: A model-based approach A plausible representation of relational information among entities in dynamic systems such as a living cell or a social community is a stochastic network which is topologically rewiring and semantically evolving over time. While there is a rich literature on modeling static or temporally invariant networks, much less has been done toward modeling the dynamic processes underlying rewiring networks, and on recovering such networks when they are not observable. We present a class of hidden temporal exponential random graph models (htERGMs) to study the yet unexplored topic of modeling and recovering temporally rewiring networks from time series of node attributes such as activities of social actors or expression levels of genes. We show that one can reliably infer the latent timespecific topologies of the evolving networks from the observation. We report empirical results on both synthetic data and a Drosophila lifecycle gene expression data set, in comparison with a static counterpart of htERGM.
Top/Computer_Science/Network_Analysis	Information Dynamics in a Networked World The shift of communication to the internet, in particular to email, weblogs (blogs), and online communities, presents an opportunity to study the information dynamics of social networks on a large scale. Blogs, now numbering in the millions, are web pages updated using blogging software that makes it easy for authors to share new content online in the form of time-stamped posts. One can track how a piece of information spreads by observing when it appears on different blogs. The exact route the information takes is not obvious, since most blog authors will not explicitly identify the source of the information when they write about it. Likely routes can be inferred, however, by analyzing timing information, blogs' past entries, and the explicit network of blogs linking to one another through blogrolls or posts. While one can gain insights from observing how information passes from one individual to another, one can also analyze networks to see how easily one can actively navigate them to locate needed information or individuals. One test of the navigability of a network is the classic small world experiment, where subjects attempt to reach a target individual through their chain of acquaintances. Examining an email network within an organization reveals how individuals are capable of routing messages locally, even though their knowledge of the organization's global social network is limited.
Top/Computer_Science/Network_Analysis	Efficient and Decentralized PageRank Approximation in a P2P Web Search Network
Top/Computer_Science/Network_Analysis	Theoretical analysis of Link Analysis Ranking
Top/Computer_Science/Network_Analysis	An Event-based Framework for Characterizing the Evolutionary Behavior of Interaction Graphs Interaction graphs are ubiquitous in many fields such as bioinformatics, sociology and physical sciences. There have been many studies in the literature targeted at studying and mining these graphs. However, almost all of them have studied these graphs from a static point of view. The study of the evolution of these graphs over time can provide tremendous insight on the behavior of entities, communities and the flow of information among them. In this work, we present an event-based characterization of critical behavioral patterns for temporally varying interaction graphs. We use non-overlapping snapshots of interaction graphs and develop a framework for capturing and identifying interesting events from them. We use these events to characterize complex behavioral patterns of individuals and communities over time. We demonstrate the application of behavioral patterns for the purposes of modeling evolution, link prediction and influence maximization. Finally, we present a diffusion model for evolving networks, based on our framework.
Top/Computer_Science/Network_Analysis	Link analysis with pajek Pajek is a program (for Windows) for large network analysis and visualization. It is freely available for noncommercial use at [[http://vlado.fmf.uni-lj.si/pub/networks/pajek/|http://vlado.fmf.uni-lj.si/pub/networks/pajek/]] Besides ordinary networks Pajek supports also multi-relational and temporal networks. In large network analysis we are often interested in important parts of given network. There are several ways how to determine them. The islands approach is based on an importance measure of vertices or lines. Let (V,L,p) be a network with vertex property p : V ? R and let t be a real number. If we delete all vertices (and corresponding links) with the property value less than t, we get subnetwork called vertex-cut at level t. The number and sizes of its components depend on t. Often we consider only components of size at least k and not exceeding K. The components of size smaller than k are discarded as noninteresting, while the components of size larger than K are cut again at some higher level. Vertex-island is a connected subnetwork which vertices have greater property value than the vertices in its neighborhood. It is easy to see that the components of vertex-cuts are all vertex-islands. We developed an efficient algorithm that identifies all maximal vertex-islands of sizes in the interval k..K in a given network. For networks with weighted lines we can similarly define line-islands. The line-islands algorithm is based on line-cuts. Both algorithms are very general - they can be applied for any vertex/line importance measure. Their complexity is for sparse networks subquadratic - they can be applied to very large networks. We will illustrate them applying different importance measures on selected (large) networks. We will also present the use of pattern searching in analysis of genealogies and some approaches to analysis of (multi-relational) temporal networks.
Top/Computer_Science/Network_Analysis	Using Rank Propagation and Probabilistic Counting for Link-based Spam Detection
Top/Computer_Science/Network_Analysis	An Introduction to Network Theory and Spatial Networks
Top/Computer_Science/Network_Analysis	Learning a Distance Metric for Structured Network Prediction Man-made or naturally-formed networks typically exhibit a high degree of structural regularity. In this paper, we introduce the problem of structured network prediction: given a set of n entities and a desired distribution for connectivity, return a likely set of edges connecting the entities together in a network having the specified degree distribution. Prediction is useful for initializing a network, augmenting an existing network, and for filtering existing networks, when the structure of the network is known. In order to capture the inter-dependencies amongst pairwise predictions to learn parameters of our model, we build upon recent structured output models. Novel in our approach is the use of partially labeled training examples, and a network structure sensitive loss function. We present encouraging results of the model predicting equivalence graphs and links in a social network.
Top/Computer_Science/Network_Analysis	Cost-effective Outbreak Detection in Networks Which blogs should we read to avoid missing important information? Where should we place sensors in a water distribution network to quickly detect contaminants? These seemingly different problems share common structure: Outbreak detection can be modeled as a problem of selecting nodes (blogs, sensor locations, ...) in a network, in order to detect the spreading of a virus or information as quickly as possible. We present a general methodology for near optimal sensor placement in these and related problems. We demonstrate that many realistic outbreak detection objectives (e.g., detection likelihood, population affected) exhibit the property of submodularity. We exploit submodularity to develop an efficient algorithm that scales to large problems, provably achieving near optimal placements, while being 700 times faster than a simple greedy algorithm. We evaluate our approach on several large real-world problems, including a model of a water distribution network, and real blog data. We also show how the approach leads to deeper insights in both applications, answering multicriteria trade-off, cost-sensitivity and generalization questions. Joint work with: Andreas Krause, Carlos Guestrin, Christos Faloutsos, Jeanne VanBriesen and Natalie Glance Recepient of best student paper award at ACM SIGKDD 07 conference.
Top/Computer_Science/Network_Analysis	Dynamics of Real-world Networks In our recent work we found interesting and unintuitive patterns for time evolving networks, which change some of the basic assumptions that were made in the past. The main objective of observing the evolution patterns is to develop models that explain processes which govern the network evolution. Such models can then be fitted to real networks, and used to generate realistic graphs or give formal explanations about their properties. In addition, our work has a wide range of applications: we can spot anomalous graphs and outliers, design better graph sampling algorithms, forecast future graph structure and run simulations of network evolution. Another important aspect of this research is the study of 'local' patterns and structures of propagation in networks. We aim to identify building blocks of the networks and find the patterns of influence that these block have on information or virus propagation over the network. Our recent work included the study of the spread of influence in a large person-to-person product recommendation network and its effect on purchases. We also model the propagation of information on the blogosphere, and propose algorithms to efficiently find influential nodes in the network. Further work will include three areas of research. We will continue investigating models for graph generation and evolution. Second, we will analyze large online communication networks and devise models on how user characteristics and geography relate to communication and network patterns. Third, we will extend the work on the propagation of influence in recommendation networks to blogs on the Web, studying how information spreads over the Web by finding influential blogs and analyzing their patterns of influence. ; : [[http://www.cs.cmu.edu/~jure/thesis/]] ; Thesis Committee: : Christos Faloutsos (Chair) : Avrim Blum : John Kleinberg (Cornell University) : John Lafferty
Top/Computer_Science/Network_Analysis	SCAN: A Structural Clustering Algorithm for Networks Network clustering (or graph partitioning) is an important task for the discovery of underlying structures in networks. Many algorithms find clusters by maximizing the number of intra-cluster edges. While such algorithms find useful and interesting structures, they tend to fail to identify and isolate two kinds of vertices that play special roles&#160;- vertices that bridge clusters (hubs) and vertices that are marginally connected to clusters (outliers). Identifying hubs is useful for applications such as viral marketing and epidemiology since hubs are responsible for spreading ideas or disease. In contrast, outliers have little or no influence, and may be isolated as noise in the data. In this paper, we proposed a novel algorithm called SCAN (Structural Clustering Algorithm for Networks), which detects clusters, hubs and outliers in networks. It clusters vertices based on a structural similarity measure. The algorithm is fast and efficient, visiting each vertex only once. An empirical evaluation of the method using both synthetic and real datasets demonstrates superior performance over other methods such as the modularity-based algorithms.
Top/Computer_Science/Network_Analysis	Emergent Networks as Distributed Reputation System
Top/Computer_Science/Network_Analysis	Mining Large Graphs: Laws and Tools
Top/Computer_Science/Network_Analysis	Identifying the Influential Bloggers
Top/Computer_Science/Network_Analysis	Discovering and Tracking User Communities
Top/Computer_Science/Network_Analysis	Discovering and Tracking User Communities
Top/Computer_Science/Network_Analysis	Diffusion and Cascading Behaviour in Networks Diffusion is a process by which information, viruses, ideas and new behavior spread over the network. For example, adoption of a new technology begins on a small scale with a few early adopters, then more and more people adopt it as they observe friends and neighbors using it. Eventually the adoption of the technology may spread through the social network as an epidemic infecting most of the network. As it spreads over the network it creates a cascade. Cascades have been studied for many years by sociologists concerned with the diffusion of innovation; more recently, researchers have investigated cascades for selecting trendsetters for viral marketing, finding inoculation targets in epidemiology, and explaining trends in blogspace.
Top/Computer_Science/Network_Analysis	Link Analysis and Text Mining : Current State of the Art and Applications for Counter Terrorism The information age has made it easy to store large amounts of data.The proliferation of documents available on the Web, on corporate intranets, on news wires, and elsewhere is overwhelming. However, while the amount of data available to us is constantly increasing, our ability to absorb and process this information remains constant. Search engines only exacerbate the problem by making more and more documents available in a matter of a few key strokes. Link Analysis is a new and exciting research area that tries to solve the information overload problem by using techniques from data mining, machine learning, Information Extraction, Text Categorization, Visualization and Knowledge Management.
Top/Computer_Science/Network_Analysis	PhD Thesis Defense: Dynamics of large networks A basic premise behind the study of large networks is that interaction leads to complex collective behavior. In our work we found very interesting and counterintuitive patterns for time evolving networks, which change some of the basic assumptions that were made in the past. We then develop models that explain processes which govern the network evolution, fit such models to real networks, and use them to generate realistic graphs or give formal explanations about their properties. In addition, our work has a wide range of applications: it can help us spot anomalous graphs and outliers, forecast future graph structure and run simulations of network evolution. Another important aspect of our research is the study of local patterns and structures of propagation in networks. We aim to identify building blocks of the networks and find the patterns of influence that these blocks have on information or virus propagation over the network. Our recent work included the study of the spread of influence in a large personto- person product recommendation network and its effect on purchases. We also model the propagation of information on the blogosphere, and propose algorithms to efficiently find influential nodes in the network. A central topic of our thesis is also the analysis of large datasets as certain network properties only emerge and thus become visible when dealing with lots of data. We analyze the worlds social and communication network of Microsoft Instant Messenger with 240 million people and 255 billion conversations. We also made interesting and counterintuitive observations about network community structure that suggest that only small network clusters exist, and that they merge and vanish as they grow.
Top/Computer_Science/Network_Analysis	Battling Networks of Rival Social Movements
Top/Computer_Science/Network_Analysis	Structure and Dynamics in Complex Networks
Top/Computer_Science/Network_Analysis	An Algorithm to find OverlappingCommunity Structure in Networks
Top/Computer_Science/Network_Analysis	Collaboration Over Time: Characterizing and Modeling Network Evolution.
Top/Computer_Science/Network_Analysis	Social Networks from the Perspective of Physics 'In the history of public speaking, there have been many famous denials. One sunny day in 1880, Karl Marx declared: 'I am not a Marxist'. On a less auspicious occasion in 1973, Richard Nixon insisted 'I am not a crook'. Neither Marx nor Nixons audience gave much credence to their denials, and you too may respond with disbelief when I tell you that 'I am not a networker'. (Marc Granovetter, Connections, 1990) ... 'Instead, the slogan of the day will be 'We are all networkers now'.
Top/Computer_Science/Network_Analysis	Generating Graphs with Predefined k-Core Structure The modeling of realistic networks is of great importance for complex systemsresearch. Previous procedures typically model the natural growth of networks byiteratively adding nodes, use geometric positioning information, define linkconnectivity with preference for nearest neighbors or already highly connectednodes, or combine several of these approaches. Our novel model is based on the well-know concept of 'k'-cores, originally introduced in social network analysis. Recent studies exposed the significant 'k'-core structure of several real world systems, e.g. the AS network of theInternet. We present two algorithms for generating networks which strictlyadhere to the sizes of a given 'k'-core structure but also exhibit adaptationto various use cases. We showcase this in a comparative evaluation with twowell-known AS network generators.
Top/Computer_Science/Network_Analysis	'Lies, Damn Lies, and Statistics': A Critical Assessment of Preferential Attachment-type Network Models of the Internet Basic Question: Do the available Internet-related connectivity measurements and their analysis support the sort of claims that can be found in the existing complex networks literature? Key Issues: What about data hygiene? What about statistical rigor? What about model validation? Author discusses some of the main problems and challenges associated with measuring, inferring, and modeling various types of Internet-related connectivity structures. To this end, he uses some known examples to illustrate the need to understand the process by which Internet connectivity measurements are obtained, explore the sensitivity of inferred graph properties to known ambiguities in the data, be more critical with respect to the dominant, preferential attachmenttype network modeling paradigm, and be more serious/ambitious when it comes to model validation. Ignoring any of these issues is bound to produce results that are best described by the well-known aphorism 'lies, damned lies, and statistics.'
Top/Computer_Science/Network_Analysis	Network Structure of Folksonomies Folksonomies can be viewed as three mode graphs or as graphs made up of nodes (tags, users, resources) connected by hyper-edges. I shall report on some network statistical properties of a folksonomy graph based on data collected for the del.icio.us system. Moreover, by introducing a suitable distance between resources based on tag co-occurrence, I shall show that folksonomies embed a meaningful semantic clusterization of resources.
Top/Computer_Science/Network_Analysis	Self-mapping Networks
Top/Computer_Science/Network_Analysis	Extracting Semantic Networks from Text via Relational Clustering Extracting knowledge from text has long been a goal of AI. Initial approaches were purely logical and brittle. More recently, the availability of large quantities of text on the Web has led to the development of machine learning approaches. However, to date these have mainly extracted ground facts, as opposed to general knowledge. Other learning approaches can extract logical forms, but require supervision and do not scale. In this paper we present an unsupervised approach to extracting semantic networks from large volumes of text. We use the TextRunner system [1] to extract tuples from text, and then induce general concepts and relations from them by jointly clustering the objects and relational strings in the tuples. Our approach is defined in Markov logic using four simple rules. Experiments on a dataset of two million tuples show that it outperforms three other relational clustering approaches, and extracts meaningful semantic networks.
Top/Computer_Science/Network_Analysis	Cost-effective Outbreak Detection in Networks Given a water distribution network, where should we place sensors to quickly detect contaminants? Or, which blogs should we read to avoid missing important stories? These seemingly different problems share common structure: Outbreak detection can be modeled as selecting nodes (sensor locations, blogs) in a network, in order to detect the spreading of a virus or information as quickly as possible. We present a general methodology for near optimal sensor placement in these and related problems. We demonstrate that many realistic outbreak detection objectives (e.g., detection likelihood, population affected) exhibit the property of submodularity. We exploit submodularity to develop an efficient algorithm that scales to large problems, achieving near optimal placements, while being 700 times faster than a simple greedy algorithm. We also derive online bounds on the quality of the placements obtained by any algorithm. Our algorithms and bounds also handle cases where nodes (sensor locations, blogs) have different costs. We evaluate our approach on several large real-world problems, including a model of a water distribution network from the EPA, and real blog data. The obtained sensor placements are provably near optimal, providing a constant fraction of the optimal solution. We show that the approach scales, achieving speedups and savings in storage of several orders of magnitude. We also show how the approach leads to deeper insights in both applications, answering multicriteria trade-off, cost-sensitivity and generalization questions.
Top/Computer_Science/Network_Analysis	Microscopic Evolution of Social Networks
Top/Computer_Science/Network_Analysis	VideoLectures.net case study The task in the VideoLectures case study is to develop a software component that will aid the VideoLectures editors in categorizing recorded lectures (i.e. ontology population). This functionality is required due to the rapid growth of the number of hosted lectures as well as due to the fact that the categorization taxonomy is rather fine-grained (200 categories and growing). In addition to aiding the categorization of new lectures, the software will also be used for re-categorization and additional categorization of lectures already categorized. We will show that we were successful in our task as the categorizer is highly accurate it achieves accuracies that stretch 1220% above the baseline and highly robust in terms of missing data. The latter means that a lecture might be missing textual annotations (such as the description and slide titles) but is still categorized correctly. Furthermore, the categorizer has been successfully integrated into the VideoLectures Web site. Categorization suggestions (termed 'quick links') are provided to the author in the categorization panel.
Top/Computer_Science/Machine_Learning/Clustering	Spectral Clustering and Transductive Inference for Graph Data
Top/Computer_Science/Machine_Learning/Clustering	An Objective Evaluation Criterion for Clustering
Top/Computer_Science/Machine_Learning/Clustering	From clustering to algorithms In this talk we firstly provide a rigorous probabilistic proof of the clustering phenomenon taking place in the space of solution of random combinatorial problems. Secondly we will discuss a generalization of the survey propagation equations efficiently exploring the clustered geometry. Finally, we discuss the computational consequences of the possibility of finding single clusters by describing a 'physical ' lossy compression scheme. Performance are optimized when the number of well separated clusters is maximal in the underlying physical model.
Top/Computer_Science/Machine_Learning/Clustering	Semi-supervised Graph Clustering: A Kernel Approach
Top/Computer_Science/Machine_Learning/Clustering	Learning predictive clustering rules Predictive clustering is based on ideas from two machine learning subareas, predictive modeling and clustering. Methods for predictive clustering enable us to construct models for predicting multiple target variables, which are normally simpler and more comprehensible than the corresponding collection of models, each predicting a single variable. To this end, predictive clustering has been restricted to decision tree methods. Our goal is to extend this approach to methods for learning rules. We have developed a generalized version of the covering algorithm that enables learning of ordered or unordered rules, on single or multiple target classification or regression domains. Performance of the new method compares favorably to existing methods. Comparison of single target and multiple target prediction models shows that multiple target models offer comparable performance and drastically lower complexity than the corresponding collections of single target models.
Top/Computer_Science/Machine_Learning/Clustering	Support Cluster Machine For large-scale classification problems, the training samples can be clustered beforehand as a downsampling pre-process, and then only the obtained clusters are used for training. Motivated by such assumption, we proposed a classification algorithm, Support Cluster Machine (SCM), within the learning framework introduced by Vapnik. For the SCM, a compatible kernel is adopted such that a similarity measure can be handled not only between clusters in the training phase but also between a cluster and a vector in the testing phase. We also proved that the SCM is a general extension of the SVM with the RBF kernel. The experimental results confirm that the SCM is very effective for largescale classification problems due to significantly reduced computational costs for both training and testing and comparable classification accuracies. As a by-product, it provides a promising approach to dealing with privacy-preserving data mining problems.
Top/Computer_Science/Machine_Learning/Clustering	The stability of a good clustering If we have found a 'good' clustering C of data set X, can we prove that C is not far from the (unknown) best clustering C* of this data set? Perhaps surprisingly, the answer to this question is sometimes yes. We can show bounds on the distance( C, C* ) for two clustering cost functions: the Normalized Cut and the squared distance cost of K-means clustering. These bounds exist in the case when the data X admits a 'good' clustering for the given cost.
Top/Computer_Science/Machine_Learning/Clustering	Supervised Clustering with Support Vector Machines Supervised clustering is the problem of training a clustering algorithm to produce desirable clusterings: given sets of items and complete clusterings over these sets, we learn how to cluster future sets of items. Example applications include noun-phrase coreference clustering, and clustering news articles by whether they refer to the same topic. In this paper we present an SVM algorithm that trains a clustering algorithm by adapting the item-pair similarity measure. The algorithm may optimize a variety of different clustering functions to a variety of clustering performance measures. We empirically evaluate the algorithm for noun-phrase and news article clustering.
Top/Computer_Science/Machine_Learning/Clustering	Generalization Bounds for Clustering
Top/Computer_Science/Machine_Learning/Clustering	Clustering from an Optimization viewpoint Exploration and Exploitation using Upper Confidence Bounds
Top/Computer_Science/Machine_Learning/Clustering	A Decomposition Of Classes Via Clustering To Explain And Improve Naive Bayes
Top/Computer_Science/Machine_Learning/Clustering	Cluster stability and robust optimization
Top/Computer_Science/Machine_Learning/Clustering	Graph mincut, transductive inference and spectral clustering: some new elements
Top/Computer_Science/Machine_Learning/Clustering	Comparing clustering with confidence
Top/Computer_Science/Machine_Learning/Clustering	Music of the (p)Spheres This lecture is talking about **Nearest Neighbours** //Once upon a time...// **//Musica universalis//**//** **or **music of the spheres** is a medieval philosophical concept that regards the proportions in the movements of the** celestial bodies** - the **Sun, Moon** and planets - as a form of **musica**, the medieval **Latin** name for **music**. This music was not thought of as an audible **sound**, but simply as a **mathematical concept**. The **Greek** philosopher **Pythagoras** was frequently credited with originating the concept, which stemmed from his semi-**mystical**, semi-**mathematical** philosophy and its associated system of **numerology **of **Pythagoreanism**. // //Some **Surat Shabda Yoga, Satgurus** considered the music of the spheres to be a term synonymous with the Shabda or the Audible Life Stream in that tradition, because they considered **Pythagoras **to be a Satguru as well.//
Top/Computer_Science/Machine_Learning/Clustering	Lectures on Clustering These lectures give an introduction to data clustering: we discuss a few algorithms, but also look at theoretical questions related to clustering. **The first two lectures** are devoted to spectral clustering: graph Laplacians and their properties, spectral clustering algorithms, mathematical derivations of the algorithms, and some implementation issues. Moreover, we discuss the related modularity approach for detecting communities in networks. **The third lecture** is devoted to the very general question 'what clustering is'. We try to look at clustering from different angles, discuss different definitions of clustering, and look into theoretical foundations of clustering in general. **In the last lecture** we work on the question how the number of clusters should be defined. The focus is on two popular approaches: the gap statistics and the stability approach.
Top/Computer_Science/Machine_Learning/Clustering	Clustering - An overview Clustering, or finding groups in data, is as old as machine learning itself, if not older. However, as more people use clustering in a variety of settings, the last few years we have brought unprecedented developments in this field. This tutorial will survey the most important clustering methods in use today from a unifying perspective. I will then present some of the current paradigms shifts in data clustering.
Top/Computer_Science/Machine_Learning/Clustering	Graph Clustering With Network Structure Indices Graph clustering has become ubiquitous in the study of relational data sets. We examine two simple algorithms: a new graphical adaptation of the k -medoids algorithm and the Girvan-Newman method based on edge betweenness centrality. We show that they can be effective at discovering the latent groups or communities that are defined by the link structure of a graph. However, both approaches rely on prohibitively expensive computations, given the size of modern relational data sets. Network structure indices (NSIs) are a proven technique for indexing network structure and efficiently finding short paths. We show how incorporating NSIs into these graph clustering algorithms can overcome these complexity limitations. We also present promising quantitative and qualitative evaluations of the modified algorithms on synthetic and real data sets.
Top/Computer_Science/Machine_Learning/Clustering	Adaptive Dimension Reduction Using Discriminant Analysis and K-means Clustering Regularized Kernel Discriminant Analysis (RKDA) performs linear discriminant analysis in the feature space via the kernel trick. The performance of RKDA depends on the selection of kernels. In this paper, we consider the problem of learning an optimal kernel over a convex set of kernels. We show that the kernel learning problem can be formulated as a semidefinite program (SDP) in the binary-class case. We further extend the SDP formulation to the multi-class case. It is based on a key result established in this paper, that is, the multi-class kernel learning problem can be decomposed into a set of binary-class kernel learning problems. In addition, we propose an approximation scheme to reduce the computational complexity of the multi-class SDP formulation. The performance of RKDA also depends on the value of the regularization parameter. We show that this value can be learned automatically in the framework. Experimental results on benchmark data sets demonstrate the efficacy of the proposed SDP formulations.
Top/Computer_Science/Machine_Learning/Clustering	Assessing the Performance of a Graph-based Clustering Algorithm Graph-based clustering algorithms are particularly suited for dealing with data that do not come from a Gaussian or a spherical distribution. They can be used for detecting clusters of any size and shape without the need of specifying the actual number of clusters; moreover, they can be profitably used in cluster detection problems. In this paper, we propose a detailed performance evaluation of four different graph-based clustering approaches. Three of the algorithms selected for comparison have been chosen from the literature. While these algorithms do not require the setting of the number of clusters, they need, however, some parameters to be provided by the user. So, as the fourth algorithm under comparison, we propose in this paper an approach that overcomes this limitation, proving to be an effective solution in real applications where a completely unsupervised method is desirable.
Top/Computer_Science/Machine_Learning/Clustering	Clustering of Brain Tumours Through Constrained Manifold Learning Using Class Information
Top/Computer_Science/Machine_Learning/Clustering	Content Aggregation on Knowledge Bases Using Graph Clustering
Top/Computer_Science/Machine_Learning/Clustering	Differential Evolution and Particle Swarm Optimization in Partitional Clustering In recent years, many partitional clustering algorithms based on genetic algorithms (GA) have been proposed to tackle the problem of finding the optimal partition of a data set. Surprisingly, very few studies considered alternative stochastic search heuristics other than GAs or simulated annealing. Two promising algorithms for numerical optimization, which are hardly known outside the heuristic search field, are particle swarm optimisation (PSO) and differential evolution (DE). In this study, we compared the performance of GAs with PSO and DE for a medoid evolution approach to clustering. Moreover, we compared these results with the nominal classification, k-means and random search (RS) as a lower bound. Our results show that DE is clearly and consistently superior compared to GAs and PSO for hard clustering problems, both in respect to precision as well as robustness (reproducibility) of the results. Only for trivial problems all algorithms can obtain comparable results. Apart from superior performance, DE is very easy to implement and requires hardly any parameter tuning compared to substantial tuning for GAs and PSOs. Our study shows that DE rather than GAs should receive primary attention in partitional cluster algorithms.
Top/Computer_Science/Machine_Learning/Clustering	Diffusion Maps, Spectral Clustering and Reaction Coordinates of Dynamical Systems A central problem in data analysis is the low dimensional representation of high dimensional data, and the concise description of its underlying geometry and density. In the analysis of large scale simulations of complex dynamical systems, where the notion of time evolution comes into play, important problems are the identification of the slow variables and the representation of the reaction coordinates that parameterize them. In this paper we provide a unifying view of these apparently different tasks, by considering a family of diffusion maps, defined as the embedding of complex data onto a low dimensional Euclidian space, via the eigenvectors of suitably normalized random walks defined on the given datasets. We show, both theoretically and by examples how this embedding can be used for dimensionality reduction, manifold learning, geometric analysis of complex data sets and fast simulations of stochastic dynamical systems. Joint work with R.R. Coifman, S. Lafon, M. Maggioni and I.G. Kevrekidis
Top/Computer_Science/Machine_Learning/Clustering	Error Bounds for Correlation Clustering
Top/Computer_Science/Machine_Learning/Clustering	Explaining Text Clustering Results using Semantic Structures
Top/Computer_Science/Machine_Learning/Clustering	Graph Signature: A Simple Approach for Clustering Similar Graph
Top/Computer_Science/Machine_Learning/Clustering	Integrate Text Clustering Features in Text Categorization System
Top/Computer_Science/Machine_Learning/Clustering	Mutual Spectral Clustering: Microarray Experiments Versus Text Corpus This work studies a machine learning technique designed for exploring relations between microarray experiment data and the corpus of gene-related literature available via PubMed. The use of this task is found in that it provides better clusters of genes by fusing both information sources together, while it can also be used to guide the expert through the large corpus of gene-related literature based on insights into microarray experiments and vice versa.
Top/Computer_Science/Machine_Learning/Clustering	Qualitative Clustering of Short Time-Series: A Case Study of Firms Reputation Data
Top/Computer_Science/Machine_Learning/Clustering	Searching the Web by Discovering and Clustering Related Terms The amount of information on the web is growing so fast that it is becoming more and more difficult for classical search engines to find relevant information. Indeed, due to the frenetic increase of webpages written in different languages and sometimes in mis-interpreted languages, the degree of ambiguity of the human language has been constantly evolving to levels unseen so far. However, people still query the systems with no more than 2 words on average. As a consequence, new information retrieval systems need to be proposed to decrease the level of ambiguity of the queries. Such systems usually make use of query expansion techniques to solve this problem. In this talk, I will present a system based on the automatic discovery of terms that are related to the query as a means of helping the user to search for relevant information. This technique can be classified within Interactive Query Expansion systems. However, unlike other systems, we use Web Mining Techniques to discover related terms based on different features such as association measures, document similarity, document relevance, etc. In the second part of my talk, I will present the future extensions of our retrieval systems based on the automatic discovery of relations between related terms. So, by using agglomerative clustering techniques and an auto-fed WebWarehouse, we hope to be able to propose less ambiguous query expansion terms than in present systems where the user needs to sort out the terms he is interested in. ; [[http://webspy.di.ubi.pt/|Web spider]] : Web Spider is a system that returns all related terms and links from a given URL and a given query. : The Spider has been developped using C5.0 machine learning algorithm.
Top/Computer_Science/Machine_Learning/Clustering	Sequential Superparamagnetic Clustering as Network Self-organisation Process Clustering methods are useful tools for the unsupervised classification and analysis of the elements of a set or scene, e.g., a visual or auditory scene. Such methods can be seen as an integral part of cognition-like operations performed by artificial systems. The problematic is that usually no a priori information is available about the structure, the size or the number of classes. Therefore, unbiased methods that are able to provide a 'natural' classification are of interest. As it has been shown (Blatt, Wiseman, Domany), superparamagnetic clustering (SC) is a promising algorithm that comes close to an ideal unbiased method. SC gives the option of choosing different classes on different resolution levels. It, however, does not directly provide an intrinsic criterion for the choice of the 'most natural' levels, i.e. for finding the most natural classes.
Top/Computer_Science/Machine_Learning/Clustering	Szemerdi's Regularity Lemma and PairwiseClustering
Top/Computer_Science/Machine_Learning/Clustering	K-nearest neighbor classification In this short animated video the k-nearest neighbor classifier is introduced with simple 3D visuals. A real-world application, word pronunciation, is used to exemplify how the classifier learns and classifies. The video features a synthesized voice over.
Top/Computer_Science/Machine_Learning/Clustering	Stability for selecting the number of clusters: literature review, questions, and ideas
Top/Computer_Science/Machine_Learning/Clustering	Mixture Models and Collaborative Filtering Algorithms
Top/Computer_Science/Machine_Learning/Clustering	Predictive Discrete Latent Factor Models for Large Scale Dyadic Data We propose a novel statistical method to predict large scale dyadic response variables in the presence of covariate information. Our approach simultaneously incorporates the effect of covariates and estimates local structure that is induced by interactions among the dyads through a discrete latent factor model. The discovered latent factors provide a predictive model that is both accurate and interpretable. We illustrate our method by working in a framework of generalized linear models, which include commonly used regression techniques like linear regression, logistic regression and Poisson regression as special cases. We also provide scalable generalized EM-based algorithms for model fitting using both 'hard' and 'soft' cluster assignments. We demonstrate the generality and efficacy of our approach through large scale simulation studies and analysis of datasets obtained from certain real-world movie recommendation and internet advertising applications.
Top/Computer_Science/Machine_Learning/Clustering	A Framework for Simultaneous Co-clustering and Learning from Complex Data For difficult classification or regression problems, practitioners often segment the data into relatively homogenous groups and then build a model for each group. This two-step procedure usually results in simpler, more interpretable and actionable models without any loss in accuracy. We consider problems such as predicting customer behavior across products, where the independent variables can be naturally partitioned into two groups. A pivoting operation can now result in the dependent variable showing up as entries in a customer by product data matrix. We present a modelbased co-clustering (meta)-algorithm that interleaves clustering and construction of prediction models to iteratively improve both cluster assignment and fit of the models. This algorithm provably converges to a local minimum of a suitable cost function. The framework not only generalizes co-clustering and collaborative filtering to model-based coclustering, but can also be viewed as simultaneous co-segmentation and classification or regression, which is better than independently clustering the data first and then building models. Moreover, it applies to a wide range of bi-modal or multimodal data, and can be easily specialized to address classification and regression problems. We demonstrate the effectiveness of our approach on both these problems through experimentation on real and synthetic data.
Top/Computer_Science/Machine_Learning/Clustering	A formal analysis of stability - lessons and open questions
Top/Computer_Science/Machine_Learning/Clustering	ROC analisys for subgroup evaluation
Top/Computer_Science/Machine_Learning/Clustering	Machine learning for access and retrieval II
Top/Computer_Science/Machine_Learning/Clustering	Impromptu session
Top/Computer_Science/Machine_Learning/Clustering	Stability and convergence
Top/Computer_Science/Machine_Learning/Clustering	Machine Learning for Stock Selection In this paper, we propose a new method called Prototype Ranking (PR) designed for the stock selection problem. PR takes into account the huge size of real-world stock data and applies a modified competitive learning technique to predict the ranks of stocks. The primary target of PR is to select the top performing stocks among many ordinary stocks. PR is designed to perform the learning and testing in a noisy stocks sample set where the top performing stocks are usually the minority. The performance of PR is evaluated by a trading simulation of the real stock data. Each week the stocks with the highest predicted ranks are chosen to construct a portfolio. In the period of 1978-2004, PRs portfolio earns a much higher average return as well as a higher risk-adjusted return than Coopers method, which shows that the PR method leads to a clear profit improvement.
Top/Computer_Science/Machine_Learning/Clustering	Constraint-Driven Clustering Clustering methods can be either data-driven or need-driven. Data-driven methods intend to discover the true structure of the underlying data while need-driven methods aims at organizing the true structure to meet certain application requirements. Thus, need-driven (e.g. constrained) clustering is able to find more useful and actionable clusters in applications such as energy aware sensor networks, privacy preservation, and market segmentation. However, the existing methods of constrained clustering require users to provide the number of clusters, which is often unknown in advance, but has a crucial impact on the clustering result. In this paper, we argue that a more natural way to generate actionable clusters is to let the application-specific constraints decide the number of clusters. For this purpose, we introduce a novel cluster model, Constraint-Driven Clustering (CDC), which finds an a priori unspecified number of compact clusters that satisfy all user-provided constraints. Two general types of constraints are considered, i.e. minimum significance constraints and minimum variance constraints, as well as combinations of these two types. We prove the NP-hardness of the CDC problem with different constraints. We propose a novel dynamic data structure, the CD-Tree, which organizes data points in leaf nodes such that each leaf node approximately satisfies the CDC constraints and minimizes the objective function. Based on CD-Trees, we develop an efficient algorithm to solve the new clustering problem. Our experimental evaluation on synthetic and real datasets demonstrates the quality of the generated clusters and the scalability of the algorithm.
Top/Computer_Science/Machine_Learning/Clustering	Joint Cluster Analysis of Attribute and Relationship Data Without Priori Specification of the Number of Clusters In many applications, attribute and relationship data are available, carrying complementary information about real world entities. In such cases, a joint analysis of both types of data can yield more accurate results than classical clustering algorithms that either use only attribute data or only relationship (graph) data. The Connected k-Center (CkC) has been proposed as the first joint cluster analysis model to discover k clusters which are cohesive on both attribute and relationship data. However, it is well-known that prior knowledge on the number of clusters is often unavailable in applications such as community identification and hotspot analysis. In this paper, we introduce and formalize the problem of discovering an a-priori unspecified number of clusters in the context of joint cluster analysis of attribute and relationship data, called Connected X Clusters (CXC) problem. True clusters are assumed to be compact and distinctive from their neighboring clusters in terms of attribute data and internally connected in terms of relationship data. Different from classical attribute-based clustering methods, the neighborhood of clusters is not defined in terms of attribute data but in terms of relationship data. To efficiently solve the CXC problem, we present JointClust, an algorithm which adopts a dynamic two-phase approach. In the first phase, we find so called cluster atoms. We provide a probability analysis for this phase, which gives us a probabilistic guarantee, that each true cluster is represented by at least one of the initial cluster atoms. In the second phase, these cluster atoms are merged in a bottom-up manner resulting in a dendrogram. The final clustering is determined by our objective function. Our experimental evaluation on several real datasets demonstrates that JointClust indeed discovers meaningful and accurate clusterings without requiring the user to specify the number of clusters.
Top/Computer_Science/Machine_Learning/Clustering	Optimal Dimensionality of Metric Space for Classification For large-scale classification problems, the training samples can be clustered beforehand as a downsampling pre-process, and then only the obtained clusters are used for training. Motivated by such assumption, we proposed a classification algorithm, Support Cluster Machine (SCM), within the learning framework introduced by Vapnik. For the SCM, a compatible kernel is adopted such that a similarity measure can be handled not only between clusters in the training phase but also between a cluster and a vector in the testing phase. We also proved that the SCM is a general extension of the SVM with the RBF kernel. The experimental results confirm that the SCM is very effective for largescale classification problems due to significantly reduced computational costs for both training and testing and comparable classification accuracies. As a by-product, it provides a promising approach to dealing with privacy-preserving data mining problems.
Top/Computer_Science/Machine_Learning/Clustering	On a statistical model of cluster stability
Top/Computer_Science/Machine_Learning/Clustering	Cluster Stability for Finite Samples
Top/Computer_Science/Machine_Learning/Clustering	Hierarchical Mixture Models: a Probabilistic Analysis Mixture models form one of the most widely used classes of generative models for describing structured and clustered data. In this paper we develop a new approach for the analysis of hierarchical mixture models. More specifically, using a text clustering problem as a motivation, we describe a natural generative process that creates a hierarchical mixture model for the data. In this process, an adversary starts with an arbitrary base distribution and then builds a topic hierarchy via some evolutionary process, where he controls the parameters of the process. We prove that under our assumptions, given a subset of topics that represent generalizations of one another (such as baseball&#160;- sports&nbsp;- base), for any document which was produced via some topic in this hierarchy, we can efficiently determine the most specialized topic in this subset, it still belongs to. The quality of the classification is independent of the total number of topics in the hierarchy and our algorithm does not need to know the total number of topics in advance. Our approach also yields an algorithm for clustering and unsupervised topical tree reconstruction. We validate our model by showing that properties predicted by our theoretical results carry over to real data. We then apply our clustering algorithm to two different datasets: (i) 20 newsgroups [19] and (ii) a snapshot of abstracts of arXiv [2] (15 categories, 240,000 abstracts). In both cases our algorithm performs extremely well.
Top/Computer_Science/Machine_Learning/Clustering	BoostCluster: Boosting Clustering by Pairwise Constraints Data clustering is an important task in many disciplines. A large number of studies have attempted to improve clustering by using the side information that is often encoded as pairwise constraints. However, these studies focus on designing special clustering algorithms that can effectively exploit the pairwise constraints. We present a boosting framework for data clustering, termed as BoostCluster, that is able to iteratively improve the accuracy of any given clustering algorithm by exploiting the pairwise constraints. The key challenge in designing a boosting framework for data clustering is how to influence an arbitrary clustering algorithm with the side information since clustering algorithms by definition are unsupervised. The proposed framework addresses this problem by dynamically generating new data representations at each iteration that are, on the one hand, adapted to the clustering results at previous iterations by the given algorithm, and on the other hand consistent with the given side information. Our empirical study shows that the proposed boosting framework is effective in improving the performance of a number of popular clustering&#160; algorithms (Kmeans, partitional SingleLink, spectral clustering), and its performance is comparable to the state-of-the-art algorithms for data clustering with side information.
Top/Computer_Science/Machine_Learning/Clustering	Nonlinear Adaptive Distance Metric Learning for Clustering A good distance metric is crucial for many data mining tasks. To learn a metric in the unsupervised setting, most metric learning algorithms project observed data to a lowdimensional manifold, where geometric relationships such as pairwise distances are preserved. It can be extended to the nonlinear case by applying the kernel trick, which embeds the data into a feature space by specifying the kernel function that computes the dot products between data points in the feature space. In this paper, we propose a novel unsupervised Nonlinear AdaptiveMetric Learning algorithm, called NAML, which performs clustering and distance metric learning simultaneously. NAML first maps the data to a high-dimensional space through a kernel function; then applies a linear projection to find a low-dimensional manifold where the separability of the data is maximized; and finally performs clustering in the low-dimensional space. The performance of NAML depends on the selection of the kernel function and the projection. We show that the joint kernel learning, dimensionality reduction, and clustering can be formulated as a trace maximization problem, which can be solved via an iterative procedure in the EM framework. Experimental results demonstrated the efficacy of the proposed algorithm.
Top/Computer_Science/Machine_Learning/Clustering	SCAN: A Structural Clustering Algorithm for Networks Network clustering (or graph partitioning) is an important task for the discovery of underlying structures in networks. Many algorithms find clusters by maximizing the number of intra-cluster edges. While such algorithms find useful and interesting structures, they tend to fail to identify and isolate two kinds of vertices that play special roles&#160;- vertices that bridge clusters (hubs) and vertices that are marginally connected to clusters (outliers). Identifying hubs is useful for applications such as viral marketing and epidemiology since hubs are responsible for spreading ideas or disease. In contrast, outliers have little or no influence, and may be isolated as noise in the data. In this paper, we proposed a novel algorithm called SCAN (Structural Clustering Algorithm for Networks), which detects clusters, hubs and outliers in networks. It clusters vertices based on a structural similarity measure. The algorithm is fast and efficient, visiting each vertex only once. An empirical evaluation of the method using both synthetic and real datasets demonstrates superior performance over other methods such as the modularity-based algorithms.
Top/Computer_Science/Machine_Learning/Clustering	Soft Topographic Map for Clustering and Classification of Bacteria In this work a new method for clustering and building a topographic representation of a bacteria taxonomy is presented. The method is based on the analysis of stable parts of the genome, the so-called housekeeping genes. The proposed method generates topographic maps of the bacteria taxonomy, where relations among different type strains can be visually inspected and verified. Two well known DNA alignement algorithms are applied to the genomic sequences. Topographic maps are optimized to represent the similarity among the sequences according to their evolutionary distances. The experimental analysis is carried out on 147 type strains of the Gammaprotebacteria class by means of the 16S rRNA housekeeping gene. Complete sequences of the gene have been retrieved from the NCBI public database. In the experimental tests the maps show clusters of homologous type strains and presents some singular cases potentially due to incorrect classification or erroneous annotations in the database.
Top/Computer_Science/Machine_Learning/Clustering	Fast Clustering based on Kernel Density Estimation The Denclue algorithm employs a cluster model based on kernel density estimation. A cluster is defined by a local maximum of the estimated density function. Data points are assigned to clusters by hill climbing, i.e. points going to the same local maximum are put into the same cluster. A disadvantage of Denclue 1.0 is, that the used hill climbing may make unnecessary small steps in the beginning and never converges exactly to the maximum, it just comes close. We introduce a new hill climbing procedure for Gaussian kernels, which adjusts the step size automatically at no extra costs. We prove that the procedure converges exactly towards a local maximum by reducing it to a special case of the expectation maximization algorithm. We show experimentally that the new procedure needs much less iterations and can be accelerated by sampling based methods with sacrificing only a small amount of accuracy.
Top/Computer_Science/Machine_Learning/Clustering	Visualising the Cluster Structure of Data Streams The increasing availability of streaming data is a consequence of the continuing advancement of data acquisition technology. Such data provides new challenges to the various data analysis communities. Clustering has long been a fundamental procedure for acquiring knowledge from data, and new tools are emerging that allow the clustering of data streams. However the dynamic, temporal components of streaming data provide extra challenges to the development of stream clustering and associated visualisation techniques. In this work we combine a streaming clustering framework with an extension of a static cluster visualisation method, in order to construct a surface that graphically represents the clustering structure of the data stream. The proposed method, OpticsStream, provides intuitive representations of the clustering structure as well as the manner in which this structure changes through time.
Top/Computer_Science/Machine_Learning/Clustering	A Prediction-based Visual Approach for Cluster Exploration and Cluster Valadation by HOV3
Top/Computer_Science/Machine_Learning/Clustering	Science mapping with asymmetric co-occurence analisys We propose new innovative methods in order to reconstruct paradigmatic fields thanks to simple statistics over a scientific content database. We first define an asymmetric paradigmatic proximity between concepts which provides hierarchical structure over the set of concepts. We propose to implement overlapping categorization to describe paradigmatic fields as sets of concepts that may have several different usage and introduce a 2D embedding to represent these sets in a structured way. This enables to have a micro, meso and macro scale approach to our set of concepts. Concepts can also be dynamically clustered providing a high-level description of the evolution of the paradigmatic fields. We apply our set of methods on a case study from the Complex Systems Community through the mapping of the dynamics of more than 400 Complex Systems Science concepts indexed in a database of of several millions of journal papers.
Top/Computer_Science/Machine_Learning/Clustering	Matching Partitions over Time to Reliably Capture Local Clusters in Noisy Domains
Top/Computer_Science/Machine_Learning/Clustering	Detecting Changes in Musical Texture
Top/Computer_Science/Machine_Learning/Clustering	An Algorithm to find OverlappingCommunity Structure in Networks
Top/Computer_Science/Machine_Learning/Clustering	Clustering
Top/Computer_Science/Machine_Learning/Clustering	Extracting Semantic Networks from Text via Relational Clustering Extracting knowledge from text has long been a goal of AI. Initial approaches were purely logical and brittle. More recently, the availability of large quantities of text on the Web has led to the development of machine learning approaches. However, to date these have mainly extracted ground facts, as opposed to general knowledge. Other learning approaches can extract logical forms, but require supervision and do not scale. In this paper we present an unsupervised approach to extracting semantic networks from large volumes of text. We use the TextRunner system [1] to extract tuples from text, and then induce general concepts and relations from them by jointly clustering the objects and relational strings in the tuples. Our approach is defined in Markov logic using four simple rules. Experiments on a dataset of two million tuples show that it outperforms three other relational clustering approaches, and extracts meaningful semantic networks.
Top/Computer_Science/Machine_Learning/Clustering	A Semi-fuzzy approach for online divisive-agglomerative clustering The Online Divisive-Agglomerative Clustering (ODAC) is an incremental approach for clustering streaming time series using a hierarchical procedure over time. It constructs a tree-like hierarchy of clusters of streams, using a top-down strategy based on the correlation between streams. The system also possesses an agglomerative phase to enhance a dynamic behavior capable of structural change detection. However, the split decision used in the algorithm focus on the crisp boundary between two groups, which implies a high risk since it has to decide based on only a small subset of the entire data. In this work we propose a semi-fuzzy approach to the assignment of variables to newly created clusters, for a better trade-off between validity and performance. Experimental work supports the benefits of our approach.
Top/Computer_Science/Machine_Learning/Clustering	S-means: similarity driven clustering and its application in gravitational-wave astronomy data mining Clustering is to classify unlabeled data into groups. It has been wellresearched for decades in many disciplines. Clustering in massive amount of astronomical data generated by multi-sensor networks has become an emerging new challenge; assumptions in many existing clustering algorithms are often violated in these domains. For example, K means implicitly assumes that underlying distribution of data is Gaussian. Such an assumption is not necessarily observed in astronomical data. Another problem is the determination of K, which is hard to decide when prior knowledge is lacking. While there has been work done on discovering the proper value for K given only the data, most existing works, such as X-means, G-means and PG-means, assume that the model is a mixture of Gaussians in one way or another. In this paper, we present a similarity-driven clustering approach for tackling large scale clustering problem. A similarity threshold T is used to constrain the search space of possible clustering models such that only those satisfying the threshold are accepted. This forces the search to: 1) explicitly avoid getting stuck in local minima, and hence the quality of models learned has a meaningful lower bound, and 2) discover a proper value for K as new clusters have to be formed if merging them into existing ones will violate the constraint given by the threshold. Experimental results on the UCI KDD archive and realistic simulated data generated for the Laser Interferometer Gravitational Wave Observatory (LIGO) suggest that such an approach is promising.
Top/Arts/Poetry	Poetry Reading
Top/Arts/Poetry	Poetry Reading
Top/Arts/Poetry	Poetry Reading
Top/Arts/Poetry	Discussion on Poetry
Top/Arts/Poetry	Poetry Reading
Top/Arts/Poetry	Poetry Reading
Top/Arts/Poetry	Poetry Reading
Top/Arts/Poetry	Poetry Reading
Top/Arts/Poetry	Poetry Reading
Top/Arts/Poetry	Poetry Reading
Top/Arts/Poetry	Poetry Reading
Top/Arts/Visual_arts	Symmetry in the works of M. C. Escher Introductory References about Tessellations (tilings) and Sources for Illustrations: [[http://www.geom.uiuc.edu/software/tilings/TilingBibliography.html|link]]
Top/Arts/Visual_arts	Aesthetic Science: Understanding Preferences for Color and Spatial Composition
Top/Computer_Science/Machine_Learning/Statistical_Learning	Advanced Statistical Learning Theory This set of lectures will complement the statistical learning theory course and focus on recent advances in the domain of classification. 1- PAC Bayesian bounds: a simple derivation, comparison with Rademacher averages. 2 - Local Rademacher complexity with classification loss, Talagrand's inequality. Tsybakov noise conditions. 3 - Properties of loss functions for classification (influence on approximation and estimation, relationship with noise conditions). 4 - Applications to SVM - Estimation and approximation properties, role of eigenvalues of the Gram matrix.
Top/Computer_Science/Machine_Learning/Statistical_Learning	A statistical learning approach to subspace identification of dynamical systems Among the different approaches to identification of linear dynamical systems, subspace identification has become increasingly popular in the last decade. The reasons are the algorithmic simplicity thanks to the absence of non-convex optimization problems, the numerical stabil- ity and the statistical properties. Interestingly, concerning the statistical side, research in subspace identification has been concentrated on proving properties related to asymptotic unbiasedness. In this extended abstract we motivate how the use of an appropriate regularization can be helpful in the small sample case. Furthermore, this regularization allows one to use the kernel trick to identify systems where the input term in the state and output equations is a nonlinear function of the input variables.
Top/Computer_Science/Machine_Learning/Statistical_Learning	Kernel Methods in Statistical Learning
Top/Computer_Science/Machine_Learning/Statistical_Learning	Kernel Methods in Statistical Learning
Top/Computer_Science/Machine_Learning/Statistical_Learning	On-line Statistical Learning
Top/Computer_Science/Machine_Learning/Statistical_Learning	Statistical Learning Theory This course will give a detailed introduction to learning theory with a focus on the classification problem. It will be shown how to obtain (pobabilistic) bounds on the generalization error for certain types of algorithms. The main themes will be: * probabilistic inequalities and concentration inequalities * union bounds, chaining * measuring the size of a function class, Vapnik Chervonenkis dimension, shattering dimension and Rademacher averages * classification with real-valued functions Some knowledge of probability theory would be helpful but not required since the main tools will be introduced.
Top/Computer_Science/Machine_Learning/Statistical_Learning	Statistical Learning Theory
Top/Computer_Science/Machine_Learning/Statistical_Learning	Statistical Learning Theory and Empirical Processes
Top/Computer_Science/Machine_Learning/Statistical_Learning	Introduction to Learning Theory The goal of this course is to introduce the key concepts of learning theory. It will not be restricted to Statistical Learning Theory but will mainly focus on statistical aspects. Instead of giving detailed proofs and precise statements, this course will aim at providing some useful conceptual tools and ideas useful for practitioners as well as for theoretically-driven people.
Top/Computer_Science/Machine_Learning/Statistical_Learning	Practical Statistical Relational Learning The tutorial will be composed of three parts:** ** # ** Foundational areas.** The first part will consist of a brief introduction to each of the four foundational areas of SRL: logical inference, inductive logic programming, probabilistic inference, and statistical learning. Obviously, in the short time available no attempt will be made to comprehensively survey these areas; rather, the focus will be on providing the key concepts and techniques required for the subsequent parts. For example, the logical inference part will focus on the basics of satisfiability testing, and the probabilistic/statistical parts on Markov networks. The duration of this part will be approximately two hours (half hour per subtopic). # **Putting the pieces together.** The second part will introduce the key ideas in SRL and survey major approaches, using Markov logic as the unifying framework. It will present state-of-the-art algorithms for statistical relational learning and inference, and give an overview of the Alchemy open-source software. This part will essentially consist of putting together the pieces introduced in the first part. Its duration will be approximately an hour. # **Applications.** The third and final part will describe how to efficiently develop state-of-the-art non-i.i.d. applications in various areas, including: hypertext classification, link-based information retrieval, information extraction and integration, natural language processing, social network modeling, computational biology, and ubiquitous computing. This part will also include practical tips on using SRL, Markov logic and Alchemy - the kind of information that is seldom found in research papers, but is key to developing successful applications. The duration of this part will be approximately an hour.
Top/Computer_Science/Machine_Learning/Statistical_Learning	Tutorial on Statistical Machine Learning with Applications to Multimodal Processing
Top/Computer_Science/Machine_Learning/Statistical_Learning	SRL - The next decade
Top/Computer_Science/Machine_Learning/Statistical_Learning	Statistical learning theory - Learning Theory: Foundations and Goals - Learning Bounds: Ingredients and Results - Implications: What to conclude from bounds
Top/Computer_Science/Machine_Learning/Statistical_Learning	Stochastic Learning
Top/Computer_Science/Machine_Learning/Statistical_Learning	Using features of probability distributions to achieve covariate shift
Top/Computer_Science/Machine_Learning/Statistical_Learning	Maximum Likelihood Estimation for a Gene Regulatory Network Defined by Differential Equations Gene regulation may be described by a set of deterministic differential equations describing the time rate evolution of the gene product concentrations, and containing parameters accounting for the regulatory relationships occurring in the gene network. We will present maximum likelihood based estimators of the parameters arising in this formalism and we will prove that they have desirable properties. Our results may be applied to a gene regulation model yielding the early Drosophila segments formation relying on a statistical modelling of gene expression data obtained by confocal laser scanning microscopy. The proposed statistical model accounts for the uncertainty in the measurement of gene expression and the uncertainty in the time at which the measurements are performed.
Top/Computer_Science/Machine_Learning/Statistical_Learning	Benchmarking parameter estimation and reverse engineering strategies Parameter estimation has become a central problem in systems biology, both in the form of calibration of bottom-up models or as a component of reverse engineering algorithms. With a proliferation of algorithms proposed for these purposes it has become important to compare them in objective ways. I will argue that in silico biochemical network models are extremely useful for this purpose. Several networks will be presented that are challenging tests for parameter estimation and network inference. An issue that arises from the use of in silico networks, though, is whether they can provide realistic data. The application of this benchmarking methodology will be illustrated with a comparison of four reverse engineering methods. //Joint work with Diogo Camacho, Paola Vera Licona, and Reinhard Laubenbacher//
Top/Computer_Science/Machine_Learning/Statistical_Learning	Trust Region Newton Methods for Large-Scale Logistic Regression Large-scale logistic regression arises in many applications such as document classification and natural language processing. In this paper, we apply a trust region Newton method to maximize the log-likelihood of the logistic regression model. The proposed method uses only approximate Newton steps in the beginning, but achieves fast convergence in the end. Experiments show that it is faster than the commonly used quasi Newton approach for logistic regression. We also compare it with linear SVM implementations.
Top/Computer_Science/Machine_Learning/Statistical_Learning	Machine Learning Flavor of Random Matrices
Top/Computer_Science/Machine_Learning/Statistical_Learning	Cluster Variation Method: from statistical mechanics to message passing algorithms The cluster variation method (CVM) is a hierarchy of approximate variational techniques for discrete (Ising--like) models in equilibrium statistical mechanics, improving on the mean--field approximation and the Bethe--Peierls approximation, which can be regarded as the lowest level of the CVM. The foundations of the CVM are briefly reviewed, considering different derivations of the method and related techniques, like for instance TAP equations and the cavity method. Issues of realizability and exactness are also addressed.
Top/Computer_Science/Machine_Learning/Statistical_Learning	Probabilistic Inference for Graph Classification Graph data is getting increasingly popular in, e.g., bioinfor- matics and text processing. A main dificulty of graph data processing lies in the intrinsic high dimensionality of graphs, namely, when a graph is represented as a binary feature vector of indicators of all possible sub- graphs, the dimensionality gets too large for usual statistical methods.
Top/Computer_Science/Machine_Learning/Statistical_Learning	How to predict with Bayes, MDL, and Experts Most passive Machine Learning tasks can be (re)stated as sequence prediction problems. This includes pattern recognition, classification, time-series forecasting, and others. Moreover, the understanding of passive intelligence also serves as a basis for active learning and decision making. In the recent past, rich theories for sequence prediction have been developed, and this is still an ongoing process. On the other hand, we are arriving at the stage where some important results are already termed classical. While much of the current Learning Theory is formulated under the assumption of independent and identically distributed (i.i.d.) observations, this lecture series focusses on situations without this prerequisite (e.g. weather or stock-market time-series).
Top/Computer_Science/Machine_Learning/Statistical_Learning	Introduction to the Workshop on Multiple Simultaneous Hypothesis Testing
Top/Computer_Science/Machine_Learning/Statistical_Learning	Learning the Kernel Matrix in Discriminant Analysis via Quadratically Constrained Quadratic Programming The kernel function plays a central role in kernel methods. In this paper, we consider the automated learning of the kernel matrix over a convex combination of pre-specified kernel matrices in Regularized Kernel Discriminant Analysis (RKDA), which performs linear discriminant analysis in the feature space via the kernel trick. Previous studies have shown that this kernel learning problem can be formulated as a semidefinite program (SDP), which is however computationally expensive, even with the recent advances in interior point methods. Based on the equivalence relationship between RKDA and least square problems in the binary-class case, we propose a Quadratically Constrained Quadratic Programming (QCQP) formulation for the kernel learning problem, which can be solved more efficiently than SDP. While most existing work on kernel learning deal with binary-class problems only, we show that our QCQP formulation can be extended naturally to the multi-class case. Experimental results on both binary-class and multiclass benchmark data sets show the efficacy of the proposed QCQP formulations.
Top/Computer_Science/Machine_Learning/Statistical_Learning	Structural Prediction in Statistical Alignment and Translation
Top/Computer_Science/Machine_Learning/Statistical_Learning	Learning Nonparametric Priors from Multiple Tasks
Top/Computer_Science/Machine_Learning/Statistical_Learning	Unsupervised Estimation for Noisy-Channel Models Shannons Noisy-Channel model, which describes how a corrupted message might be reconstructed, has been the corner stone for much work in statistical language and speech processing. The model factors into two components: a language model to characterize the original message and a channel model to describe the channels corruptive process. The standard approach for estimating the parameters of the channel model is unsupervised Maximum-Likelihood of the observation data, usually approximated using the Expectation-Maximization (EM) algorithm. In this paper we show that it is better to maximize the joint likelihood of the data at both ends of the noisy-channel. We derive a corresponding bi-directional EM algorithm and show that it gives better performance than standard EM on two tasks: (1) translation using a probabilistic lexicon and (2) adaptation of a part-of-speech tagger between related languages.
Top/Computer_Science/Machine_Learning/Statistical_Learning	Can Adaptive Regularization Help?
Top/Computer_Science/Machine_Learning/Statistical_Learning	MarkusSparse Grid Methods The search for interesting variable stars, the discovery of relations between geomorphological properties, satellite observations and mineral concentrations, and the analysis of biological networks all require the solution of a large number of complex learning problems with large amounts of data. A major computational challenge faced in these investigations is posed by the curse of dimensionality. A well known aspect of this curse is the exponential dependence of the size of regular grids on the dimension of the domain. This makes traditional finite element approaches infeasible for high-dimensional domains. It is less known that this curse also affects computations of radial basis function approximations -- in a slightly more subtle way. Sparse grid functions can deal with the major problems of the curse of dimensionality. As they are the superposition of traditional finite element spaces, many well-known algorithms can be generalized to the sparse grid context. Sparse grids have been successfully used to solve partial differential equations in the past and, more recently, have been shown to be competitive for learning problems as well. The talk will provide a general introduction to the major properties of sparse grids and will discuss connections with kernel based methods and parallel learning algorithms. It will conclude with a short review over some recent work on algorithms based on the combination technique.
Top/Computer_Science/Machine_Learning/Statistical_Learning	A Divergence Prior for Adaptive Learning.
Top/Computer_Science/Machine_Learning/Statistical_Learning	Statistical Relational Learning - Part 2 Statistical machine learning is in the midst of a 'relational revolution'. After many decades of focusing on independent and identically-distributed (iid) examples, many researchers are now studying problems in which the examples are linked together into complex networks. These networks can be a simple as sequences and 2-D meshes (such as those arising in part-of-speech tagging and remote sensing) or as complex as citation graphs, the world wide web, and relational data bases.
Top/Computer_Science/Machine_Learning/Statistical_Learning	Introduction to Statistical Machine Learning The first part of his tutorial provides a brief overview of the fundamental methods and applications of statistical machine learning. The other speakers will detail or built upon this introduction. Statistical machine learning is concerned with the development of algorithms and techniques that learn from observed data by constructing stochastic models that can be used for making predictions and decisions. Topics covered include Bayesian inference and maximum likelihood modeling; regression, classification, density estimation, clustering, principal component analysis; parametric, semi-parametric, and non-parametric models; basis functions, neural networks, kernel methods, and graphical models; deterministic and stochastic optimization; overfitting, regularization, and validation.
Top/Computer_Science/Machine_Learning/Statistical_Learning	Probabilistic inference methods in robotics-filling the gap between high-level reasoning and low-level motion control
Top/Computer_Science/Machine_Learning/Statistical_Learning	Infer.NET - Practical Implementation Issues and a Comparison of Approximation Techniques Infer.NET is an efficient, general-purpose inference engine developed at Microsoft Cambridge by Tom Minka, John Winn and others. It aims to be highly efficient, general purpose and extensible --- three normally contradictory goals. We have largely managed to achieve these goals using a compiler-like architecture, so that code is generated to perform the desired inference task. Infer.NET can apply one of a range of inference algorithms to a given probabilistic model, and so provides a useful framework for comparing the performance of different algorithms. In this talk, I will describe the capabilities and infrastructure of Infer.NET and give examples of applying both expectation propagation and variational message passing on the same model. I will also describe some failure cases that we have encountered for each algorithm.
Top/Computer_Science/Machine_Learning/Statistical_Learning	An Introduction to Statistical Relational Learning
Top/Computer_Science/Machine_Learning/Statistical_Learning	Foundations of Statistical Learning Theory : Empirical Infe-rence in high-dimention spaces
Top/Computer_Science/Machine_Learning/Statistical_Learning	Tractable Inference for Probabilistic Models by Free Energy Approximations Probabilistic models explain complex observed data by a set of unobserved, hidden random variables based on the joint distribution of the variables. Statistical inference requires the evaluation of high dimensional sums or integrals. Hence, one has to deal with a vast computational complexity when the number of hidden variables is large and it is important to develop tractable approximations. I will discuss ideas for such approximations which are based on a variational formulation of inference. Quantities of interest, like marginal moments of the distribution are found as minima of an entropic quantity, often called the Gibbs Free Energy. While an exact computation of the Free Energy is computationally intractable, sensible approximations often provide quite accurate results. I will discusss applications of these techniques to the estimation of wind fields from satellite measurements, to a model of an error correcting code in telecommunication and to approximate resampling methods.
Top/Computer_Science/Machine_Learning/Statistical_Learning	Statistical Modeling of Relational Data KDD has traditionally been concerned with mining data from a single relation. However, most applications involve multiple interacting relations, either explicitly (in relational databases) or implicitly (in semi-structured and multimodal data). Examples include link analysis, social networks, bioinformatics, information extraction, security, ubiquitous computing, etc. Mining such data has become a topic of keen interest in the KDD community in recent years. The key difficulty is that data in relational domains is no longer i.i.d. (independent and identically distributed), greatly complicating statistical modeling. However, research has now advanced to the point where robust, easy-to-use, general-purpose techniques and languages for mining non-i.i.d. data are available. The goal of this tutorial is to add a sufficient subset of these concepts and techniques to the toolkits of both researchers and practitioners.
Top/Computer_Science/Machine_Learning/Statistical_Learning	Statistical Methods These two sessions give an introduction to classical statistics. The first talk is a brief coverage of some basic statistical theory that I feel might be useful during the week. The topics included are: probability, likelihood inference, Bayesian inference and the concept of the bias variance tradeoff. The second talk covers two main areas of statistics, namely linear modelling and exploratory multivariate analysis. Linear modelling has an elaborate set of procedures for determining which of the potential inputs should be included in a model for predicting an output. The inputs can be of any data type (categorical or numerical), as can the output to some extent. This sort of modelling has been used successfully for many years on what would be considered quite small sets of data. Some linear models are now being used on much larger problems, for example, in the construction of scorecards for deciding whether to approve somebodys application for a credit card. This is the primary type of statistical model that would be used in scientific research. Exploratory multivariate analysis is a collection of techniques that are all based on the same sort of mathematical background (vectors/matrix algebra). They are also all intended to investigate the structure of observations that are vectors. Some of these techniques are being used on a massive scale in business; for example, hierarchical cluster analysis is used to build (offline) classifications of all the postal codes (each code represents about 16 houses) in a system such as Mosaic which is then used for targeting of marketing. The specific techniques covered are: principal components analysis; correspondence analysis; scaling; cluster analysis.
Top/Computer_Science/Machine_Learning/Bayesian_Learning	Bayesian Learning Bayes Rule provides a simple and powerful framework for machine learning. This tutorial will be organised as follows: 1. I will give motivation for the Bayesian framework from the point of view of rational coherent inference, and highlight the important role of the marginal likelihood in Bayesian Occam's Razor. 2. I will discuss the question of how one should choose a sensible prior. When Bayesian methods fail it is often because no thought has gone into choosing a reasonable prior. 3. Bayesian inference usually involves solving high dimensional integrals and sums. I will give an overview of numerical approximation techniques (e.g. Laplace, BIC, variational bounds, MCMC, EP...). 4. I will talk about more recent work in non-parametric Bayesian inference such as Gaussian processes (i.e. Bayesian kernel 'machines'), Dirichlet process mixtures, etc.
Top/Computer_Science/Machine_Learning/Bayesian_Learning	Introduction to Reinforcement Learning and Bayesian learning Although Bayesian methods for Reinforcement Learning can be traced back to the 1960s (Howard's work in Operations Research), Bayesian methods have only been used sporadically in modern Reinforcement Learning. This is in part because non-Bayesian approaches tend to be much simpler to work with. However, recent advances have shown that Bayesian approaches do not need to be as complex as initially thought and offer several theoretical advantages. For instance, by keeping track of full distributions (instead of point estimates) over the unknowns, Bayesian approaches permit a more comprehensive quantification of the uncertainty regarding the transition probabilities, the rewards, the value function parameters and the policy parameters. Such distributional information can be used to optimize (in a principled way) the classic exploration/exploitation tradeoff, which can speed up the learning process. Similarly, active learning for reinforcement learning can be naturally optimized. The estimation of gradient performance with respect to value function or and/or policy parameters can also be done more accurately while using less data. Bayesian approaches also facilitate the encoding of prior knowledge and the explicit formulation of domain assumptions. The primary goal of this tutorial is to raise the awareness of the research community with regard to Bayesian methods, their properties and potential benefits for the advancement of Reinforcement Learning. An introduction to Bayesian learning will be given, followed by a historical account of Bayesian Reinforcement Learning and a description of existing Bayesian methods for Reinforcement Learning. The properties and benefits of Bayesian techniques for Reinforcement Learning will be discussed, analyzed and illustrated with case studies.
Top/Computer_Science/Machine_Learning/Bayesian_Learning	Bayesian Inference for Systems Biological Models via a Diffusion Approximation As post-genomic biology becomes more predictive, the ability to infer rate parameters (known as reverse-engineering) of biochemical networks will become increasingly important. One approach is to replace the underlying model by a diffusion approximation and the model is identified using discrete-time (and often incomplete) data that is subject to error. Unfortunately, likelihood based inference can be problematic as closed form transition densities of nonlinear diffusions are rarely available. A widely used solution involves the introduction of latent data points between every pair of observations to allow an Euler-Maruyama approximation of the true transition densities to become accurate. Markov chain Monte Carlo (MCMC) methods can then be used to sample the posterior distribution of latent data and model parameters; however, naive schemes suffer from a mixing problem that worsens with the degree of augmentation. A reparameterisation is therefore implemented to overcome this difficulty and the methodology is applied to a simple prokaryotic auto-regulatory gene network. Joint work with Darren J. Wilkinson
Top/Computer_Science/Machine_Learning/Bayesian_Learning	Bayesian models of human inductive learning In everyday learning and reasoning, people routinely draw successful generalizations from very limited evidence. Even young children can infer the meanings of words, hidden properties of objects, or the existence of causal relations from just one or a few relevant observations -- far outstripping the capabilities of conventional learning machines. How do they do it? And how can we bring machines closer to these human-like learning abilities? I will argue that people's everyday inductive leaps can be understood as approximations to Bayesian computations operating over structured representations of the world, what cognitive scientists have called 'intuitive theories' or 'schemas'. For each of several everyday learning tasks, I will consider how appropriate knowledge representations are structured and used, and how these representations could themselves be learned via Bayesian methods. The key challenge is to balance the need for strongly constrained inductive biases -- critical for generalization from very few examples -- with the flexibility to learn about the structure of new domains, to learn new inductive biases suitable for environments which we could not have been pre-programmed to perform in. The models I discuss will connect to several directions in contemporary machine learning, such as semi-supervised learning, structure learning in graphical models, hierarchical Bayesian modeling, and nonparametric Bayes.
Top/Computer_Science/Machine_Learning/Bayesian_Learning	Dynamic Bayesian Networks for Multimodal Interaction Dynamic Bayesian networks (DBNs) offer a natural upgrade path beyond classical hidden Markov models and become especially relevant when temporal data contains higher order structure, multiple modalities or multi-person interaction. We describe several instantiations of dynamic Bayesian networks that are useful for modeling temporal phenomena spanning audio, video and haptic channels in single, two-person and multi-person activity. These models include input-output hidden Markov models, switched Kalman filters and, most generally, dynamical systems trees (DSTs). These models are used to learn audio-video interaction in social activities, video interaction in multi-person game playing and haptic-video interaction in robotic laparoscopy. Model parameters are estimated from data in an unsupervised setting using generalized expectation maximization methods. Subsequently, these models can predict, synthesize and classify various types of rich multimodal human activity. Experiments in gesture interaction, audio-video conversation, football game playing and surgical drill evaluation are shown.
Top/Computer_Science/Machine_Learning/Bayesian_Learning	Optimality of Bayesian Transduction - Implications for Input Non-stationarity
Top/Computer_Science/Machine_Learning/Bayesian_Learning	A Bayesian Probability Calculus for Density Matrices One of the main concepts in quantum physics is a density matrix, which is a symmetric positive definite matrix of trace one. Finite probability distributions can be seen as a special case when the density matrix is restricted to be diagonal. We develop a probability calculus based on these more general distributions that includes definitions of joints conditionals and formulas that relate these, including analogs of the Theorem of Total Probability and various Bayes rules for the calculation of posterior density matrices. The resulting calculus parallels the familiar ``conventional probability calculus and always retains the latter as a special case when all matrices are diagonal.
Top/Computer_Science/Machine_Learning/Bayesian_Learning	Applications of Bayesian Sensitivity and Uncertainty Analysis to the Statistical Analysis of Computer Simulators for Carbon Dynamics Uncertainties about the dynamics of carbon in forest ecosystems have a major impact on defining and verifying policies, as is evident from the difficulties in ratifying the Kyoto protocol. Quantifying and reducing this uncertainty requires the combination of mathematical models for ecological processes, and earth observation data, within a unifying statistical framework. The Centre for Terrestrial Carbon Dynamics (CTCD) is developing several computer codes to simulate the relevant processes at different spatial and temporal scales. Inputs to theses codes at a given site describe the characteristics of the vegetation grown there. Soil and climate data are also used to drive the model. My talk will illustrate the use of efficient Bayesian tools both in the development of these codes and in their use for prediction and uncertainty reduction. The first step is to build an emulator of the computer code. The emulator is a statistical representation of the code output based on a Gaussian process prior model. From this we can derive inferences about a range of sensitivity and uncertainty measures: Sensitivity analysis is performed to find out the level of influence each input or group of inputs have on the output. This can lead to efficiency gains by revealing inactive inputs. Examination of the expected response curve of the output as a function of individual inputs has also uncovered a number of coding errors. Uncertainty analysis is employed to assess the uncertainty in the prediction resulting from the various uncertain input conditions. It also tells us where to concentrate research effort in reducing uncertainties in inputs if we want to reduce the total uncertainty in the output. Conventional approaches to sensitivity analysis and uncertainty analysis involve Monte Carlo sampling of code outputs. This is highly inefficient and is not feasible for complex models. Bayesian methods can reduce the required number of simulator runs by several orders of magnitude. I will also mention some extensions to the methodology that are being developed to handle the dynamic and multivariate nature of the CTCD vegetation models.
Top/Computer_Science/Machine_Learning/Bayesian_Learning	BayesANIL - A Bayesian Model for Handling Approximate, Noisy or Incomplete Labeling in Text Classification
Top/Computer_Science/Machine_Learning/Bayesian_Learning	Bayesian Data Fusion with Gaussian Process Priors : An Application to Protein Fold Recognition Various emerging quantitative measurement technologies are producing genome, transcriptome and proteome-wide data collections which has motivated the de- velopment of data integration methods within an inferential framework. It has been demonstrated that for certain prediction tasks within computational biol- ogy synergistic improvements in performance can be obtained via integration of a number of (possibly heterogeneous) data sources. In [1] six different parameter representations of proteins were employed for fold recognition of proteins using Support Vector Machines (SVM).
Top/Computer_Science/Machine_Learning/Bayesian_Learning	Bayesian Inference: Principles and Practice The aim of this course is two-fold: to convey the basic principles of Bayesian machine learning and to describe a practical implementation framework. Firstly, we will give an introduction to Bayesian approaches, focussing on the advantages of probabilistic modelling, the concept of priors, and the key principle of marginalisation. Secondly, we will exploit these ideas to realise practical algorithms for sparse linear regression and classification, as exemplified by models such as the 'relevance vector machine'.
Top/Computer_Science/Machine_Learning/Bayesian_Learning	Bayesian Kernel Methods
Top/Computer_Science/Machine_Learning/Bayesian_Learning	Bayesian Methods In the last decade probabilistic graphical models - in particular Bayes networks and Markov networks - became very popular as tools for structuring uncertain knowledge about a domain of interest and for building knowledge-based systems that allow sound and efficient inferences about this domain. The core idea of graphical models is that usually certain independence relations hold between the attributes that are used to describe a domain of interest. In most uncertainty calculi -- and in particular in probability theory -- the structure of these independence relations is very similar to properties concerning the connectivity of nodes in a graph. As a consequence, it is tried to capture the independence relations by a graph, in which each node represents an attribute and each edge a direct dependence between attributes. In addition, provided that the graph captures only valid independences, it prescribes how a probability distribution on the (usually high-dimensional) space that is spanned by the attributes can be decomposed into a set of smaller (marginal or conditional) distributions. This decomposition can be exploited to derive evidence propagation methods and thus enables sound and efficient reasoning under uncertainty. The lecture gives a brief introduction into the core ideas underlying graphical models, starting from their relational counterparts and highlighting the relation between independence and decomposition. Furthermore, the basics of model construction and evidence propagation are discussed, with an emphasis on join/junction tree propagation. A substantial part of the lecture is then devoted to learning graphical models from data, in which quantitative learning (parameter estimation) as well as the more complex qualitative or structural learning (model selection) are studied. The lecture closes with a brief discussion of example applications.
Top/Computer_Science/Machine_Learning/Bayesian_Learning	Bayesian Methods for Inverse Problems
Top/Computer_Science/Machine_Learning/Bayesian_Learning	Convergence of MDL and Bayesian Methods We introduce a complexity measure which we call KL-complexity. Based on this concept, we present a general information exponential inequality that measures the statistical complexity of some deterministic and randomized estimators. We show that simple and clean finite sample convergence bounds can be obtained from this approach. In particular, we are able to improve some classical results concerning the convergence of MDL density estimation and Bayesian posterior distributions
Top/Computer_Science/Machine_Learning/Bayesian_Learning	Dirichlet Processes and Nonparametric Bayesian Modelling Bayesian modeling is a principled approach to updating the degree of belief in a hypothesis given prior knowledge and given available evidence. Both prior knowledge and evidence are combined using Bayes' rule to obtain the a posterior hypothesis. In most cases of interest to machine learning, the prior knowledge is formulated as a prior distribution over parameters and the evidence corresponds to the observed data. By applying Bayes' formula we can perform inference about new data. Having observed sufficient data, the a posteriori parameter distribution is increasingly concentrated and the influence of the prior distribution diminishes. Under some assumptions (in particular that the likelihood model is correct and that the true parameters have positive a priori probability), the a posteriori distribution converges to a point distribution located at the true parameters. The challenges in Bayesian modeling are, first, to find suitable application specific statistical models and, second, to (approximately) solve the resulting inference equations.
Top/Computer_Science/Machine_Learning/Bayesian_Learning	Efficient discriminative learning of Bayesian network classifier via Boosted Augmented Naive Bayes The use of Bayesian networks for classification problems has received significant recent attention. Although computationally efficient, the standard maximum likelihood learning method tends to be suboptimal due to the mismatch between its optimization criteria (data likelihood) and the actual goal for classification (label prediction). Recent approaches to optimizing the classification performance during parameter or structure learning show promise, but lack the favorable computational properties of maximum likelihood learning. In this paper we present the Boosted Augmented Naive Bayes (BAN) classifier. We show that a combination of discriminative data-weighting with generative training of intermediate models can yield a computationally efficient method for discriminative parameter learning and structure selection.
Top/Computer_Science/Machine_Learning/Bayesian_Learning	Empirical Bayesian test for the smoothness In the context of adaptive nonparametric curve estimation problem, a common assumption is that the function (signal) to estimate belongs to a nested family of functional classes, parameterized by a quantity which often has a meaning of smoothness amount. It has already been realized that the problem of estimating the smoothness is meaningless. What can then be inferred about the smoothness? We try to answer this question and discuss the implications of our results for the hypothesis testing problem for the smoothness parameter. The imbedded model structure (nested family of classes) accounts for the fact that a consistent test can be constructed only for the one-sided hypothesis. The test statistic is based on the marginalized maximum likelihood estimator of the smoothness for the appropriate choice of the prior distribution on the unknown signal.
Top/Computer_Science/Machine_Learning/Bayesian_Learning	Experimental Design for Efficient Identification of Gene Regulatory Networks using Sparse Bayesian Models Identifying large gene regulatory networks is an important task, while the acquisition of data through perturbation experiments (e.g., gene switches, RNAi) is expensive. It is thus desirable to use an identification method that effectively incorporates available prior knowledge --- such as sparse connectivity --- and that allows to design experiments such that maximal information is gained from each one. Our main contributions are twofold: a method for consistent inference of network structure is provided, incorporating prior knowledge about sparse connectivity. The algorithm is time efficient and robust to violations of model assumptions. Moreover, we show how to use it for optimal experimental design, reducing the number of required experiments substantially. We employ sparse linear models, and show how to perform full Bayesian inference for these. We not only estimate a single maximum likelihood network, but compute a posterior distribution over networks, using a novel variant of the expectation propagation method. The representation of uncertainty enables us to do effective experimental design in a standard statistical setting: experiments are selected such that on average the experiments are maximally informative. Few methods have addressed the design issue so far. Compared to the most well-known one, our method is more transparent, and is shown to perform qualitatively superior. In the former, hard and unrealistic constraints have to be placed on the network structure for mere computational tractability, while such are not required in our method. We demonstrate reconstruction and optimal experimental design capabilities on tasks generated from realistic non-linear network simulators. Joint work with Florian Steinke and Koji Tsuda.
Top/Computer_Science/Machine_Learning/Bayesian_Learning	Generative Models for Visual Objects and Object Recognition via Bayesian Inference
Top/Computer_Science/Machine_Learning/Bayesian_Learning	How Random is a Coin Toss? Bayesian Inference and the Symbolic Dynamics of Deterministic Chaos
Top/Computer_Science/Machine_Learning/Bayesian_Learning	Multitask learning: the Bayesian way Multi-task learning lends itself particularly well to a Bayesian approach. Cross-inference between tasks can be implemented by sharing parameters in the likelihood model and the prior for the task-specific model parameters. Choosing different priors, one can implement task clustering and task gating. Throughout my presentation, predicting single-copy newspaper sales will serve as a running example.
Top/Computer_Science/Machine_Learning/Bayesian_Learning	Nonparametric Bayesian Models in Machine Learning Bayesian methods make it possible to handle uncertainty in a principled manner, sidestep the problem of overfitting, and incorporate domain knowledge. However, most parametric models are too limited to adequately model complex real-world problems. Thus, interest has shifted to nonparametric models which can capture much richer and more complex probability distributions. This talk will review some of the core nonparametric tools for regression and classification (Gaussian processes; GPs) and density estimation (Dirichlet process mixtures). We will then focus on extensions of these basic tools (such as mixtures of GPs, warped GPs, and GPs for ordinal regression) and approximation methods which allow efficient inference in these models (such as expectation propagation; EP).
Top/Computer_Science/Machine_Learning/Bayesian_Learning	Objective Bayesian Nets for Breast Cancer Prognosis According to objective Bayesianism, an agents degrees of belief should be determined by a probability function, out of all those that satisfy constraints imposed by background knowledge, that maximises entropy. A Bayesian net offers a way of efficiently representing a probability function and efficiently drawing inferences from that function.
Top/Computer_Science/Machine_Learning/Bayesian_Learning	Part 1: A Novel Bayesian Approach for Uncovering Potential Spectroscopic Counterparts for Clinical Variables in 1H NMR Metabonomic Applications Metabonomic approaches based on spectroscopic data are in their infancy in biomedicine. A key challenge in clinical metabonomics is uncovering and understanding the relations between the multidimensional spectroscopic data and the clinical measures currently used for disease risk assessment and diagnostics. A novel Bayesian approach for revealing clinically relevant signals is presented here for a real 1H NMR metabonomics data set. The results are not only mathematically superior but also biochemically fully coherent.
Top/Computer_Science/Machine_Learning/Bayesian_Learning	Part 2: A Novel Bayesian Approach for Uncovering Potential Spectroscopic Counterparts for Clinical Variables in 1H NMR Metabonomic Applications Metabonomic approaches based on spectroscopic data are in their infancy in biomedicine. A key challenge in clinical metabonomics is uncovering and understanding the relations between the multidimensional spectroscopic data and the clinical measures currently used for disease risk assessment and diagnostics. A novel Bayesian approach for revealing clinically relevant signals is presented here for a real 1H NMR metabonomics data set. The results are not only mathematically superior but also biochemically fully coherent.
Top/Computer_Science/Machine_Learning/Bayesian_Learning	Probabilistic and Bayesian Modelling I There is a dramatic growth in the availability of complex data from a wide range of different applications. The challenge of the data analyzer is to extract knowledge from the raw data by identifying the useful patterns and structures that underlie it. This module introduces adaptive and probabilistic approaches to modeling such complex data. We first consider finding structure in high-dimensional data. The kernel methods approach to identifying non-linear patterns in introduced while addressing the issues of statistical reliability of inferences made from limited data. Subspace identification is considered and correlations across different data modalities are shown to provide a useful approach to eliciting semantic representations. The final section of the course will introduce learning probabilistic models, (e.g. in biological sequence data), fusing prior knowledge and data, complex and approximate inference.
Top/Computer_Science/Machine_Learning/Bayesian_Learning	Probabilistic and Bayesian Modelling II There is a dramatic growth in the availability of complex data from a wide range of different applications. The challenge of the data analyzer is to extract knowledge from the raw data by identifying the useful patterns and structures that underlie it. This module introduces adaptive and probabilistic approaches to modeling such complex data. We first consider finding structure in high-dimensional data. The kernel methods approach to identifying non-linear patterns in introduced while addressing the issues of statistical reliability of inferences made from limited data. Subspace identification is considered and correlations across different data modalities are shown to provide a useful approach to eliciting semantic representations. The final section of the course will introduce learning probabilistic models, (e.g. in biological sequence data), fusing prior knowledge and data, complex and approximate inference.
Top/Computer_Science/Machine_Learning/Bayesian_Learning	Probability, Information Theory and Bayesian Inference
Top/Computer_Science/Machine_Learning/Bayesian_Learning	Reconstructing Transcriptional Networks using Bayesian State Space Model A major challenge in systems biology is the ability to model complex regulatory interactions. In previous work, we have used Linear-Gaussian state-space models (SSMs), also known as Linear Dynamical Systems (LDS) or Kalman filter models to 'reverse-engineer' regulatory networks from high-throughput data sources, such as microarray gene expression profiling. SSM models are a subclass of dynamic Bayesian networks used for modeling time series data and have been used extensively in many areas of control and signal processing. The parameters of an SSM can be learned using maximum likelihood (ML) methods. However, in general the ML approach is prone to overfitting, especially when fitting models with many variables with relatively small amounts of data. We have instead turned to a fully Bayesian analysis, which avoids overfitting and provides error bars on all model parameters ? in this paradigm the objective function is simply the probability of the data, that which results from integrating out the parameters of the model with respect to their prior distribution. Optimizing a model with respect to such an objective function avoids overfitting in the conventional sense. In practice, a Bayesian learning scheme infers distributions over all the parameters and makes modeling predictions by taking into account all possible parameter settings. In doing so we penalize models with too many parameters, embodying an automatic Occam's Razor effect. We describe results from simulation studies based on synthetic mRNA time series data. Receiver Operating Characteristic (ROC) analysis demonstrates an overall accuracy in transcriptional network reconstruction from the mRNA time series measurements alone of approximately 68% Area Under the Curve (AUC) for 12 time points and better still for data sampled at a higher rate. Incorporation of prior information about known regulatory connections improves this accuracy in a fashion which appears be be linear with the number of known connections included. The implications of these simulation studies for experimental design will be discussed. Joint work with Matthew J. Beal and Juan Li.
Top/Computer_Science/Machine_Learning/Bayesian_Learning	Universal Coding/Prediction and Statistical (In)consistency of Bayesian inference Part of this talk is based on results of A. Barron (1986) and recent joint work with J. Langford (2004). We introduce the information-theoretic concepts of universal coding and prediction. Under weak conditions on the prior, Bayesian sequential prediction is universal. This means that a code based on the Bayesian predictive distribution allows one to substantially compress data. We give a simple proof of the fact that universality implies consistency of the Bayesian posterior. It follows that Bayesian inconsistency in nonparametric settings (a la Diaconis & Freedman) can only occur if priors are used that do not allow for data compression. This gives a frequentist rationale for Rissanen's Minimum Description Length Principle. We also show that under misspecification, the Bayesian predictions can substantially outperform the predictions of the best distribution in the model. Ironically, this implies that the Bayesian posterior can become *inconsistent*: in some sense good predictive performance implies inconsistency!
Top/Computer_Science/Machine_Learning/Bayesian_Learning	Variational Bayesian methods for audio indexing
Top/Computer_Science/Machine_Learning/Bayesian_Learning	Gaussian Process Temporal Difference Although Bayesian methods for Reinforcement Learning can be traced back to the 1960s (Howard's work in Operations Research), Bayesian methods have only been used sporadically in modern Reinforcement Learning. This is in part because non-Bayesian approaches tend to be much simpler to work with. However, recent advances have shown that Bayesian approaches do not need to be as complex as initially thought and offer several theoretical advantages. For instance, by keeping track of full distributions (instead of point estimates) over the unknowns, Bayesian approaches permit a more comprehensive quantification of the uncertainty regarding the transition probabilities, the rewards, the value function parameters and the policy parameters. Such distributional information can be used to optimize (in a principled way) the classic exploration/exploitation tradeoff, which can speed up the learning process. Similarly, active learning for reinforcement learning can be naturally optimized. The estimation of gradient performance with respect to value function or and/or policy parameters can also be done more accurately while using less data. Bayesian approaches also facilitate the encoding of prior knowledge and the explicit formulation of domain assumptions. The primary goal of this tutorial is to raise the awareness of the research community with regard to Bayesian methods, their properties and potential benefits for the advancement of Reinforcement Learning. An introduction to Bayesian learning will be given, followed by a historical account of Bayesian Reinforcement Learning and a description of existing Bayesian methods for Reinforcement Learning. The properties and benefits of Bayesian techniques for Reinforcement Learning will be discussed, analyzed and illustrated with case studies.
Top/Computer_Science/Machine_Learning/Bayesian_Learning	Model-based Bayesian RL Although Bayesian methods for Reinforcement Learning can be traced back to the 1960s (Howard's work in Operations Research), Bayesian methods have only been used sporadically in modern Reinforcement Learning. This is in part because non-Bayesian approaches tend to be much simpler to work with. However, recent advances have shown that Bayesian approaches do not need to be as complex as initially thought and offer several theoretical advantages. For instance, by keeping track of full distributions (instead of point estimates) over the unknowns, Bayesian approaches permit a more comprehensive quantification of the uncertainty regarding the transition probabilities, the rewards, the value function parameters and the policy parameters. Such distributional information can be used to optimize (in a principled way) the classic exploration/exploitation tradeoff, which can speed up the learning process. Similarly, active learning for reinforcement learning can be naturally optimized. The estimation of gradient performance with respect to value function or and/or policy parameters can also be done more accurately while using less data. Bayesian approaches also facilitate the encoding of prior knowledge and the explicit formulation of domain assumptions. The primary goal of this tutorial is to raise the awareness of the research community with regard to Bayesian methods, their properties and potential benefits for the advancement of Reinforcement Learning. An introduction to Bayesian learning will be given, followed by a historical account of Bayesian Reinforcement Learning and a description of existing Bayesian methods for Reinforcement Learning. The properties and benefits of Bayesian techniques for Reinforcement Learning will be discussed, analyzed and illustrated with case studies.
Top/Computer_Science/Machine_Learning/Bayesian_Learning	Learning Bayesian Networks Bayesian networks are graphical structures for representing the probabilistic relationships among a large number of variables and doing probabilistic inference with those variables. The 1990's saw the emergence of excellent algorithms for learning Bayesian networks from passive data. I will discuss the constraint-based learning method using an intuitive approach that concentrates on causal learning. Then I will discuss the Bayesian approach with some simple examples. I will show how, using the Bayesian approach, we can even learning something about causal influences from passive data on two variables. Finally, I will show some applications to finance and marketing.
Top/Computer_Science/Machine_Learning/Bayesian_Learning	Demo - Control of an octopus arm using GPTD Although Bayesian methods for Reinforcement Learning can be traced back to the 1960s (Howard's work in Operations Research), Bayesian methods have only been used sporadically in modern Reinforcement Learning. This is in part because non-Bayesian approaches tend to be much simpler to work with. However, recent advances have shown that Bayesian approaches do not need to be as complex as initially thought and offer several theoretical advantages. For instance, by keeping track of full distributions (instead of point estimates) over the unknowns, Bayesian approaches permit a more comprehensive quantification of the uncertainty regarding the transition probabilities, the rewards, the value function parameters and the policy parameters. Such distributional information can be used to optimize (in a principled way) the classic exploration/exploitation tradeoff, which can speed up the learning process. Similarly, active learning for reinforcement learning can be naturally optimized. The estimation of gradient performance with respect to value function or and/or policy parameters can also be done more accurately while using less data. Bayesian approaches also facilitate the encoding of prior knowledge and the explicit formulation of domain assumptions. The primary goal of this tutorial is to raise the awareness of the research community with regard to Bayesian methods, their properties and potential benefits for the advancement of Reinforcement Learning. An introduction to Bayesian learning will be given, followed by a historical account of Bayesian Reinforcement Learning and a description of existing Bayesian methods for Reinforcement Learning. The properties and benefits of Bayesian techniques for Reinforcement Learning will be discussed, analyzed and illustrated with case studies.
Top/Computer_Science/Machine_Learning/Bayesian_Learning	Bayesian inference and Gaussian processes
Top/Computer_Science/Machine_Learning/Bayesian_Learning	Dirichlet Processes: Tutorial and Practical Course **The Bayesian approach** allows for a coherent framework for dealing with uncertainty in machine learning. By integrating out parameters, Bayesian models do not suffer from overfitting, thus it is conceivable to consider models with infinite numbers of parameters, aka Bayesian nonparametric models. An example of such models is the Gaussian process, which is a distribution over functions used in regression and classification problems. Another example is the Dirichlet process, which is a distribution over distributions. Dirichlet processes are used in density estimation, clustering, and nonparametric relaxations of parametric models. It has been gaining popularity in both the statistics and machine learning communities, due to its computational tractability and modelling flexibility. In the tutorial I shall introduce Dirichlet processes, and describe different representations of Dirichlet processes, including the Blackwell-MacQueen? urn scheme, Chinese restaurant processes, and the stick-breaking construction. I shall also go through various extensions of Dirichlet processes, and applications in machine learning, natural language processing, machine vision, computational biology and beyond. In the practical course I shall describe inference algorithms for Dirichlet processes based on Markov chain Monte Carlo sampling, and we shall implement a Dirichlet process mixture model, hopefully applying it to discovering clusters of NIPS papers and authors.
Top/Computer_Science/Machine_Learning/Bayesian_Learning	Tree Augmented Naive Bayes for Regression Using Mixtures of Truncated Exponentials: Application to Higher Education Management In this paper we explore the use of Tree Augmented Naive Bayes (TAN) in regression problems where some of the independent variables are continuous and some others are discrete. The proposed solution is based on the approximation of the joint distribution by a Mixture of Truncated Exponentials (MTE). The construction of the TAN structure requires the use of the conditional mutual information, which cannot be analytically obtained for MTEs. In order to solve this problem, we introduce an unbiased estimator of the conditional mutual information, based on Monte Carlo estimation. We test the performance of the proposed model in a real life context, related to higher education management, where regression problems with discrete and continuous variables are common. This work has been supported by the Spanish Ministry of Education and Science, project TIN2004-06204-C03-01 and by Junta de Andaluca, project P05-TIC-00276.
Top/Computer_Science/Machine_Learning/Bayesian_Learning	Large-scale Bayesian Inference for Collaborative Filtering The Netflix prize problem provides an excellent testing ground for machine learning. The problem is large scale and the data complex and noisy. It is therefore likely that relatively complex models with careful regularization are needed in order to get reasonable predictions. A Bayesian modeling approach seems ideal for the task if it is possible to scale it up to the size of the Netflix data set, where extremely high-dimensional Bayesian expectations will possibly have to be approximated. In this talk, an ordinal regression low-rank matrix decomposition model is presented. We use a variational Bayes (VB) inference algorithm to demonstrate that it is possible to make a large scale Bayesian algorithm. This model also highlight some of the general limitations of VB. The more accurate expectation propagation/expectation consistent (EP/C) inference cannot be applied to this bi-linear model without further approximations. We therefore propose a hybrid approach with EP/C inspired modifications of the VB algorithm. We compare the different variational approximations with a Laplace approximation, a MAP approximation and a Hamiltonian MCMC. In the latter one sample takes around 6 hours of computing time on a 1GHz processor, with fast C++ code, so there is a very clear case to be made for deterministic approximate inference. Another good feature of the Netflix data is the magnitude of the the test set which makes even small differences in the performance significant.
Top/Computer_Science/Machine_Learning/Bayesian_Learning	Gene regulatory network reconstruction by Bayesian integration of prior knowledge and/or different experimental conditions There have been various attempts to improve the reconstruction of gene regulatory networks from microarray data by the systematic integration of biological prior knowledge. Our approach follows the Bayesian paradigm where the prior knowledge is expressed in terms of energy functions, from which a prior distribution over network structures is obtained in the form of a Gibbs distribution. The hyperparameters of this distribution represent the weights associated with the prior knowledge relative to the data. We have derived and tested an MCMC scheme for sampling networks and hyperparameters simultaneously from the posterior distribution, thereby automatically learning how to trade off information from the prior and the data. We have extended this approach to a Bayesian coupling scheme for learning gene regulatory networks from a combination of related data sets that were obtained under different experimental conditions and are therefore potentially associated with different active subpathways.
Top/Computer_Science/Machine_Learning/Bayesian_Learning	Bayesian methods for data Modelling His presentation introduces the basic ideas of Bayesian methods for data modelling.
Top/Computer_Science/Machine_Learning/Bayesian_Learning	Time delay analysis Bayesian Inference and Markov Chain Monte Carlo methods have been ad- vocated for the estimation of model parameters from ODEs by Rogers et al. (Bayesian model-based inference of transcription factor activity, BMC Bioin- formatics, 8(2), 2006). We look at some of the issues involved in extend- ing Bayesian inference methods to systems containing time delays. Verdugo and Rand (Hopf bifurcation in a DDE model of gene expression, Commu- nications in Nonlinear Science and Numerical Simulation, 13:235-242, 2008) apply Lindstedt's method to the nonlinear system of delay dierential equa- tions proposed as a model by Monk (Oscillatory Expression of Hes1, p53 and NF B Driven by Transcriptional Time Delays, Current Biology, 13:1409- 1413, 2003) for the Hes1 feedback loop, resulting in closed form approximate expressions for the amplitude and frequency of oscillation. Analysis shows that oscillatory solutions can arise through Hopf bifurcation in the delay pa- rameter. We extend the work of Verdugo and Rand to the more realistic case where the decay parameters of hes1 mRNA and Hes1 protein, key com- ponents of the feedback, are not equal, focusing on oscillatory behaviours. We aim for results that explain how the model parameters aect the system dynamics and hence could be used to inform a parameter estimation from expression data. We illustrate our results by applying Bayesian inference to some real biological data. It has been observed that mRNAs for Notch signalling molecules such as the bHLH factor Hes1 oscillate with 2-hour cycles during somite segmentation. Hirata et al. (Oscillatory Expression of the bHLH Factor Hes1 Regulated by a Negative Feedback Loop, Science 298, 840-843, 2002) investigated the molec- ular mechanism behind observed oscillations of mRNAs for Notch signalling molecules. They examined the time course of hes1 mRNA in detail. Hirata et al. measured the half lives of hes1 mRNA and Hes1 protein and identied the proteases for Hes1 protein degradation. Their experiments show that the degradation of Hes1 protein is required for Hes1 mRNA increase and that de novo production of the protein is required for reduction of hes1 mRNA. These facts together support their theory that Hes1 is an essential compo- nent of a two hour cycle clock and not just an output of a primary clock. The Hirata data comprises scaled hes1 mRNA expression level every 30 min- utes over a 12 hour period. Monk's model was able to explain, via numerical simulations, the oscillation of hes1 mRNA and Hes1 protein in cultured cells observed by Hirata et al. We use a Bayesian approach to the parameter t- ting problem which takes into account the inherent uncertainity in the data and uses our a priori bifurcation analysis to inform the choice of priors.
Top/Computer_Science/Machine_Learning/Bayesian_Learning	Probabilistic inference methods in robotics-filling the gap between high-level reasoning and low-level motion control
Top/Computer_Science/Machine_Learning/Bayesian_Learning	Shrinkage Estimator for Bayesian Network Parametrs
Top/Computer_Science/Machine_Learning/Bayesian_Learning	Learning Bayesian networks from postgenomic data with an improved structure MCMC sampling scheme Our paper contributes to recent research on sampling Bayesian network structures from the poste- rior distribution with MCMC. Two principled paradigms have been applied in the past. Structure MCMC, first proposed by Madigan and York, defines a Markov chain in the space of graph struc- tures by applying basic operations to individual edges of the graph, like the creation, deletion or reversal of an edge. Alternatively, order MCMC, proposed by Friedman and Koller, defines a Markov chain in the space of node orders. While the second approach has been found to sub- stantially improve the mixing and convergence of the Markov chain, it does not allow an explicit specification of the prior distribution over graph structures or, to phrase this difierently, it incurs a distortion of the specified prior distribution as a consequence of the marginalization over node or- ders. This distortion can lead to problems for applications in systems biology, where owing to the limited number of experimental conditions the integration of biological prior knowledge into the inference scheme becomes desirable. Diferent approaches and modifications have been developed in the literature to address this shortcoming (e.g. by Ellis, Eaton and Murphy). Unfortunately, these methods incur extra computational costs and are not practically viable for inferring large networks with more than 20 to 30 nodes. There have been suggestions of how to improve the classical structure MCMC approach by using the concept of the inclusion boundary, as proposed by Castelo and Kocka, but these methods only partially address the convergence and mixing problems. In the present paper we propose a novel structure MCMC scheme, which augments the classical structure MCMC method of Madigan and York with a novel edge reversal move. The idea of the new move is to resample the parent sets of the two nodes involved in such a way that the selected edge is reversed subject to the acyclicity constraint. The proposal of the new parent sets is done efectively by adopting ideas from importance sampling; in this way faster convergence is efected. For methodological consistency, and in contrast to inclusion-driven MCMC, we have properly derived the Hastings factor, which is a function of various partition functions that are straightforward to compute. The resulting Markov chain is reversible, satisfies the condition of detailed balance, and is hence guaranteed to theoretically converge to the desired posterior dis- tribution. For our empirical evaluation, we have tested our method on various data sets from the UCI repository, such as Vote, Flare, Boston Housing, and Alarm, which have previously been used by Friedman and Koller to demonstrate that order MCMC outperforms structure MCMC. Our experimental results show that integrating the novel edge reversal move yields a substantial improvement of the resulting MCMC sampler over classical structure MCMC, with convergence and mixing properties that are similar to those of order MCMC. To demonstrate the avoidance of the distortional effect incurred with order MCMC, we have extended our empirical evaluation by analysing ow cytometry protein concentrations from the Raf-Mek-Erk signalling pathway. The experimental results show that the novel MCMC scheme can lead to a slight yet significant performance improvement over order MCMC when explicit prior knowledge is integrated into the learning scheme. This suggests that the avoidance of any systematic distortion of the prior probability distribution on network structures renders our improved structure MCMC sampler preferable to order MCMC, especially for those contemporary systems biology applications where the number of experimental conditions relative to the complexity of the investigated system, and hence the weight of the likelihood, is relatively low, and explicit prior knowledge about network structures from publicly accessible data bases is included.
Top/Computer_Science/Machine_Learning/Bayesian_Learning	Topic Models Conditioned on Arbitrary Features with Dirichlet-multinomial Regression Although fully generative models have been successfully used to model the contents of text documents, they are often awkward to apply to combinations of text data and document metadata. In this paper we propose a Dirichlet-multinomial regression (DMR) topic model that includes a log-linear prior on document-topic distributions that is a function of observed features of the document, such as author, publication venue, references, and dates. We show that by selecting appropriate features, DMR topic models can meet or exceed the performance of several previously published topic models designed for specific data.
Top/Computer_Science/Machine_Learning/Bayesian_Learning	Multi-Task Compressive Sensing with Dirichlet Process Priors Compressive sensing (CS) is an emerging field that, under appropriate conditions, can significantly reduce the number of measurements required for a given signal. In many applications, one is interested in multiple signals that may be measured in multiple CS-type measurements, where here each signal corresponds to a sensing 'task'. In this paper we propose a novel multi-task compressive sensing framework based on a Bayesian formalism, where a Dirichlet process (DP) prior is employed, yielding a principled means of simultaneously inferring the appropriate sharing mechanisms as well as CS inversion for each task. A variational Bayesian (VB) inference algorithm is employed to estimate the full posterior on the model parameters.
Top/Computer_Science/Machine_Learning/Bayesian_Learning	Dirichlet Component Analysis: Feature Extraction for Compositional Data We consider feature extraction (dimensionality reduction) for compositional data, where the data vectors are constrained to be positive and constant-sum. In real-world problems, the data components (variables) usually have complicated 'correlations' while their total number is huge. Such scenario demands feature extraction. That is, we shall de-correlate the components and reduce their dimensionality. Traditional techniques such as the Principle Component Analysis (PCA) are not suitable for these problems due to unique statistical properties and the need to satisfy the constraints in compositional data. This paper presents a novel approach to feature extraction for compositional data. Our method first identifies a family of dimensionality reduction projections that preserve all relevant constraints, and then finds the optimal projection that maximizes the estimated Dirichlet precision on projected data. It reduces the compositional data to a given lower dimensionality while the components in the lower-dimensional space are de-correlated as much as possible. We develop theoretical foundation of our approach, and validate its effectiveness on some synthetic and real-world datasets.
Top/Computer_Science/Machine_Learning/Bayesian_Learning	Should all Machine Learning be Bayesian? Should all Bayesian models be non-parametric? I'll present some thoughts and research directions in Bayesian machine learning. I'll contrast black-box approaches to machine learning with model-based Bayesian statistics. Can we meaningfully create Bayesian black-boxes? If so what should the prior be? Is non-parametrics the only way to go? Since we often can't control the effect of using approximate inference, are coherence arguments meaningless? How can we convert the pagan majority of ML researchers to Bayesianism? If the audience gets bored of these philosophical musings, I will switch to talking about our latest technical work on Indian buffet processes.
Top/Computer_Science/Machine_Learning/Bayesian_Learning	Learning the Bayesian Network Structure: Dirichlet Prior versus Data In the Bayesian approach to structure learning of graphical models, the equivalent sample size (ESS) in the Dirichlet prior over the model parameters was recently shown to have an important effect on the maximum-a-posteriori estimate of the Bayesian network structure. In our first contribution, we theoretically analyze the case of large ESS-values, which complements previous work: among other results, we find that the presence of an edge in a Bayesian network is favored over its absence even if both the Dirichlet prior and the data imply independence, as long as the conditional empirical distribution is notably different from uniform. In our second contribution, we focus on realistic ESS-values, and provide an analytical approximation to the optimal ESS-value in a predictive sense (its accuracy is also validated experimentally): this approximation provides an understanding as to which properties of the data have the main effect determining the optimal ESS-value.
Top/Computer_Science/Machine_Learning/Bayesian_Learning	Bayesian Inference of Mechanistic Systems Models Using Population MCMC We demonstrate how Population Markov Chain Monte Carlo techniques may be used to sample from the complex posterior distributions which arise when estimating parameters over nonlinear mechanistic mathematical models of biological processes given noisy data. Further, we show how the samples obtained may be employed, using a Power Posteriors method, to accurately calculate the marginal likelihoods and Bayes factors over such models.
Top/Computer_Science/Machine_Learning/Bayesian_Learning	Dirichlet Processes, Chinese Restaurant Processes, and all that Bayesian approaches to learning problems have many virtues, including their ability to make use of prior knowledge and their ability to link related sources of information, but they also have many vices, notably the strong parametric assumptions that are often invoked willy-nilly in practical Bayesian modeling. Nonparametric Bayesian methods offer a way to make use of the Bayesian calculus without the parametric handcuffs. In this talk I describe several recent explorations in nonparametric Bayesian modeling and inference, including various versions of 'Chinese restaurant process priors' that allow flexible structures to be learned and allow sharing of statistical strength among sets of related structures. I discuss applications to problems in bioinformatics and information retrieval.
Top/Computer_Science/Machine_Learning/Pattern_Recognition	Graphical Models for Structural Pattern Recognition In the 'structural' paradigm for visual pattern recognition, or what some call 'strong' pattern recognition, one is not satisfied with simply assigning a class label to an input object, but instead we aim at finding exactly which parts of the template object correspond to which parts of the scene. This is a much harder problem in principle, because it is inherently combinatorial on the number of parts (features) involved, both in the template object and in the scene. This talk describes a summary of our research efforts in setting this as a mathematical optimization problem and solving it efficiently by exploiting geometric constraints. The key insight involves encoding geometric constraints as conditional independency assumptions in a probabilistic graphical model. Due to some geometric facts, it is possible to show that such models are very well behaved: they allow for exact probabilistic inference in polynomial time. The result is a unified framework for structural visual pattern recognition that is able to handle in a principled way a variety of problems, including point pattern matching in its many instances: invariant to translations, isometries, scalings, affine or projective transformations. Attributed graph matching problems, such as matching road networks, can also be solved within such framework. Limitations and future directions will be discussed.
Top/Computer_Science/Machine_Learning/Pattern_Recognition	Kernel-Based Methods for Pattern Recognition
Top/Computer_Science/Machine_Learning/Pattern_Recognition	Algorithmic and Combinatorial Foundations of Pattern Discovery
Top/Computer_Science/Machine_Learning/Pattern_Recognition	An Introduction to Pattern Classification
Top/Computer_Science/Machine_Learning/Pattern_Recognition	Fast Best-Effort Pattern Matching in Large Attributed Graphs We focus on large graphs where nodes have attributes, such as a social network where the nodes are labelled with each persons job title. In such a setting, we want to find subgraphs that match a user query pattern. For example, a star query would be, find a CEO who has strong interactions with a Manager, a Lawyer, and an Accountant, or another structure as close to that as possible. Similarly, a loop query could help spot a money laundering ring. Traditional SQL-based methods, as well as more recent graph indexing methods, will return no answer when an exact match does not exist. Our method can find exact-, as well as near-matches, and it will present them to the user in our proposed goodness order. For example, our method tolerates indirect paths between, say, the CEO and the Accountant of the above sample query, when direct paths do not exist. Its second feature is scalability. In general, if the query has nq nodes and the data graph has n nodes, the problem needs polynomial time complexity O(nnq ), which is prohibitive. Our G-Ray (Graph X-Ray) method finds high-quality subgraphs in time linear on the size of the data graph. Experimental results on the DLBP author-publication graph (with 356K nodes and 1.9M edges) illustrate both the effectiveness and scalability of our approach. The results agree with our intuition, and the speed is excellent. It takes 4 seconds on average for a 4- node query on the DBLP graph.
Top/Computer_Science/Machine_Learning/Pattern_Recognition	Finding frequent patterns from data Discovery of frequent patterns = finding positive conjunctions that are true for a given fraction of the observations - this basic idea can be instantiated in many ways: - finding frequent sets from 0/1 data (association mining) - finding frequent episodes in sequences - finding frequent subgraphs in graphs etc. - efficient algorithms exist -- the levelwise approach - theoretical analysis of the algorithms is not trivial - leads to connections to hypergraph transversals etc. - the second part: how can the patterns be used? - sometimes interesting in themselves - can be used to approximate the joint distribution - maximum entropy approaches - combining information from several patterns - ordering patterns
Top/Computer_Science/Machine_Learning/Pattern_Recognition	Identifying Temporal Patterns and Key Players in Document Collections We consider the problem of analyzing the development of a document collection over time without requiring meaningful citation data. Given a collection of timestamped documents, we formulate and explore the following two questions. First, what are the main topics and how do these topics develop over time? Second, to gain insight into the dynamics driving this development, what are the documents and who are the authors that are most influential in this process? Unlike prior work in citation analysis, we propose methods addressing these questions without requiring the availability of citation data. The methods use only the text of the documents as input. Consequentially, they are applicable to a much wider range of document collections (email, blogs, etc.), most of which lack meaningful citation data. We evaluate our methods on the proceedings of the Neural Information Processing Systems (NIPS) conference. Even with the preliminary methods that we implemented, the results show that the methods are effective and that addressing the questions based on the text alone is feasible. In fact, the text-based methods sometimes even identify influential papers that are missed by citation analysis.
Top/Computer_Science/Machine_Learning/Pattern_Recognition	Learning patterns in omic data: applications of learning theory
Top/Computer_Science/Machine_Learning/Pattern_Recognition	Pattern Analysis with Graphs and Trees Spectral representations of graphs, Pattern spaces from graph spectra, Spectral approaches to matching, Heat kernel methods Probabilistic and spectral methods for graph matching and clustering. Applications in computer vision.
Top/Computer_Science/Machine_Learning/Pattern_Recognition	Pattern Classification and Large Margin Classifiers These lectures will provide an introduction to the theory of pattern classification methods. They will focus on relationships between the minimax performance of a learning system and its complexity. There will be four lectures. The first will review the formulation of the pattern classification problem, and several popular pattern classification methods, and present general risk bounds in terms of Rademacher averages, a measure of the complexity of a class of functions. The second lecture will consider pattern classification in a minimax setting, and show that, in this setting, the Vapnik-Chervonenkis dimension is the key measure of complexity. The third lecture will focus on a theme of computational complexity. It will present the elegant relationship between the complexity of a class, as measured by its VC-dimension, and the computational complexity of functions from the class. This lecture will also review general results on the computational complexity of the pattern classification problem, and its tight relationship with that of an associated empirical risk optimization problems. The fourth lecture will consider large margin classification methods, such as AdaBoost, support vector machines, and neural networks, viewing them as convex relaxations of intractable empirical minimization problems. It will review several statistical properties of these large margin methods, in particular, a characterization of the convex optimization problems that lead to accurate classifiers, and relationships between these methods and probability models.
Top/Computer_Science/Machine_Learning/Pattern_Recognition	Patterns in sets of points: an overview 1 - 'Patterns in sets of points: an overview' 'We illustrate the importance of optimization principles in the search for interesting patterns, more in particular for patterns in sets of points embedded in a metric space. This talk will be a journey along the types of patterns in point sets that can efficiently be searched for, and general principles will be outlined. We provide examples from dimensionality reduction, classification, clustering, and others. The emphasis will be on patterns that can be expressed in terms of linear functions of the data.'
Top/Computer_Science/Machine_Learning/Pattern_Recognition	Patterns in sets of points: the myriad virtues of eigenproblems 2-'Patterns in sets of points: the myriad virtues of eigenproblems' :-) 'In the second hour, one specific powerful type of optimization problem will be highlighted: the eigenvalue problem. A brief discussion of the computational aspects, and an overview of its applications in finding patterns in point sets will be provided. The talk will cover principal component analysis, canonical correlation analysis, Fisher's discriminant, partial least squares, and spectral clustering. We will emphasize connections between these algorithms where appropriate.'
Top/Computer_Science/Machine_Learning/Pattern_Recognition	Patterns, Randomness and Information Information, Complexity, Patterns, Randomness and Compression. And how these ideas can be traced back through Hermann Weyl to Leibniz in 1686, and connect them with Godel &amp; Turing and with the question of how math compares &amp; contrasts with physics and with biology.
Top/Computer_Science/Machine_Learning/Pattern_Recognition	Research 16: Evaluating Conjunctive Triple Pattern Queries over Large Structured Overlay Networks
Top/Computer_Science/Machine_Learning/Pattern_Recognition	Statistical Aspects of Pattern Analysis Abstract: The lectures will introduce the role of statistics in pattern analysis with a discussion of the difference between pattern significance and pattern stability. We will go on to discuss composite hypothesis testing and the Bonferroni correction. Concentration inequalities will be introduced and used to assess the statistical reliability of empirical estimates. We move to consider uniform convergence in order to analyse pattern stability. Rademacher complexity will be discussed as a theoretical tool for the bounding of uniform convergence.
Top/Computer_Science/Machine_Learning/Pattern_Recognition	Suffix tree and Hidden Markov techniques for pattern analysis Suffix tree construction. Mention the new linear time array constructions - - using suffix trees for finding motifs with gaps (some new observations: 0.5 - 1 hours). - finding cis-regulatory motifs by comparative genomics (1 hour) - Hidden Markov techniques for haplotyping
Top/Computer_Science/Machine_Learning/Pattern_Recognition	The Analysis of Patterns
Top/Computer_Science/Machine_Learning/Pattern_Recognition	Trajectory Pattern Mining The increasing pervasiveness of location-acquisition technologies (GPS, GSM networks, etc.) is leading to the collection of large spatio-temporal datasets and to the opportunity of discovering usable knowledge about movement behaviour, which fosters novel applications and services. In this paper, we move towards this direction and develop an extension of the sequential pattern mining paradigm that analyzes the trajectories of moving objects. We introduce trajectory patterns as concise descriptions of frequent behaviours, in terms of both space (i.e., the regions of space visited during movements) and time (i.e., the duration of movements). In this setting, we provide a general formal statement of the novel mining problem and then study several different instantiations of different complexity. The various approaches are then empirically evaluated over real data and synthetic benchmarks, comparing their strengths and weaknesses.
Top/Computer_Science/Machine_Learning/Pattern_Recognition	Trees, Arrays, Networks and Optimization for Finding Patterns in Biological Sequences a) The use of suffix trees and integer programming for finding optimal virus signatures. b) A current treatment of suffix-arrays and their uses. In the last several years simple linear-time algorithms for building suffix arrays have been developed making explicit suffix-trees mostly obsolete. c) Algorithms for finding signatures (patterns) of historical recombination and gene-conversion in SNP (binary) sequences. The techniques here relate to graph-theory.
Top/Computer_Science/Machine_Learning/Pattern_Recognition	Characterizing Implications of Injective Partial Orders
Top/Computer_Science/Machine_Learning/Pattern_Recognition	A general purpose segmentation algorithm using analytically evaluated random walks An ideal segmentation algorithm could be applied equally to the problem of isolating organs in a medical volume or to editing a digital photograph without modifying the algorithm, changing parameters, or sacrificing segmentation quality. However, a general-purpose, multiway segmentation of objects in an image/volume remains a challenging problem. In this talk, I will describe a recently developed approach to this problem that inputs a few training points from a user (e.g., from mouse clicks) and produces a segmentation by computing the probabilities that a random walker leaving unlabeled pixels/voxels will first strike the training set. By exact mathematical equivalence with a problem from potential theory, these probabilities may be computed analytically and deterministically. The algorithm is developed on an arbitrary, weighted, graph/mesh in order to maximize the broadness of application. I will illustrate the use of this approach with examples from several segmentation problems (without modifying the algorithm or the single free parameter), compare this algorithm to other approaches and discuss the theoretical properties that describe its behavior.
Top/Computer_Science/Machine_Learning/Pattern_Recognition	Learning with spectral representations and use of MDL principles
Top/Computer_Science/Machine_Learning/Pattern_Recognition	A framework for pattern analysis
Top/Computer_Science/Data_Mining	Data Mining and Decision Support Integration The aim of this presentation is twofold: (1) to introduce the field of Decision Support (DS), and (2) to provide an overview of possible approaches and benefits of combining DS with Data Mining (DM) in solving real-life decision and data-analysis problems. Related to DS, we define the concepts of decision problem and decision-making, introduce the taxonomy of disciplines related to DS, overview the approach of decision analysis, introduce the method of multi-attribute modeling, and illustrate it through real-life examples of housing loan allocation and risk assessment in medicine. In the main part, we investigate the ways to combine and integrate DS and DM, which generally involve the following categories: (1) DS for DM, (2) DM for DS, (3) DM, then DS, (4) DS, then DM, and (5) DM and DS. Each category is illustrated by a practical example. Two categories are investigated in greater detail. The category (1) DS for DM is represented by a method for selecting a best DM-induced classifier based on ROC space exploration. For the category (5) DM and DS, we explore an approach of developing qualitative multi-attribute models by combining the systems DEX and HINT. DEX is a DS tool for expert-based (hand-crafted) development of models, whereas HINT is a DM tool that develops models from data by a method based on function decomposition.
Top/Computer_Science/Data_Mining	Data Mining and Knowledge Discovery
Top/Computer_Science/Data_Mining	Data Mining as a Tool for Analysing Environmental Systems
Top/Computer_Science/Data_Mining	Data Mining Vs. Semantic Web This tutorial covers the field of datamining in general, talks about its possible applications (special case studies can be added on request), and elaborates on the issue of hardware accelerators for datamining. The introduction gives a formal and an informal definition (through an example), plus it points to possible missunderstandings typical of the topic. The part on methods and algorithms covers a number of different approaches, each one presented thru animation, using the examples that are both colourfull and unusual, but excellent for pointing into the essence. The part on tools lists about a dozen different tools, and selects one for a detailed case study. The part on applications includes examples from a variety of different fields (engineering, science, medicine, psychiatry, etc...) The part on hardware accelerators is available on special request. This tutorial was presented so far many times for industry and academia in the USA and Europe, and received the best tutorial award at several conferences.
Top/Computer_Science/Data_Mining	Distributed Data Mining Data mining is the automated analysis of large volumes of data looking for relationships and knowledge that are implicit in data. Data mining and knowledge discovery in large amounts of data can benefit from the use of parallel and distributed computational environments to improve both performance and quality of data selection. The goal of this tutorial is to provide researchers and practitioners with an introduction to mining large data sets by exploiting techniques from high performance parallel and distributed computing. This tutorial is organized in two parts. In the first part an introduction to high performance parallel and distributed computing is provided. Different forms of parallelism that can be exploited in data mining techniques and algorithms are analyzed. The second part presents a review of distributed data mining approaches. For each data mining technique, different ways for parallel implementation are presented and discussed. Furthermore, parallel and distributed data mining systems and algorithms are discussed. Finally, current research issues and perspectives in high-performance data mining are outlined.
Top/Computer_Science/Data_Mining	Next Generation Data Mining Tools: Power laws and self-similarity for graphs, streams and traditional data
Top/Computer_Science/Data_Mining	Parallel session 4 - Hands-on section Data mining with R
Top/Computer_Science/Data_Mining	Relational Data Mining and ILP
Top/Computer_Science/Data_Mining	Spatial Data Mining Querie language in a GIS System The strength of GIS is in providing a rich data infrastructure for combining disparate data in meaningful ways by using a spatial arrangement (e.g., proximity). As a toolbox, a GIS allows planners to perform spatial analysis using geo-processing functions such as map overlay, connectivity measurements or thematic map coloring. Although, this makes effective the geographic visualization of individual variables, complex multi-variate dependencies are easily overlooked. The required step to take GIS beyond a tool for automating cartography is to incorporate the ability of analyzing and condensing a large number of geo-referenced variables into a single forecast or score. This is where data mining promises great potential benefits and the reason why there is such a hand-in-glove fit between GIS and data mining. INGENS (INductive GEographic iNformation System) is a prototype GIS which integrates data mining tools to assist users in their task of topographic map interpretation. The spatial data mining process is aimed at a user who controls the parameters of the process by means of a query written in a mining query language. In this talk, I present SDMQL (Spatial Data Mining Query Language), a spatial data mining query language used in INGENS. Currently, SDMQL supports two data mining tasks: inducing classification rules and discovering association rules. For both tasks the language permits the specification of the task-relevant data, the kind of knowledge to be mined, the background knowledge and the hierarchies, the interestingness measures and the visualization for discovered patterns. Some constraints on the query language are identified by the particular mining task. I describe the syntax of the query language and finally I briefly illustrate the application to a real repository of maps.
Top/Computer_Science/Data_Mining	Teaching Data Mining: a Specific Experience
Top/Computer_Science/Data_Mining	The Sol-Eu-Net Project: Data mining Lessons Learned
Top/Computer_Science/Data_Mining	A Data Miners Story Getting to Know the Grand Challenges
Top/Computer_Science/Data_Mining	Privacy Preserving DataMining The rapid growth of the Internet over the last decade has been startling. However, efforts to track its growth have often fallen afoul of bad data --- for instance, how much traffic does the Internet now carry? The problem is not that the data is technically hard to obtain, or that it does not exist, but rather that the data is not shared. Obtaining an overall picture requires data from multiple sources, few of whom are open to sharing such data, either because it violates privacy legislation, or exposes business secrets. The approaches used so far in the Internet, e.g., trusted third parties, or data anonymization, have been only partially successful, and are not widely adopted. The paper presents a method for performing computations on shared data without any participants revealing their secret data. For example, one can compute the sum of traffic over a set of service providers without any service provider learning the traffic of another. The method is simple, scalable, and flexible enough to perform a wide range of valuable operations on Internet data.
Top/Computer_Science/Data_Mining	Statistical Aspects of Data Mining (Stats 202) This is the Google campus version of Stats 202 which is being taught at Stanford thissummer. I will follow the material from the Stanford class very closely. That material can be found at [[http://www.stats202.com/|www.stats202.com]]. The main topics are exploring and visualizing **data**, association analysis, classification, and clustering. The textbook is Introduction to **Data** **Mining** by Tan, Steinbach and Kumar. Googlers are welcome to attend any classes which they think might be of interest to them.
Top/Computer_Science/Data_Mining	Extracting Relevant Named Entities for Automated Expense Reimbursement Expense reimbursement is a time-consuming and labor-intensive process across organizations. In this talk, we present an automated expense reimbursement system developed at IBM Almaden Research Center. Our complete solution involves (1) an electronic document management infrastructure that provides multi-channel image capture, transport and storage of paper documents, such as receipts; (2) an unconstrained data mining approach to extracting relevant named entities from un-structured document images; (3) automation of manual auditing procedures using extracted metadata. The main focus of this presentation is our approach to automatically extracting important metadata, once we aggregate documents through such a scalable infrastructure. Extracting relevant named entities robustly from document images with unconstrained layouts and diverse formatting is a fundamental technical challenge to image-based data mining, question answering, and other information retrieval tasks. In many applications that require such capability, applying traditional language modeling techniques to the stream of OCR text does not give satisfactory result due to the absence of linguistic contexts, such as language constructs and punctuation. We present a novel approach for extracting relevant named entities from document images by learning the statistical dependencies between page layout and language features collectively from the sequence of geometrically decomposed regions on a document using a discriminative conditional random fields (CRFs) framework. We integrate this named entity extraction engine into our expense reimbursement solution and evaluate the system performance on large collections of real world receipt images provided by IBM World Wide Reimbursement Center.
Top/Computer_Science/Data_Mining	ACM SIGKDD Data Mining Practice Prize Winners
Top/Computer_Science/Data_Mining	From Trees to Forests and Rule Sets - A Unified Overview of Ensemble Methods Ensemble methods are one of the most influential developments in Machine Learning over the past decade. They perform extremely well in a variety of problem domains, have desirable statistical properties, and scale well computationally. By combining competing models into a committee, they can strengthen weak learning procedures. ;This tutorial explains two recent developments with ensemble methods: :**Importance Sampling** reveals classic ensemble methods (bagging, random forests, and boosting) to be special cases of a single algorithm. This unified view clarifies the properties of these methods and suggests ways to improve their accuracy and speed. :**Rule Ensembles** are linear rule models derived from decision tree ensembles. While maintaining (and often improving) the accuracy of the tree ensemble, the rule-based model is much more interpretable. This tutorial is aimed at both novice and advanced data mining researchers and practitioners especially in Engineering, Statistics, and Computer Science. Users with little exposure to ensemble methods will gain a clear overview of each method. Advanced practitioners already employing ensembles will gain insight into this breakthrough way to create next-generation models. ;**John Elder's lecture**: : In a Nutshell, Examples & Timeline : Predictive Learning : Decision Trees ;**Giovanni Seni's lecture**: : Model Selection (Bias-Variance Tradeoff , Regularization via shrinkage) : Ensemble Learning & Importance Sampling (ISLE) : Generic Ensemble Generation : Bagging, Random Forest, AdaBoost, MART : Rule Ensembles : Interpretation
Top/Computer_Science/Data_Mining	Successes, Failures and Learning From Them Over the last eighteen years, while there have clearly been successful deployment of knowledge discovery and data mining solutions, there have also undoubtedly been mistakes and failures. This aspect of the panel discussion will examine the low-lights (important mistakes and failures)&#160;with the end goal of trying to learn from them.
Top/Computer_Science/Data_Mining	Cost-effective Outbreak Detection in Networks Given a water distribution network, where should we place sensors to quickly detect contaminants? Or, which blogs should we read to avoid missing important stories? These seemingly different problems share common structure: Outbreak detection can be modeled as selecting nodes (sensor locations, blogs) in a network, in order to detect the spreading of a virus or information as quickly as possible. We present a general methodology for near optimal sensor placement in these and related problems. We demonstrate that many realistic outbreak detection objectives (e.g., detection likelihood, population affected) exhibit the property of submodularity. We exploit submodularity to develop an efficient algorithm that scales to large problems, achieving near optimal placements, while being 700 times faster than a simple greedy algorithm. We also derive online bounds on the quality of the placements obtained by any algorithm. Our algorithms and bounds also handle cases where nodes (sensor locations, blogs) have different costs. We evaluate our approach on several large real-world problems, including a model of a water distribution network from the EPA, and real blog data. The obtained sensor placements are provably near optimal, providing a constant fraction of the optimal solution. We show that the approach scales, achieving speedups and savings in storage of several orders of magnitude. We also show how the approach leads to deeper insights in both applications, answering multicriteria trade-off, cost-sensitivity and generalization questions.
Top/Computer_Science/Data_Mining	Successes, Failures and Learning From Them Over the last eighteen years, the field of knowledge discovery and data mining has matured considerably. Although the field has evolved as a result of synergistic co-operation among researchers in databases, artificial intelligence, statistics and systems, it has maintained its own identity. From a single workshop in 1989, the field can now lay claim to at least 5 major conferences and numerous symposium devoted to its central theme.
Top/Computer_Science/Data_Mining	Correlation Search in Graph Databases Correlation mining has gained great success in many application domains for its ability to capture the underlying dependency between objects. However, the research of correlation mining from graph databases is still lacking despite the fact that graph data, especially in various scientific domains, proliferate in recent years. In this paper, we propose a new problem of correlation mining from graph databases, called Correlated Graph Search (CGS). CGS adopts Pearsons correlation coefficient as a correlation measure to take into consideration the occurrence distributions of graphs. However, the problem poses significant challenges, since every subgraph of a graph in the database is a candidate but the number of subgraphs is exponential. We derive two necessary conditions which set bounds on the occurrence probability of a candidate in the database. With this result, we design an efficient algorithm that operates on a much smaller projected database and thus we are able to obtain a significantly smaller set of candidates. To further improve the efficiency, we develop three heuristic rules and apply them on the candidate set to further reduce the search space. Our extensive experiments demonstrate the effectiveness of our method on candidate reduction. The results also justify the efficiency of our algorithm in mining correlations from large real and synthetic datasets.
Top/Computer_Science/Data_Mining	Successes, Failures and Learning From Them
Top/Computer_Science/Data_Mining	Other ML/DM software (R, Weka, Yale)
Top/Computer_Science/Data_Mining	Introduction to the Panel Since the 1989 workshop on knowledge discovery in databases, the field has seen sustained growth and interest and has attained significant maturity. The main objectives of this panel will be to reflect on the successes and failures in the field of data mining over the last eighteen years and to examine what insights we can take with us as we move forward.
Top/Computer_Science/Data_Mining	Successes, Failures and Learning From Them At an abstract level, the theme of the field is concerned with extracting actionable and interpretable knowledge from data in as efficient a manner as possible. The primary purpose of this panel, in the context of this underlying theme, is to consider the following questions. What have been the major successes and breakthroughs that we as a field can point to with pride? What have been the critical mistakes or mis-steps that have been taken along the way? And finally, what can we hope to learn from both our successes and mistakes and how can this knowledge be used to determine how to focus our efforts in the future?
Top/Computer_Science/Data_Mining	E-Science and Computational Scientific Discovery
Top/Computer_Science/Data_Mining	Event Summarization for System Management In system management applications, an overwhelming amount of data are generated and collected in the form of temporal events. While mining temporal event data to discover interesting and frequent patterns has obtained rapidly increasing research efforts, users of the applications are overwhelmed by the mining results. The extracted patterns are generally of large volume and hard to interpret, they may be of no emphasis, intricate and meaningless to non-experts, even to domain experts. While traditional research efforts focus on finding interesting patterns, in this paper, we take a novel approach called event summarization towards the understanding of the seemingly chaotic temporal data. Event summarization aims at providing a concise interpretation of the seemingly chaotic data, so that domain experts may take actions upon the summarized models. Event summarization decomposes the temporal information into many independent subsets and finds well fitted models to describe each subset.
Top/Computer_Science/Data_Mining	Successes, Failures and Learning From Them Another topic of interest here is to highlight some of the classic mistakes made in the field. Topics of interest here could range from the use of non-representative training data to the ignorance of population drift when modeling time-varying data, from not accounting for errors in data or labels in the model to an over reliance on a single technique for the task on hand and from asking the wrong question in the context of the application driver to sampling without care. A related topic here might be to think about the role of benchmark datasets and algorithms, and reflect on the general importance and requirement for repeatable and reproducible results.
Top/Computer_Science/Data_Mining	Taking causality seriously: Propensity score methodology applied to estimate the effects of marketing interventions - Best PKDD paper
Top/Computer_Science/Data_Mining	A general purpose segmentation algorithm using analytically evaluated random walks An ideal segmentation algorithm could be applied equally to the problem of isolating organs in a medical volume or to editing a digital photograph without modifying the algorithm, changing parameters, or sacrificing segmentation quality. However, a general-purpose, multiway segmentation of objects in an image/volume remains a challenging problem. In this talk, I will describe a recently developed approach to this problem that inputs a few training points from a user (e.g., from mouse clicks) and produces a segmentation by computing the probabilities that a random walker leaving unlabeled pixels/voxels will first strike the training set. By exact mathematical equivalence with a problem from potential theory, these probabilities may be computed analytically and deterministically. The algorithm is developed on an arbitrary, weighted, graph/mesh in order to maximize the broadness of application. I will illustrate the use of this approach with examples from several segmentation problems (without modifying the algorithm or the single free parameter), compare this algorithm to other approaches and discuss the theoretical properties that describe its behavior.
Top/Computer_Science/Data_Mining	Matching of Tree Structures for Registration of Medical Images Many medical applications require a registration of different images of the same organ. In many cases, such a registration is accomplished by manually placing landmarks in the images. In this paper we propose a method which is able to find reasonable landmarks automatically. To achieve this, nodes of the vessel systems, which have been extracted from the images by a segmentation algorithm, will be assigned by the so-called association graph method and the coordinates of these matched nodes can be used as landmarks for a non-rigid registration algorithm.
Top/Computer_Science/Data_Mining	Detecting Changes in Large Data Sets of Payments Cards Data: A Case Study An important problem in data mining is detecting changes in large data sets. Although there are a variety of change detection algorithms that have been developed, in practice it can be a problem to scale these algorithms to large data sets due to the heterogeneity of the data. In this paper, we describe a case study involving payment card data in which we built and monitored a separate change detection model for each cell in a multi-dimensional data cube. We describe a system that has been in operation for the past two years that builds and monitors over 15,000 separate baseline models and the process that is used for generating and investigating alerts using these baselines.
Top/Computer_Science/Data_Mining	IMDS: Intelligent Malware Detection System The proliferation of malware has presented a serious threat to the security of computer systems. Traditional signature-based antivirus systems fail to detect polymorphic and new, previously unseen malicious executables. In this paper, resting on the analysis of Windows API execution sequences called by PE files, we develop the Intelligent Malware Detection System (IMDS) using Objective Oriented Association (OOA) mining based classification. IMDS is an integrated system consisting of three major modules: PE parser, OOA rule generator, and rule based classifier. An OOA Fast FPGrowth algorithm is adapted to efficiently generate OOA rules for classification. A comprehensive experimental study on a large collection of PE files obtained from the anti-virus laboratory of King Soft Corporation is performed to compare various malware detection approaches. Promising experimental results demonstrate that the accuracy and efficiency of our IMDS system outperform popular anti-virus software such as Norton AntiVirus and McAfee VirusScan, as well as previous data mining based detection systems which employed Naive Bayes, Support Vector Machine (SVM) and Decision Tree techniques.
Top/Computer_Science/Data_Mining	iAQ: A Program that Discovers Rules This video presents an entertaining program that discovers rules from data and outputs them in the form of English text and speech
Top/Computer_Science/Data_Mining	Google News Personalization: Scalable Online Collaborative Filtering
Top/Computer_Science/Data_Mining	Decision Support for Everyone: Olap in Ms Excel
Top/Computer_Science/Data_Mining	Spooky Stuff in Metric Space Decision trees are intelligible, but do they perform well enough that you should use them? Have SVMs replaced neural nets, or are neural nets still best for regression, and SVMs best for classification? Boosting maximizes margins similar to SVMs, but can boosting compete with SVMs? And if it does compete, is it better to boost weak models, as theory might suggest, or to boost stronger models? Bagging is simpler than boosting -- how well does bagging stack up against boosting? Breiman said Random Forests are better than bagging and as good as boosting. Was he right? And what about old friends like logistic regression, KNN, and naive bayes? Should they be relegated to the history books, or do they still fill important niches? In this talk we compare the performance of ten supervised learning methods on nine criteria: Accuracy, F-score, Lift, Precision/Recall Break-Even Point, Area under the ROC, Average Precision, Squared Error, Cross-Entropy, and Probability Calibration. The results show that no one learning method does it all, but some methods can be 'repaired' so that they do very well across all performance metrics. In particular, we show how to obtain the best probabilities from max margin methods such as SVMs and boosting via Platt's Method and isotonic regression. We then describe a new ensemble method that combines select models from these ten learning methods to yield much better performance. Although these ensembles perform extremely well, they are too complex for many applications. We'll describe what we're doing to try to fix that. Finally, if time permits, we'll discuss how the nine performance metrics relate to each other, and which of them you probably should (or shouldn't) use. During this talk I'll briefly describe the learning methods and performance metrics to help make the lecture accessible to non-specialists in machine learning.
Top/Computer_Science/Data_Mining	Statistical Change Detection for Multi-Dimensional Data This paper deals with detecting change of distribution in multi-dimensional data sets. For a given baseline data set and a set of newly observed data points, we define a statistical test called the density test for deciding if the observed data points are sampled from the underlying distribution that produced the baseline data set. We define a test statistic that is strictly distribution-free under the null hypothesis. Our experimental results show that the density test has substantially more power than the two existing methods for multi-dimensional change detection.
Top/Computer_Science/Data_Mining	Statistical techniques for fraud detection, prevention, and evaluation The talk begins by setting the context: fraud is defined and its breadth outlined; figures are given showing how significant fraud is; and different areas of fraud are examined, including health care fraud, banking fraud, and scientific fraud. The particular data analytic challenges of banking fraud are described and illustrated in detail. These include the fact that the classes are highly unbalanced (with typically no more than 1 in a 1000 transactions being fraudulent), that class labels may often be incorrect, that there will typically be delays in discovering the true labels, that the transaction arrival times are random, that the data are dynamic, and, perhaps most challenging of all, that the distributions are reactive, changing in response to the implementation of fraud detection systems. The role of mechanistic and empirical models in tackling these problems is described. Both have been widely used, and both have a contribution to make. Banking data, and in particular banking fraud data are examined in detail. Raw credit card transaction data have 70-80 variables per transaction, and this can be multiplied many-fold for behavioural data, as in fraud detection problems. Questions arise as to how to aggregate the data: should one try to classify individual transactions or should activity records be constructed? A fundamental aspect of any predictive problem in data analysis is the choice of an appropriate criterion for estimation and performance assessment. In the case of fraud, one needs, in particular, to combine both classification accuracy and timeliness of classification. This means that standard measures of classification performance, such as error rate, AUC, KS statistic, information value, etc, are not sufficient. Suitable measures and performance curves are described which combine these aspects and which are now being adopted by the industry. Various statistical (used here in John Chamberss sense of greater statistics) approaches have been developed for fraud detection problems, and some are described and illustrated, using data from some of the banks which have been collaborating with us. In particular, we look at supervised classification and anomaly detection methods. Finally in the context of banking fraud, some of the deeper but very important conceptual issues are outlined, including the economic imperative, whether fraud is now becoming acceptable, and what exactly we learn from empirical comparisons, Scientific fraud is contrasted with banking fraud. They have rather different drivers. In particular, financial gain is generally irrelevant to scientific fraud, which makes it an unusual kind of fraud - although, of course, the impact can be even more serious. Several examples are given, from a range of disciplines. The role of data analytic tools in detecting scientific fraud, and the nature of such tools, is described
Top/Computer_Science/Data_Mining	Relational Data Pre-Processing Techniques for Improved Securities Fraud Detection Commercial datasets are often large, relational, and dynamic. They contain many records of people, places, things, events and their interactions over time. Such datasets are rarely structured appropriately for knowledge discovery, and they often contain variables whose meanings change across different subsets of the data. We describe how these challenges were addressed in a collaborative analysis project undertaken by the University of Massachusetts Amherst and the National Association of Securities Dealers (NASD). We describe several methods for data preprocessing that we applied to transform a large, dynamic, and relational dataset describing nearly the entirety of the U.S. securities industry, and we show how these methods made the dataset suitable for learning statistical relational models. To better utilize social structure, we first applied known consolidation and link formation techniques to associate individuals with branch office locations. In addition, we developed an innovative technique to infer professional associations by exploiting dynamic employment histories. Finally, we applied normalization techniques to create a suitable class label that adjusts for spatial, temporal, and other heterogeneity within the data. We show how these pre-processing techniques combine to provide the necessary foundation for learning high-performing statistical models of fraudulent activity.
Top/Computer_Science/Data_Mining	Mining Queries
Top/Computer_Science/Data_Mining	Discovering interesting rules from financial data
Top/Computer_Science/Data_Mining	Robust Visual Mining of Data with Error Information
Top/Computer_Science/Data_Mining	Data variability could be your friend Deterministic modeling, in the form of ordinary differential equations (ODE), is the dominant paradigm in systems biology. This stems partially from the type of data that is available. Input data (e.g. gene expression data, protein concentrations) for these models is normally derived from whole cell populations. Consequently, what is modeled is the behaviour of one average cell rather than a multitude of individual cells. Variability within the data originates mainly from the measurement apparatus (technical error) or from difficult-to-control environmental conditions that precede the measurement (biological variability) and can constitute an impediment to clear cut conclusions. For example, kinetic parameters cannot be known with absolute precision and have to be accompanied with confidence intervals that are generally commensurate with the rather high variability attached to biological data. Data variability can also put obstacles in the way of decisive model selection. Measurement techniques are, however, increasingly being applied to individual cells. It is possible to average the individual cell observations, estimate the dispersion of this synthetic measurement, and use these data along with the modeling paradigms outlined above. However, inter-cell variability can be the result of intrinsic system noise. In particular, this is the case if molecular species involved exist in very low concentrations, such as in signaling networks. We argue that because this variability is in part intrinsic, it can be harnessed rather than tolerated, so that it provides novel insights into the mechanisms governing the system under study. This requires a paradigm shift from deterministic to stochastic modeling- even though ODEs are still central in the latter. To illustrate this, the example system we use is DNA Double Strand Break repair dynamics in irradiated human cells. Recent assaying techniques allow the quantification of DNA Double Strand Break (DSB) at the individual cell level. Repeated measurements in time form a dynamic image of the DSB decay process of cells after they have been exposed to a pulse of ionising irradiation. Crucially, individual cell measurements allow the monitoring of distributional features of the DSB count in a population. Existing deterministic models correctly mimic global features in this system. In particular, they can fit very well different decay regimes that are being observed when one focuses on the average DSB count in the population. We show however that these models, when translated into the stochastic realm, provide a poor data fit when one considers distributional features, such as the variance of the DSB count. Furthermore, using simple stochastic models that are partly amenable to analytical manipulation, we show that enriching the existing models with extra feedback loops produces an outcome more in tune with observations. Three independent data sets are used. Possible biological consequences are briefly discussed.
Top/Computer_Science/Data_Mining	State of the Art in Data Stream Mining
Top/Computer_Science/Data_Mining	Frequent graph mining - what is the question? The objective of data mining is to find regularities, or interesting patterns in large data sets, such as business transactions. More recently, there has been great interest in extending this work to structured data, such as graphs. The domain could be a database of molecular graphs, or the web graph, and the question could be to find subgraphs which occur frequently in the data. Algorithms usually list frequent subgraphs or other patterns. There are many different formulations of this problem. At this stage of the development of the field, it appears to be of some interest to put together a general picture of the different variants. In this talk we present an attempt towards this direction.
Top/Computer_Science/Data_Mining	State of the Art in Data Stream Mining
Top/Computer_Science/Data_Mining	Lost in Translation from Genes to Organisms Data mining genomes and developmental genetics provides information for building mathematical gene-network models on development. These models can be used to provide insilico predictions about processes such as developmental expression and regulation of genes.
Top/Computer_Science/Data_Mining	Planning to Learn with a Knowledge Discovery Ontology Assembly of optimized knowledge discovery workfows requires awareness of and extensive knowledge about the principles and mutual relations between diverse data processing and mining algorithms.We aim at alleviating this burden by automatically proposing workfows for the given type of inputs and required outputs of the discovery process. The methodology adopted in this study is to define a formal conceptualization of knowledge types and data mining algorithms and design a planning algorithm, which extracts constraints from this conceptualization for the given user's input-output requirements. We demonstrate our approach in two use cases, one from scientific discovery in genomics and another from advanced engineering.
Top/Computer_Science/Data_Mining	Data Mining for Anomaly Detection Anomaly detection corresponds to discovery of events that typically do not conform to expected normal behavior. Such events are often referred to as anomalies, outliers, exceptions, deviations, aberrations, surprise, peculiarities or contaminants in different application domains Detection of anomalies is a common problem in many domains, such as detecting fraudulent credit card transactions, insurance and tax fraud detection, intrusion detection for cyber security, failure detection, direct marketing, and medical diagnostics. Although anomalies are by definition infrequent, in many examples their importance is quite high compared to other events, making their detection extremely important. This tutorial will provide an overview of the research done in the increasingly important field of anomaly detection. The tutorial will cover the existing literature from a variety of perspectives, such as nature of input/output, and the availability of supervision. Anomalies will be divided into three broad groups: (i) Point anomalies, (ii) Contextual anomalies, and (iii) Structural anomalies, and a wide variety of anomaly detection methods appropriate for each type of anomaly will be presented. Additionally, the tutorial will discuss several application domains, such as intrusion detection, fraud detection, industrial damage detection, healthcare informatics, where anomaly detection plays a central role.
Top/Computer_Science/Data_Mining	Mining Massive Data Sets Today, the amount of data coming from all possible sources is enormous and growing at a fast pace due, in large part, to the ubiquitous Web and its increasing presence in our everyday life; but also to emails, cell phones, credit cards, retail, finance ... These data serve all sorts of functions : from query and search, to extracting information, providing services as well as managing security. Many fields are involved : statistics, data mining, text mining, data streams, search, social networks ... There is no lack of sophisticated techniques produced by academic activity, where challenges mostly deal with novelty, accuracy, and scalability of algorithms. However, in real-world applications, challenges are quite different : scalability (usually one or two orders of magnitude more than in academic publications), ease-of-use and capability to integrate efficient techniques into working systems in a transparent way, while always producing value for the customer. Real-world solutions are complex and usually need to integrate many technical components, from the various fields mentioned before: it thus becomes important to assess how these fields can complement one another. In the first part of the talk, I will present the challenges of real-world data mining applications. I will introduce the general Statistical Learning Theory framework and discuss some of the technical issues involved (large dimension data sets, missing data, outliers, non-i.i.d. structured data, unlabelled data ...) In the second part, I will show, taking examples from the implementation in KXEN and applications developed, how a theoretical framework (Structural Risk Minimization [1]) can be used to solve some of the challenges met in the real-world. I will finally describe some open practical issues which will require further theoretical investigation.
Top/Computer_Science/Data_Mining	Association Mining in Large Databases: A Re-examination of Its Measures
Top/Computer_Science/Data_Mining	VideoLectures.net case study The task in the VideoLectures case study is to develop a software component that will aid the VideoLectures editors in categorizing recorded lectures (i.e. ontology population). This functionality is required due to the rapid growth of the number of hosted lectures as well as due to the fact that the categorization taxonomy is rather fine-grained (200 categories and growing). In addition to aiding the categorization of new lectures, the software will also be used for re-categorization and additional categorization of lectures already categorized. We will show that we were successful in our task as the categorizer is highly accurate it achieves accuracies that stretch 1220% above the baseline and highly robust in terms of missing data. The latter means that a lecture might be missing textual annotations (such as the description and slide titles) but is still categorized correctly. Furthermore, the categorizer has been successfully integrated into the VideoLectures Web site. Categorization suggestions (termed 'quick links') are provided to the author in the categorization panel.
Top/Computer_Science/Text_Mining	Information Retrieval and Text Mining This four hour course will provide an overview of applications of machine learning and statistics to problems in information retrieval and text mining. More specifically, it will cover tasks like document categorization, concept-based information retrieval, question-answering, topic detection and document clustering, information extraction, and recommender systems. The emphasis is on showing how machine learning techniques can help to automatically organize content and to provide efficient access to information in textual form.
Top/Computer_Science/Text_Mining	Information Retrieval and Text Mining This four hour course will provide an overview of applications of machine learning and statistics to problems in information retrieval and text mining. More specifically, it will cover tasks like document categorization, concept-based information retrieval, question-answering, topic detection and document clustering, information extraction, and recommender systems. The emphasis is on showing how machine learning techniques can help to automatically organize content and to provide efficient access to information in textual form.
Top/Computer_Science/Text_Mining	Java library for support of text mining and retrieval
Top/Computer_Science/Text_Mining	Predictive methods for Text mining I will give a general overview of using prediction methods in text mining applications, including text categorization, information extraction, summarization, and question answering. I will then discuss some of the more advanced issues encountered in real applications such as structured and complicated output classification, the use of unlabeled data, modeling link structures, collective inference and community effect, and transfer learning under changing environment, etc.
Top/Computer_Science/Text_Mining	Text mining
Top/Computer_Science/Text_Mining	Text Mining for Ontology Learning
Top/Computer_Science/Text_Mining	Text Mining for Ontology Learning
Top/Computer_Science/Text_Mining	Text Mining and Link Analysis for Web and Semantic Web The tutorial on Text Mining and Link Analysis for Web Data will focus on two main analytical approaches when analyzing web data: text mining and link analysis for the purpose of analyzing web documents and their linkage. First, the tutorial will cover some basic steps and problems when dealing with the textual and network (graph) data showing what is possible to achieve without very sophisticated technology. The idea of this first part is to present the nature of un-structured and semi-structured data. Next, in the second part, more sophisticated methods for solving more difficult and challenging problems will be shown. In the last part, some of the current open research issues will be presented and some practical pointers on the available tolls for solving previously mentioned problems will be provided.
Top/Computer_Science/Text_Mining	Text Classification
Top/Computer_Science/Text_Mining	Text and web data mining: Tutorial II
Top/Computer_Science/Text_Mining	Text and web data mining: Tutorial I
Top/Computer_Science/Text_Mining	Active, Semi-Supervised Learning for Textual Information Access
Top/Computer_Science/Text_Mining	Generative Latent Space Models for Text and Image
Top/Computer_Science/Text_Mining	The WikiPedia
Top/Computer_Science/Text_Mining	Latent Semantic Variable Models In the context of information retrieval and natural language processing, latent variable models are quite useful in modeling and discovering hidden structure that often leads to 'semantic' data representations. This talk will provide an overview of the most popular approaches and discuss the range of possible applications for such models, including language modeling, ad hoc retrieval, text categorization and collaborative filtering.
Top/Computer_Science/Text_Mining	Telugu - English Dictionary Based Cross Language Query Focused Multi-Document Summarization
Top/Computer_Science/Text_Mining	Automatic Labeling of Multinomial Topic Models Multinomial distributions over words are frequently used to model topics in text collections. A common, major challenge in applying all such topic models to any text mining problem is to label a multinomial topic model accurately so that a user can interpret the discovered topic. So far, such labels have been generated manually in a subjective way. In this paper, we propose probabilistic approaches to automatically labeling multinomial topic models in an objective way. We cast this labeling problem as an optimization problem involving minimizing Kullback-Leibler divergence between word distributions and maximizing mutual information between a label and a topic model. Experiments with user study have been done on two text data sets with different genres. The results show that the proposed labeling methods are quite effective to generate labels that are meaningful and useful for interpreting the discovered topic models. Our methods are general and can be applied to labeling topics learned through all kinds of topic models such as PLSA, LDA, and their variations.
Top/Computer_Science/Text_Mining	Robust Textual Inference Using Diverse Knowledge Sources
Top/Computer_Science/Text_Mining	System for extracting data (facts) from large amount of unstructured documents
Top/Computer_Science/Text_Mining	Joint Mining of Biological Text and Images: Case Studies
Top/Computer_Science/Text_Mining	Automated Text Summarization using MEAD: Experience with the IMF Staff Reports
Top/Computer_Science/Text_Mining	Mixtures of Hierarchical Topics with Pachinko Allo cation The four-level pachinko al location model (PAM) (Li & McCallum, 2006) represents correlations among topics using a DAG structure. It does not, however, represent a nested hierarchy of topics, with some topical word distributions representing the vocabulary that is shared among several more specific topics. This paper presents hierarchical PAM -- an enhancement that explicitly represents a topic hierarchy. This model can be seen as combining the advantages of hLD's topical hierarchy representation with PAM's ability to mix multiple leaves of the topic hierarchy. Experimental results show improvements in likelihood of held-out documents, as well as mutual information between automatically-discovered topics and humangenerated categories such as journals.
Top/Computer_Science/Text_Mining	Learning Hierarchical Multi-Category Text Classification Models
Top/Computer_Science/Text_Mining	A Concept-based Model for Enhancing Text Categorization Most of text categorization techniques are based on word and/or phrase analysis of the text. Statistical analysis of a term frequency captures the importance of the term within a document only. However, two terms can have the same frequency in their documents, but one term contributes more to the meaning of its sentences than the other term. Thus, the underlying model should indicate terms that capture the semantics of text. In this case, the model can capture terms that present the concepts of the sentence, which leads to discover the topic of the document. A new concept-based model that analyzes terms on the sentence and document levels rather than the traditional analysis of document only is introduced. The concept-based model can effectively discriminate between non-important terms with respect to sentence semantics and terms which hold the concepts that represent the sentence meaning. The proposed model consists of concept-based statistical analyzer, conceptual ontological graph representation, and concept extractor. The term which contributes to the sentence semantics is assigned two different weights by the concept-based statistical analyzer and the conceptual ontological graph representation. These two weights are combined into a new weight. The concepts that have maximum combined weights are selected by the concept extractor. A set of experiments using the proposed concept-based model on different datasets in text categorization is conducted. The experiments demonstrate the comparison between traditional weighting and the concept-based weighting obtained by the combined approach of the concept-based statistical analyzer and the conceptual ontological graph. The evaluation of results is relied on two quality measures, the Macro-averaged F1 and the Error rate. These quality measures are improved when the newly developed concept-based model is used to enhance the quality of the text categorization
Top/Computer_Science/Text_Mining	The use of machine translation tools for cross-lingual text-mining
Top/Computer_Science/Text_Mining	Recognizing Textual Entailment with LCCs GROUNDHOG System We introduce a new system for recognizing textual entailment (known as GROUNDHOG) which utilizes a classification-based approach to combine lexico-semantic information derived from text processing applications with a large collection of paraphrases acquired automatically from the WWW. Trained on 200,000 examples of textual entailment extracted from newswire corpora, our system managed to classify more than 75% of the pairs in the 2006 PASCAL RTE Test Set correctly.
Top/Computer_Science/Text_Mining	Industry 3: How Co-Occurrence can Complement Semantics? Analysis of texts is an obvious way for semantic annotation and extraction of structured knowledge. A basic task is the recognition of references to entities (people, locations, organizations, etc). A next step is relation extraction, e.g. identifying that an organization is located in a particular city. Automatic extraction of such relations is a tough linguistic problem - the solutions are either very partial, expensive to implement, or slow. On the other hand, relationships are crucial for the usability of the extracted knowledge for navigation and search purposes. We demonstrate how efficient co-occurrence analysis, performed on top of semantic annotation, can be used for several purposes: relation extraction, faceted search, and popularity timelines. The faceted search interface allows an easy way for augmenting full-text search by means of entity references, derived through co-occurrence profiling and semantic relationships. Although this sort of analytics can be used in virtually any domain, their development within the KIM platform was driven by the requirements for news analysis and research. We demonstrate the usage of these interfaces on top of 1 million news articles - a corpus of the major international news for the last five years. This sort of co-occurrence analysis has the potential of aiding identity resolution, which is recognized to be a crucial problem for several tasks: cross-document co-reference resolution, record linkage, object linking, and data integration.
Top/Computer_Science/Text_Mining	Semi-supervised Learning for Text Classification
Top/Computer_Science/Text_Mining	Text Categorization This course will cover the principal topics important to creating a working text categorization system. It will focus on the components of such a system and processes required to create it based on the practical experiences of the Scamseek project. The role of machine learning will be the center of the discussion but the surrounding tasks of language modeling, computational linguistics and software engineering will all be discussed to varying degrees. Discussion of some aspects of the Scamseek project are restricted under secrecy agreements with ASIC.
Top/Computer_Science/Text_Mining	On TREC Blog Track
Top/Computer_Science/Text_Mining	Text Mining in Biological Texts
Top/Computer_Science/Text_Mining	Link Analysis and Text Mining : Current State of the Art and Applications for Counter Terrorism The information age has made it easy to store large amounts of data.The proliferation of documents available on the Web, on corporate intranets, on news wires, and elsewhere is overwhelming. However, while the amount of data available to us is constantly increasing, our ability to absorb and process this information remains constant. Search engines only exacerbate the problem by making more and more documents available in a matter of a few key strokes. Link Analysis is a new and exciting research area that tries to solve the information overload problem by using techniques from data mining, machine learning, Information Extraction, Text Categorization, Visualization and Knowledge Management.
Top/Computer_Science/Text_Mining	Text mining for semantically enabled social browsing
Top/Computer_Science/Text_Mining	Extracting Semantic Networks from Text via Relational Clustering Extracting knowledge from text has long been a goal of AI. Initial approaches were purely logical and brittle. More recently, the availability of large quantities of text on the Web has led to the development of machine learning approaches. However, to date these have mainly extracted ground facts, as opposed to general knowledge. Other learning approaches can extract logical forms, but require supervision and do not scale. In this paper we present an unsupervised approach to extracting semantic networks from large volumes of text. We use the TextRunner system [1] to extract tuples from text, and then induce general concepts and relations from them by jointly clustering the objects and relational strings in the tuples. Our approach is defined in Markov logic using four simple rules. Experiments on a dataset of two million tuples show that it outperforms three other relational clustering approaches, and extracts meaningful semantic networks.
Top/Computer_Science/Text_Mining	Topic Models Conditioned on Arbitrary Features with Dirichlet-multinomial Regression Although fully generative models have been successfully used to model the contents of text documents, they are often awkward to apply to combinations of text data and document metadata. In this paper we propose a Dirichlet-multinomial regression (DMR) topic model that includes a log-linear prior on document-topic distributions that is a function of observed features of the document, such as author, publication venue, references, and dates. We show that by selecting appropriate features, DMR topic models can meet or exceed the performance of several previously published topic models designed for specific data.
Top/Computer_Science/Text_Mining	Latent Variable Models for Document Analysis Wray Buntine will consider various problems in document analysis (named entity recognition, natural language parsing, information retrieval), and look at various probabilistic graphical models and algorithms for addressing the problem. This will not be an extensive coverage of information extraction or natural language processing, but rather a look at some of the theory, methods and practice of particular cases, including the use of software environments.
Top/Computer_Science/Text_Mining	Filtering Multi-Lingual Terrorist Content with Graph-Theoretic Classifi-cation Tools Since the web is increasingly used by terrorist organizations, the ability to automatically detect multi-lingual terrorist-related content is extremely important. In this talk, we present an efficient detection methodology based on the recently developed graph-based web document representation models. Evaluation is performed on corpora in English and Arabic languages.
Top/Computer_Science/Information_Retrieval	Applications of Influence Diagrams to Information Retrieval
Top/Computer_Science/Information_Retrieval	Comparison of information retrieval techniques: Latent semantic indexing (LSI) and Concept indexing (CI) Information retrieval in the vector space model is based on literal matching of terms in the documents and the queries. The model is implemented by creating the term-document matrix, which is formed on the base of frequencies of terms in documents. Literal matching of terms does not necessarily retrieve all relevant documents. Synonymy (multiple words having the same meaning) and polysemy (words having multiple meaning) are two major obstacles for efficient information retrieval. Latent semantic indexing (LSI) and concept indexing (CI) are information retrieval techniques embedded in the vector space model, which address the problem of synonymy and polysemy. The method of LSI is an information retrieval technique using a low-rank singular value decomposition (SVD) of the term-document matrix. Although the LSI method has empirical success, it suffers from the lack of interpretation for the low-rank approximation and, consequently, the lack of controls for accomplishing specific tasks in information retrieval. The method of CI uses centroids of clusters or so-called concept decomposition (CD) for lowering the rank of the term-document matrix. Here we compare SVD/LSI and CD/CI in terms of matrix approximations and precision of information retrieval.
Top/Computer_Science/Information_Retrieval	Dealing with Heterogeneity in Profiles for Personalized Information Retrieval
Top/Computer_Science/Information_Retrieval	Efficient Top-k Queries for XML Information Retrieval
Top/Computer_Science/Information_Retrieval	From query based Information Retrieval to context driven Information Supply
Top/Computer_Science/Information_Retrieval	Information Retrieval and Language Technology The course will give an overview of how statistical learning can help organize and access information that is represented in textual form. In particular, it will cover tasks like text classification, information retrieval, information extraction, topic detection, and topic tracking. The course will introduce the basic techniques for representing text and analyze their statistical properties. An emphasis of the course will be on giving an overview of interesting learning problems in this area, providing starting points for future research.
Top/Computer_Science/Information_Retrieval	Information Retrieval and Text Mining This four hour course will provide an overview of applications of machine learning and statistics to problems in information retrieval and text mining. More specifically, it will cover tasks like document categorization, concept-based information retrieval, question-answering, topic detection and document clustering, information extraction, and recommender systems. The emphasis is on showing how machine learning techniques can help to automatically organize content and to provide efficient access to information in textual form.
Top/Computer_Science/Information_Retrieval	Information Retrieval and Text Mining This four hour course will provide an overview of applications of machine learning and statistics to problems in information retrieval and text mining. More specifically, it will cover tasks like document categorization, concept-based information retrieval, question-answering, topic detection and document clustering, information extraction, and recommender systems. The emphasis is on showing how machine learning techniques can help to automatically organize content and to provide efficient access to information in textual form.
Top/Computer_Science/Information_Retrieval	Information Retrieval in GATE
Top/Computer_Science/Information_Retrieval	Language Models for Information Retrieval
Top/Computer_Science/Information_Retrieval	Learning Rankings for Information Retrieval
Top/Computer_Science/Information_Retrieval	Learn to Weight Term in Information Retrieval Using Category Information
Top/Computer_Science/Information_Retrieval	Mining XML documents - Bridging the gap between Machine Learning and Information Retrieval
Top/Computer_Science/Information_Retrieval	Proactive Information Retrieval by User Modeling from Eye Tracking
Top/Computer_Science/Information_Retrieval	User models from implicit feedback for proactive information retrieval Our research consortium develops user modeling methods for proactive applications. In this project we use machine learning methods for predicting users preferences from implicit relevance feedback. Our prototype application is information retrieval, where the feedback signal is measured from eye movements or users behavior. Relevance of a read text is extracted from the feedback signal with models learned from a collected data set. Since it is hard to define relevance in general, we have constructed an experimental setting where relevance is known a priori.
Top/Computer_Science/Information_Retrieval	Active, Semi-Supervised Learning for Textual Information Access
Top/Computer_Science/Information_Retrieval	Applications of Query Mining
Top/Computer_Science/Information_Retrieval	Telugu - English Dictionary Based Cross Language Query Focused Multi-Document Summarization
Top/Computer_Science/Information_Retrieval	Current Approaches to Personalized Web Search
Top/Computer_Science/Information_Retrieval	Compact indexing of versioned data
Top/Computer_Science/Information_Retrieval	(Semantic) Structure in Structured Document Retrieval
Top/Computer_Science/Information_Retrieval	Machine learning for access and retrieval I
Top/Computer_Science/Information_Retrieval	Large Scale Ranking Problem: some theoretical and algorithmic issues The talk is divided into two parts. The first part focuses on web-search ranking, for which I discuss training relevance models based on DCG (discounted cumulated gain) optimization. Under this metric, the system output quality is naturally determined by the performance near the top of its rank-list. I will mainly focus on various theoretical issues for this learning problem. The second part discusses related algorithmic issues in the context of optimizing the scoring function of a statistical machine translation system according to the BLEU metric (standard measure of translation quality). Our approach treats machine translation as a black-box, and can optimize millions of system parameters automatically. This has not been attempted before in this context. I will present our method and some initial results.
Top/Computer_Science/Information_Retrieval	Iterative Algorithms for Collaborative Filtering with Mixture Models
Top/Computer_Science/Information_Retrieval	Evaluation of MT and CLITIA
Top/Computer_Science/Information_Retrieval	Efficient Lazy Algorithms for Minimal-Interval Semantics
Top/Computer_Science/Information_Retrieval	Boosting Performance of Web Search Engines Using Query Logs
Top/Computer_Science/Information_Retrieval	Workshop: Personalized Question Answering: A Use Case for Business Analysis
Top/Computer_Science/Information_Retrieval	Personalized Web Search Engine for Mobile Devices
Top/Computer_Science/Information_Retrieval	Implicit feedback learning in semantic and collaborative information retrieval systems Information retrieval is a very wide domain which can involve various types of activities and tasks. Many complex factors are participating in a search for information and many systems have been experimented. Nowadays a general consensus has been established around a keyword/document matching process which appears to be efficient on large scale and have enough reliability to satisfy a significant part of the users. Btu this claim has to be limited and for some subjects, search is still a difficult task. Many reasons can be proposed to explain these phenomena, but the most salient ones are the difficulty for users to express their needs while searching for information and the limitation of shared knowledge between users and information retrieval systems, meaning that both users and machines don't really understand the information and knowledge space used as references by the other. This presentation try to provide an overview of one way to resolve those gaps: using feedback learning. The aim is to make the system learning on user behaviour in order to better define its current needs. Machine learning algorithms applied on signal coming from user while performing a search can lead to the understanding of what is really relevant to the users and then can be exploited to help him during its tasks. The work, engaged through the VITALAS1 project, is presented: study of users search logs and definition of a feedback learning framework. Then research on implicit relevance feedback and query optimisation is presented as a first attempt to exploit the feedback learning framework. Finally an overview of the next steps within those studies is presented and especially their impact on the VITALAS project.
Top/Computer_Science/Information_Retrieval	Content-based Document Routing and Index Partitioning for Scalable Similarity-based Searches in a Large Corpus We present a document routing and index partitioning scheme for scalable similarity-based search of documents in a large corpus. We consider the case when similarity-based search is performed by finding documents that have features in common with the query document. While it is possible to store all the features of all the documents in one index, this suffers from obvious scalability problems. Our approach is to partition the feature index into multiple smaller partitions that can be hosted on separate servers, enabling scalable and parallel search execution. When a document is ingested into the repository, a small number of partitions are chosen to store the features of the document. To perform similarity-based search, also, only a small number of partitions are queried. Our approach is stateless and incremental. The decision as to which partitions the features of the document should be routed to (for storing at ingestion time, and for similarity based search at query time) is solely based on the features of the document. Our approach scales very well. We show that executing similarity-based searches over such a partitioned search space has minimal impact on the precision and recall of search results, even though every search consults less than 3% of the total number of partitions.
Top/Computer_Science/Information_Retrieval	A Social Network Based Approach to Personalized Recommendation of Participatory Media Content
Top/Computer_Science/Information_Retrieval	Evaluation and benchmarking
Top/Computer_Science/Information_Retrieval	Methods for Fusing Eye Movements and Text Content for Information Retrieval
Top/Computer_Science/Cryptography_and_Security	Parametric Autentication. How I know who you are?
Top/Computer_Science/Cryptography_and_Security	IMDS: Intelligent Malware Detection System The proliferation of malware has presented a serious threat to the security of computer systems. Traditional signature-based antivirus systems fail to detect polymorphic and new, previously unseen malicious executables. In this paper, resting on the analysis of Windows API execution sequences called by PE files, we develop the Intelligent Malware Detection System (IMDS) using Objective Oriented Association (OOA) mining based classification. IMDS is an integrated system consisting of three major modules: PE parser, OOA rule generator, and rule based classifier. An OOA Fast FPGrowth algorithm is adapted to efficiently generate OOA rules for classification. A comprehensive experimental study on a large collection of PE files obtained from the anti-virus laboratory of King Soft Corporation is performed to compare various malware detection approaches. Promising experimental results demonstrate that the accuracy and efficiency of our IMDS system outperform popular anti-virus software such as Norton AntiVirus and McAfee VirusScan, as well as previous data mining based detection systems which employed Naive Bayes, Support Vector Machine (SVM) and Decision Tree techniques.
Top/Computer_Science/Cryptography_and_Security	Plastic Card Fraud Detection using Peer Group Analysis Fraud detection describesmethods that attempt to identify fraudulent activity as quickly as possible. From a statistical methods perspective there are broadly two approaches to fraud detection [1]. These relate to whether we intend to detect known examples of fraudulent activity or whether we intend to detect novel forms of fraudulent behaviour. In the former case pattern matching techniques are used in the latter case anomaly detection techniques are deployed. Peer group analysis is an unsupervised method for monitoring behaviour over time [2] and it can be used for anomaly detection [3]. In the context of plastic card fraud detection, peer groups are built for each account, where a peer group is collection of other accounts that behave similarly. The subsequent behaviour of each account is measured in relation to its peer group. Should an accounts behaviour deviate strongly from its peer group then the account is flagged as anomalous and its recent transactions are flagged as potential frauds. This approach differs from the usual anomaly detection methods where each accounts current behaviour is measured in relation to its own past behaviour. We show how to apply peer group analysis to times series that consist of timealigned multivariate continuous data. The initial analysis comprises of a method to determine the peer group members for each time series. For this we need to compare time series [4], we describe one method that is useful for plastic card transaction data. Once we have the peer groups, the analysis then comprises of a method for tracking a time series with respect to its peer group. An anomaly is said to have occurred should the separation between the time series and its peer group exceed some externally set threshold. Account histories of plastic card transaction data are neither time aligned nor do they consist of purely continuous data. A transaction can occur at any time and each transaction has associated with it a record containing a large amount of information. This enables the card issuer to distinguish between the large number of possible transaction types that can occur. For example an account holder who checks their balance at an ATM (an example of a transaction where no money is transferred) or an account holder who purchases a rental car but was not present at the point of sale. We describe one way to time align different account transaction histories and to transform some pertinent information into continuous variables. We summarise experiments performed using peer group analysis on real credit card transaction data. In particular we examine the effect that missed fraudulent transactions have on the performance of the peer groups. We describe a method for robustifying against fraudulent transactions contaminating peer groups.We present our results using a new measure of performance that has been designed specifically for plastic card fraud [5]. Not all accounts can be tracked well enough by their respective peer groups to usefully identify anomalous behaviour.We describe a measure of peer group quality which we use to identify accounts that are more likely to be successfully analysed using peer groups.
Top/Computer_Science/Cryptography_and_Security	Relational Data Pre-Processing Techniques for Improved Securities Fraud Detection Commercial datasets are often large, relational, and dynamic. They contain many records of people, places, things, events and their interactions over time. Such datasets are rarely structured appropriately for knowledge discovery, and they often contain variables whose meanings change across different subsets of the data. We describe how these challenges were addressed in a collaborative analysis project undertaken by the University of Massachusetts Amherst and the National Association of Securities Dealers (NASD). We describe several methods for data preprocessing that we applied to transform a large, dynamic, and relational dataset describing nearly the entirety of the U.S. securities industry, and we show how these methods made the dataset suitable for learning statistical relational models. To better utilize social structure, we first applied known consolidation and link formation techniques to associate individuals with branch office locations. In addition, we developed an innovative technique to infer professional associations by exploiting dynamic employment histories. Finally, we applied normalization techniques to create a suitable class label that adjusts for spatial, temporal, and other heterogeneity within the data. We show how these pre-processing techniques combine to provide the necessary foundation for learning high-performing statistical models of fraudulent activity.
Top/Computer_Science/Cryptography_and_Security	Privilege Management using X.509 Attribute Certificates (upravljanje z digitalnimi pooblastili)
Top/Computer_Science/Image_Analysis	Computer Vision
Top/Computer_Science/Image_Analysis	Image Analysis
Top/Computer_Science/Image_Analysis	Kernel Methods for Higher Order Image Statistics The conditions under which natural vision systems evolved show statistical regularities determined both by the environment and by the actions of the organism. Many aspects of biological vision can be understood as evolutionary adaptations to these regularities. This is demonstrated by the recent sucess in explaining properties of retinal and cortical neurons from the statistics of natural images. At the same time, we observe an increasing interest in statistical modeling techniques in the computer vision community. Here, the motivation comes from the need for powerful image models in image processing tasks such as super-resolution or denoising. In the literature, the statistical analysis of natural images has mainly been done with linear techniques such as Principal Component Analysis (PCA) or Fourier analysis. These techniques capture only the second-order statistics of an image ensemble. A large part of the interesting image structure, however, is contained in the higher-order statistics. Unfortunately, the estimation of these statistics involves a huge number of terms which makes their explicit computation for images infeasible in practice. Kernel methods provide an implicit access to higher-order statistics that avoids this combinatorial explosion. In the course, we start with an overview of existing approaches to image statistics. The need to go beyond the usual linear, second-order techniques will lead us to the classical higher-order statistics such as Wiener series, higher-order cumulants and spectra. We will see that the exponential number of terms involved in these statistics prevents them from being applied to images. This motivates the introduction of kernel techniques. Here, we will discuss two approaches: 1. The Wiener series can be estimated implicitly via polynomial kernel regression. We will use this technique to decompose an image into components that are characterized by pixel interactions of a given order. 2. Kernel PCA of image patches provides a powerful image model that takes higher-order statistics into account. We will show applications of this model to various image processing tasks.
Top/Computer_Science/Image_Analysis	Comparing Sets of 3D Digital Shapes through Topological Structures New technologies for shape acquisition and rendering of digital shapes have simplified the process of creating virtual scenes; nonetheless, shape annotation, recognition and manipulation of both the complete virtual scenes and even of subparts of them are still open problems. Once the main components of a virtual scene are represented by structural descriptions, this paper deals with the problem of comparing two (or more) sets of 3D objects, where each model is represented by an attributed graph. We will define a new distance to estimate the possible similarities among the sets of graphs and we will validate our work using a shape graph.
Top/Computer_Science/Image_Analysis	Image categorization
Top/Computer_Science/Image_Analysis	Learning to Reconstruct 3D Human Pose and Motion from Silhouettes We will describe our ongoing work on learning-based methods for recovering 3D human body pose and motion from single images and from monocular image sequences. The methods work directly with raw image observations and require neither an explicit 3D body model nor a prior labelling of body parts in the image. Instead, they recover the body pose or motion by direct nonlinear regression against shape descriptors extracted automatically from image silhouettes or contours.
Top/Computer_Science/Image_Analysis	Learning issues in image segmentation Image segmentation is often defined as a partitioning of pixels or image blocks into homogeneous groups. These groups are characterized by a prototypical vector in feature space, e.g., the space of Gabor filter responses, by a prototypical histograms of features or by pairwise dissimilarities between image blocks. For all three data formats cost functions have been proposed to measure distortion and, thereby, to encode the quality of a partition. Learning in image segmentation can be defined as the inference of prototypical descriptors of segments like codebook vectors or average feature probability within a segment. Contrary to classification or regression, the empirical risk of image segmentation is often composed of sums of dependent random variables like in Normalized Cut, Pairwise Clustering or k-means clustering with smoothness constraints. One of the core challenges for machine learning is to discover what kind of information can be learned from these data sources assuming MRF cost functions as image models. The validation procedure for image segmentations strongly depends on this issue. I will demonstrate the learning and validation issue in the context of image analysis based on color and texture features.
Top/Computer_Science/Image_Analysis	Color-based object recognition This demonstrates a robodog that recognize objects in a 'fetch' task. The software runs on a world-wide computing grid, distributing the computational load over several beowolf clusters.
Top/Computer_Science/Image_Analysis	Generative Models for Visual Objects and Object Recognition via Bayesian Inference
Top/Computer_Science/Image_Analysis	Generative Latent Space Models for Text and Image
Top/Computer_Science/Image_Analysis	Building local part models for category-level recognition This talk addresses the problem of building models for category-level recognition. The starting point is a set of local invariant features which have been shown to support the robust recognition of specific objects and scenes. In the category-level object recognition context, it is no longer sufficient to use individual features, and it becomes necessary to model intra-class variations, to select discriminant features, and to model spatial relations. Furthermore, it is important to use shape information, as in many cases objects of a given category are different in grey-level appearance, but similar in shape. This leads to a part-based approach to category-level recognition that I will illustrate with several examples, including feature selection, local affine-invariant part models, and contour-based shape description. This is joint work with Gyuri Dorko, Frederic Jurie, Svetlana Lazebnik and Jean Ponce.
Top/Computer_Science/Image_Analysis	Trainable visual models for object classification The general theme of the tutorial will be 'trainable visual models for object classification'. I will cover: the difficulty of the problem a few approaches Perona and Welling Pictorial structures of Felzenszwalb and Huttenlocher Borenstein and Ullman Agarwal and Roth Leibe and Schiele covering the method, invariance, data preparation and then go into detail on the constellation model end with research challenges
Top/Computer_Science/Image_Analysis	Qualitative Spatial Relationships for Image Interpretation by using Semantic Graph In this paper, a new way to express complex spatial relations is proposed in order to integrate them in a Constraint Satisfaction Problem with bilevel constraints. These constraints allow to build semantic graphs, which can describe more precisely the spatial relations between subparts of a composite object that we look for in an image. For example, it allows to express complex spatial relations such as is surrounded by. This approach can be applied to image interpretation and some examples on real images are presented.
Top/Computer_Science/Image_Analysis	Feature extraction & content description I
Top/Computer_Science/Image_Analysis	Content Analysis: Marriage of Signal Processing and Machine Learning
Top/Computer_Science/Image_Analysis	Graphs Regularization for Data Sets and Images: Filtering and Semi-Supervised Classification
Top/Computer_Science/Image_Analysis	Learning to Compress Images and Video We present an intuitive scheme for lossy color-image compression: Use the color information from a few representative pixels to learn a model which predicts color on the rest of the pixels. Now, storing the representative pixels and the image in grayscale suffice to recover the original image. A similar scheme is also applicable for compressing videos, where a single model can be used to predict color on many consecutive frames, leading to better compression. Existing algorithms for colorization - the process of adding color to a grayscale image or video sequence - are tedious, and require intensive human-intervention. We bypass these limitations by using a graph-based inductive semi-supervised learning module for colorization, and a simple active learning strategy to choose the representative pixels. Experiments on a wide variety of images and video sequences demonstrate the efficacy of our algorithm.
Top/Computer_Science/Image_Analysis	Matching of Tree Structures for Registration of Medical Images Many medical applications require a registration of different images of the same organ. In many cases, such a registration is accomplished by manually placing landmarks in the images. In this paper we propose a method which is able to find reasonable landmarks automatically. To achieve this, nodes of the vessel systems, which have been extracted from the images by a segmentation algorithm, will be assigned by the so-called association graph method and the coordinates of these matched nodes can be used as landmarks for a non-rigid registration algorithm.
Top/Computer_Science/Image_Analysis	Graph Spectral Image Smoothing A new method for smoothing both gray-scale and color images is presented that relies on the heat diffusion equation on a graph. We represent the image pixel lattice using a weighted undirected graph. The edge weights of the graph are determined by the Gaussian weighted distances between local neighbouring windows. We then compute the associated Laplacian matrix (the degree matrix minus the adjacency matrix). Anisotropic diffusion across this weighted graph-structure with time is captured by the heat equation, and the solution, i.e. the heat kernel, is found by exponentiating the Laplacian eigen-system with time. Image smoothing is accomplished by convolving the heat kernel with the image, and its numerical implementation is realized by using the Krylov subspace technique. The method has the effect of smoothing within regions, but does not blur region boundaries. We also demonstrate the relationship between our method, standard diffusion-based PDEs, Fourier domain signal processing and spectral clustering. Experiments and comparisons on standard images illustrate the effectiveness of the method.
Top/Computer_Science/Image_Analysis	Deducing Local Influence Neighbourhoods With Application to Edge-Preserving Image Denoising Traditional image models enforce global smoothness, and more recently Markovian Field priors. Unfortunately global models are inadequate to represent the spatially varying nature of most images, which are much better modeled as piecewise smooth. This paper advocates the concept of local influence neighbourhoods (LINs). The influence neighbourhood of a pixel is defined as the set of neighbouring pixels which have a causal influence on it. LINs can therefore be used as a part of the prior model for Bayesian denoising, deblurring and restoration. Using LINs in prior models can be superior to pixel-based statistical models since they provide higher order information about the local image statistics. LINs are also useful as a tool for higher level tasks like image segmentation. We propose a fast graph cut based algorithm for obtaining optimal influence neighbourhoods, and show how to use them for local filtering operations. Then we present a new expectation-maximization algorithm to perform locally optimal Bayesian denoising. Our results compare favourably with existing denoising methods.
Top/Computer_Science/Image_Analysis	Joint Mining of Biological Text and Images: Case Studies
Top/Computer_Science/Image_Analysis	Detecting Motifs Under Uniform Scaling Time series motifs are approximately repeated patterns found within the data. Such motifs have utility for many data mining algorithms, including rule-discovery, novelty-detection, summarization and clustering. Since the formalization of the problem and the introduction of efficient linear time algorithms, motif discovery has been successfully applied to many domains, including medicine, motion capture, robotics and meteorology. In this work we show that most previous applications of time series motifs have been severely limited by the definitions brittleness to even slight changes of uniform scaling, the speed at which the patterns develop. We introduce a new algorithm that allows discovery of time series motifs with invariance to uniform scaling, and show that it produces objectively superior results in several important domains. Apart from being more general than all other motif discovery algorithms, a further contribution of our work is that it is simpler than previous approaches, in particular we have drastically reduced the number of parameters that need to be specified.
Top/Computer_Science/Image_Analysis	The UoS LAVA group Approach to Generic Image Categorisation
Top/Computer_Science/Image_Analysis	The Future of Image Search There are billions of images on the Internet. Today, searching for a desired image is largely based on textual data such as filename or associated text on the web page; not much use is made of the image content. There are good reasons for this. The field of content-based image retrieval, which emerged during the 1990s, focused primarily on color and texture cues. These were easier to model than shape, but they turned out to be much less useful than originally hoped. I shall review some of the recent developments in the field of visual object recognition in the computer vision community that offer greater promise. Much better image features for characterizing shape, advances in machine learning techniques, and the availability of large amounts of training data lie at the heart of these approaches.
Top/Computer_Science/Image_Analysis	Probabilistic models for understanding images Getting a computer to understand an image is challenging due to the numerous sources of variability that influence the imaging process. The pixels of a typical photograph will depend on the scene type and geometry, the number, shape and appearance of objects present in the scene, their 3D positions and orientations, as well as effects such as occlusion, shading and shadows. The good news is that research into physics and computer graphics has given us a detailed understanding of how these variables affect the resulting image. This understanding can help us to build the right prior knowledge into our probabilistic models of images. In theory, building a model containing all of this knowledge would solve the image understanding problem. In practice, such a model would be intractable for current inference methods. The open challenge for machine learning and machine vision researchers is to create a model which captures the imaging process as accurately as possible, whilst remaining tractable for accurate inference. To illustrate this challenge, I will show how different aspects of the imaging process can be incorporated into models for object detection and segmentation, and discuss techniques for making inference tractable in such models. **//Disclaimer:// Videolectures.Net emphasises that the quality of this video can not be improved, because of low light quality conditions provided in the lecture auditorium.**
Top/Computer_Science/Image_Analysis	Regularisation in Image Analysis
Top/Computer_Science/Image_Analysis	Sparse Geometric Super-Resolution What is the maximum signal resolution that can be recovered from partial noisy or degraded data ? This inverse problem is a central issue, from medical to satellite imaging, from geophysical seismic to HDTV visualization of Internet videos. Increasing an image resolution is possible by taking advantage of 'geometric regularities', whatever it means. Super-resolution can indeed be achieved for signals having a sparse representation which is 'incoherent' relatively to the measurement system. For images and videos, it requires to construct sparse representations in redundant dictionaries of waveforms, which are adapted to geometric image structures. Signal recovery in redundant dictionaries is discussed, and applications are shown in dictionaries of bandlets for image super-resolution.
Top/Computer_Science/Data_Visualisation	Context dependent visualization of protein function Assignment of protein function is a nontrivial task due to the fact that the same proteins may be involved in different biological processes, depending on the state of the biological system and protein localization. Therefore, protein function is context dependent and textual annotations commonly utilized to describe protein function lack the flexibility to address such contextuality.
Top/Computer_Science/Data_Visualisation	Visualization
Top/Computer_Science/Data_Visualisation	Visualization
Top/Computer_Science/Data_Visualisation	Visualization of graphs using their product structure Graphs are combinatorial structures given by a set of vertices and a set of edges giving the adjacencies between pairs of vertices. Drawing graphs nicely is a challenging task. If a graph has some particular structure it is often very useful to use the knowledge about this for visualizing the object. In many cases, there are polynomial time algorithms for recognition of product graphs and graph bundles. We provide several examples and give a short survey of results and open problems.
Top/Computer_Science/Data_Visualisation	Visualization of text document corpus From the automated text processing point of view, natural language is very redundant in the sense that many different words share a common or similar meaning. For computer this can be hard to understand without some background knowledge. Latent Semantic Indexing (LSI) is a technique that helps in extracting some of this background knowledge from corpus of text documents.
Top/Computer_Science/Data_Visualisation	Functional 4D visualisation of proteoliosys: the influence of the micro environment
Top/Computer_Science/Data_Visualisation	Genomic Repeat Visualisation Using Suffix Arrays Repeat analysis is an important technique for understanding the structure of genomic sequences. Here we present a visualisation for describing the repeat character of a sequence, the repeat-score plot. This visualisation allows the identification of all repeats within a sequence.
Top/Computer_Science/Data_Visualisation	PASCAL Visualisation Challenge - Part 1
Top/Computer_Science/Data_Visualisation	PASCAL Visualisation Challenge - Part 2
Top/Computer_Science/Data_Visualisation	Visualisation of Cost Landscapes in Combinatorial Optimisation Problems Understanding the structure of the cost landscape of optimisation problems is important for algorithm design. In this talk, I discuss one approach based on Barrier Trees which captures the structure of the local minima and barriers between minima for search spaces with up to 10^12 states. This structure allows visualisation of heuristic search strategies. Furthermore, the visualisation can be used to construct a model of the problem which captures many of the relevant features of the real problem, but with a vastly reduced number of states. The model can be used for investigating optimal heuristic strategies.
Top/Computer_Science/Data_Visualisation	Text Visualisation Tutorial
Top/Computer_Science/Data_Visualisation	Debunking third-world myths with the best stats you've ever seen You've never seen data presented like this. With the drama and urgency of a sportscaster, [[http://www.ted.com/index.php/speakers/view/id/90|Hans Rosling]] debunks myths about the so-called 'developing world' using extraordinary animation software developed by his Gapminder Foundation. The Trendalyzer software (recently acquired by Google) turns complex global trends into lively animations, making decades of data pop. Asian countries, as colorful bubbles, float across the grid -- toward better national health and wealth. Animated bell curves representing national income distribution squish and flatten. In Rosling's hands, global trends -- life expectancy, child mortality, poverty rates -- become clear, intuitive and even playful. (Recorded February 2006 in Monterey, CA. Duration: 20:35) - More TEDTalks at [[http://www.ted.com/]] ;//'Rosling believes that making information more accessible has the potential to change the quality of the information itself.' ://Business Week Online//
Top/Computer_Science/Data_Visualisation	Unsupervised Prediction of Citation Influences Publication repositories contain an abundance of information about the evolution of scientific research areas. We address the problem of creating a visualization of a research area that describes the flow of topics between papers, quantifies the impact that papers have on each other, and helps to identify key contributions. To this end, we devise a probabilistic topic model that explains the generation of documents; the model incorporates the aspects of topical innovation and topical inheritance via citations. We evaluate the model's ability to predict the strength of influence of citations against manually rated citations.
Top/Computer_Science/Data_Visualisation	Visualizing Pairwise Similarity via Semidefinite Programming
Top/Computer_Science/Data_Visualisation	Visualising the Cluster Structure of Data Streams The increasing availability of streaming data is a consequence of the continuing advancement of data acquisition technology. Such data provides new challenges to the various data analysis communities. Clustering has long been a fundamental procedure for acquiring knowledge from data, and new tools are emerging that allow the clustering of data streams. However the dynamic, temporal components of streaming data provide extra challenges to the development of stream clustering and associated visualisation techniques. In this work we combine a streaming clustering framework with an extension of a static cluster visualisation method, in order to construct a surface that graphically represents the clustering structure of the data stream. The proposed method, OpticsStream, provides intuitive representations of the clustering structure as well as the manner in which this structure changes through time.
Top/Computer_Science/Data_Visualisation	WP2: Learning Web-service Domain Ontologies
Top/Computer_Science/Data_Visualisation	Robust Visual Mining of Data with Error Information
Top/Computer_Science/Data_Visualisation	Self-Adapting Architecture (SAA) New tools and concepts for manipulation of objects in architecture are presented. The concepts include manipulation of local coordinate systems and objects contained in them, as well as performing boolean 3D operations on them. Growing cellular automata (GCA) are used to build and manipulate a large amount of data and relations which occur during the process of construction.
Top/Computer_Science/Data_Visualisation	Transitioning legacy applications to ontologies: Hands-on tutorial - Learning Domain Ontologies
Top/Computer_Science/Semantic_Web/Ontologies	Benchmark Suites for Improving the RDF(S) Importers and Exporters of Ontology Development Tools
Top/Computer_Science/Semantic_Web/Ontologies	Bootstrapping Ontology Evolution with Multimedia Information Extraction
Top/Computer_Science/Semantic_Web/Ontologies	Community-Driven Ontology Matching
Top/Computer_Science/Semantic_Web/Ontologies	Industry 2: Deploying Enterprise Level, Ontology-Driven Faceted Search Currently, one of the most practical application spaces for Semantic Web technologies is creating an ontological layer over existing legacy databases. Such layering allows flexible applications to be built without the cost of restructuring large amounts of data while maintaining the performance advantages of a relational database. Whereas applications designed to directly query a database encode business logic in specific queries, ontological layer offers a flexible framework whereby dynamically generated queries are resilient to schema changes. This same approach can be used to query multiple decentralized databases from a seemingly centralized point of view, allowing access to multiple database schemas via a single interface. In an ontology, Semantic Web technologies such as RDFS, OWL and SWRL can be used to specify composition rules and abstractions, making it possible to answer complex questions without developing complex queries. TopQuadrant has applied this approach to develop and deploy a flexible faceted search system over a network of large, decentralized legacy databases. The system uses ontologies in two distinct ways: as an abstraction layer over an underlying relational data model; and as a search interface model driving the system itself. This model-based approach allows dynamic system configuration simply through changes to the model. The model controls what data can be searched, what facets can be used for building queries, and even how data should be displayed. The system combines the structured power of ontologies with more conventional keyword-based search over a related unstructured document corpus. The resulting hybrid system provides capabilities beyond what is possible with either approach alone. This talk will describe the process used for developing the ontological layer; discuss challenges and technical solutions in integrating the databases and bringing together structured and unstructured search. We will also show the benefits of using ontology to specify the search interface and interaction.
Top/Computer_Science/Semantic_Web/Ontologies	Interactive Ontology-based User Knovledge
Top/Computer_Science/Semantic_Web/Ontologies	In-Use 4: Construction and Use of Role-ontology for Task-based Service Navigation System
Top/Computer_Science/Semantic_Web/Ontologies	Modeling Ontology Evaluation
Top/Computer_Science/Semantic_Web/Ontologies	Ontology Learning - Knowledge Discovery and the Semantic Web
Top/Computer_Science/Semantic_Web/Ontologies	Ontology Management
Top/Computer_Science/Semantic_Web/Ontologies	Research 16: On How to Perform a Gold Standard Based Evaluation of Ontology Learning
Top/Computer_Science/Semantic_Web/Ontologies	Research 1: Innovation Detection based on User-Interest Ontology of Blog Community
Top/Computer_Science/Semantic_Web/Ontologies	Research 3: Formal Model for Ontology Mapping Creation
Top/Computer_Science/Semantic_Web/Ontologies	Research 4: A Survey of the Web Ontology Landscape
Top/Computer_Science/Semantic_Web/Ontologies	Research 4: ONTOCOM: A Cost Estimation Model for Ontology Engineering
Top/Computer_Science/Semantic_Web/Ontologies	Research 5: Ontology-Driven Automatic Entity Disambiguation in Unstructured Text
Top/Computer_Science/Semantic_Web/Ontologies	Research 5: Ontology-driven Information Extraction with OntoSyphon
Top/Computer_Science/Semantic_Web/Ontologies	Research 6: Ontology Query Answering on Databases
Top/Computer_Science/Semantic_Web/Ontologies	Semantic Web and Ontology Management
Top/Computer_Science/Semantic_Web/Ontologies	Semantic Web and Ontology Management Technology
Top/Computer_Science/Semantic_Web/Ontologies	Semantic Web and Ontology Management Technology
Top/Computer_Science/Semantic_Web/Ontologies	Text Mining for Ontology Learning
Top/Computer_Science/Semantic_Web/Ontologies	Text Mining for Ontology Learning
Top/Computer_Science/Semantic_Web/Ontologies	Use of Ontology for production of access systems on Legislation Jurisprudence and Comments
Top/Computer_Science/Semantic_Web/Ontologies	Workshop: Improving the recruitment process through ontology-based querying
Top/Computer_Science/Semantic_Web/Ontologies	Workshop: OntoCAT: An Ontology Consumer Analysis Tool and Its Use on Product Services Categorization Standards
Top/Computer_Science/Semantic_Web/Ontologies	Empiric Merging of Ontologies - A Proposal of Universal Uncertainty Representation Framework
Top/Computer_Science/Semantic_Web/Ontologies	Research 10: On the Semantics of Linking and Importing in Modular Ontologies
Top/Computer_Science/Semantic_Web/Ontologies	Research 11: Block Matching for Ontologies
Top/Computer_Science/Semantic_Web/Ontologies	Research 13: Using Ontologies for Extracting Product Features from Web Pages
Top/Computer_Science/Semantic_Web/Ontologies	Research 4: Ranking Ontologies with AKTiveRank
Top/Computer_Science/Semantic_Web/Ontologies	Semantic Network Analysis of Ontologies - Part 1
Top/Computer_Science/Semantic_Web/Ontologies	Semantic Network Analysis of Ontologies - Part 2
Top/Computer_Science/Semantic_Web/Ontologies	Working with ontologies
Top/Computer_Science/Semantic_Web/Ontologies	Context sensitivity for networked ontologies
Top/Computer_Science/Semantic_Web/Ontologies	Dynamics of Networked Ontologies. Knowledge Transfer talk at the NeOn kick-off meeting, Rome, Italy, March 2006.
Top/Computer_Science/Semantic_Web/Ontologies	Learning from the Masters: Understanding Ontologies found on the Web The purpose of this tutorial is to help attendees gain sufficient experience of working with OWL and tools to allow them to fruitfully explore new ontologies that they may encounter. In other words, they should be able to do the equivalent of view source on an ontology. Also, they will get better fluency in the use and abuse of OWL by examining features, limitations, and workarounds in real contexts, as well as gaining an understanding of the impact of future extensions of OWL, in particular of rules and the proposed revision of the language called OWL 1.1. [[rimg:iswc06_parsia_uofw]] ;This lecture given by Bernardo Cuenca Grau is combined with Bijan Parsia and will encopass Part 1, Part 3, Part 4 of the complete lecture. :Part 2, 5, 6 and 7 of this lecture can be found at [[iswc06_parsia_uofw|//Bijan Parsia 's lecture//]]
Top/Computer_Science/Semantic_Web/Ontologies	Learning from the Masters: Understanding Ontologies found on the Web The purpose of this tutorial is to help attendees gain sufficient experience of working with OWL and tools to allow them to fruitfully explore new ontologies that they may encounter. In other words, they should be able to do the equivalent of view source on an ontology. Also, they will get better fluency in the use and abuse of OWL by examining features, limitations, and workarounds in real contexts, as well as gaining an understanding of the impact of future extensions of OWL, in particular of rules and the proposed revision of the language called OWL 1.1. [[rimg:iswc06_grau_uofwi]] ;This lecture given by Bijan Parsia is combined with Bernardo Cuenca Grau and will encopass Part 2, Part 5, Part 6, Part 7 of the complete lecture. :Part 1, 3 and 4 of this lecture can be found at [[iswc06_grau_uofwi|//Bernardo Cuenca Grau's lecture//]]
Top/Computer_Science/Semantic_Web/Ontologies	Learning from the Masters: Understanding Ontologies found on the Web - Part 7 The purpose of this tutorial is to help attendees gain sufficient experience of working with OWL and tools to allow them to fruitfully explore new ontologies that they may encounter. In other words, they should be able to do the equivalent of view source on an ontology. Also, they will get better fluency in the use and abuse of OWL by examining features, limitations, and workarounds in real contexts, as well as gaining an understanding of the impact of future extensions of OWL, in particular of rules and the proposed revision of the language called OWL 1.1.
Top/Computer_Science/Semantic_Web/Ontologies	Semantic Web Rules with Ontologies, and their E-Services Applications Rules are a main emerging area of the Semantic Web. There has been significant progress in just the last three years in several aspects of Semantic Web rules. This includes exciting developments in the underlying knowledge representation formalisms as well as advances in integration of rules with ontologies; translations between heterogeneous commercial rule engines; development of open-source tools for inferencing and interoperability; standards proposals and efforts (including RuleML, SWRL, Semantic Web Service Framework, and recently W3C Rule Interchange Format); proposals for rule-based semantic Web services; and pilot applications in the emerging area of e-services. This tutorial will provide an introduction to these developments and will explore techniques, applications, and challenges. We will also touch upon the issues of business value, adoption, investment, and strategy considerations.
Top/Computer_Science/Semantic_Web/Ontologies	Ontologies, semantic web and VE
Top/Computer_Science/Semantic_Web/Ontologies	An Efficient Ontology-Based Expert Peering System This paper proposes a novel expert peering system for information exchange. Our objective is to develop a real-time search engine for an online community where users can query experts, who are simply other participating users knowledgeable in that area, for help on various topics.We consider a graph-based scheme consisting of an ontology tree where each node represents a (sub)topic. Consequently, the fields of expertise or profiles of the participating experts correspond to subtrees of this ontology. Since user queries can also be mapped to similar tree structures, assigning queries to relevant experts becomes a problem of graph matching. A serialization of the ontology tree allows us to use simple dot products on the ontology vector space effectively to address this problem. As a demonstrative example, we conduct extensive experiments with different parameterizations. We observe that our approach is efficient and yields promising results.
Top/Computer_Science/Semantic_Web/Ontologies	Cyc Representing, Acquiring and Using Knowledge This lecture will be given by Michael Witbrock, chief director of Cycorp (http://www.cyc.com) which is involved in the construction of the largest knowledge database and system for assumptions. On the lecture he will present us how to encode knowledge and how this knowledge is being used by computers with the intent to perform new unknown facts. He will also show some demos, which are pointing to the capabilities of Cyc which with its quality performs better results than any other known modern method which can be found on the web. We have to mention also that Cycorp has opened its new branch office in Slovenia. On the seminar we will discuss the Cyc system which is said to be one of the most controversial experiments in computer and artificial intelligence history. The idea of Cyc has its beginnings in the 80s when the goal of a group of scientists from the University of Stanford was to build base of knowledge which would incorporate most of the knowledge we operate with in everyday life. On top of the knowledge base a mechanism of conclusion making would be used and would enable the use of encoded knowledge for the formulation of unknown knowledge. After 20 years of development the Cyc system contains a great quantity of common sense knowledge encoded in formal logic. The system was used for a series of difficult applications where a deeper view into the stored information was needed. On the lecture we will be able to see how Cyc works in practice. Cyc and Jozef Stefan Institut, Slovenia have opened a new branch of Cyc to continue the development of the system. Michael Witborck is also the director of the Slovene affiliation.
Top/Computer_Science/Semantic_Web/Ontologies	Cyc: Knowledge Begets Knowledge
Top/Computer_Science/Semantic_Web/Ontologies	Cyc: Knowledge Begets Knowledge
Top/Computer_Science/Semantic_Web/Ontologies	In-Use 1: OntoWiki - A Tool for Social, Semantic Collaboration
Top/Computer_Science/Semantic_Web/Ontologies	Ontology Engineering Methodologies Using the insights gained in a decade of research Asun will describe the main principles and phases underlying the construction of ontologies, including acquisition, conceptualization, evaluation and integration. By the end of this presentation attendees will have an overview of the main steps involved in creating an industrial strength ontology.
Top/Computer_Science/Semantic_Web/Ontologies	Learning to Reason Knowledge Acquisition in Cyc
Top/Computer_Science/Semantic_Web/Ontologies	WP2: Learning Web-service Domain Ontologies
Top/Computer_Science/Semantic_Web/Ontologies	Research 17: A Method for Learning Part-Whole Relations
Top/Computer_Science/Semantic_Web/Ontologies	Ontology construction from text
Top/Computer_Science/Semantic_Web/Ontologies	Planning to Learn with a Knowledge Discovery Ontology Assembly of optimized knowledge discovery workfows requires awareness of and extensive knowledge about the principles and mutual relations between diverse data processing and mining algorithms.We aim at alleviating this burden by automatically proposing workfows for the given type of inputs and required outputs of the discovery process. The methodology adopted in this study is to define a formal conceptualization of knowledge types and data mining algorithms and design a planning algorithm, which extracts constraints from this conceptualization for the given user's input-output requirements. We demonstrate our approach in two use cases, one from scientific discovery in genomics and another from advanced engineering.
Top/Computer_Science/Semantic_Web/Ontologies	Transitioning legacy applications to ontologies: Hands-on tutorial - Learning Domain Ontologies
Top/Computer_Science/Semantic_Web/Ontologies	Reasoning for Ontology Engineering and Usage We will provide a brief introduction to OWL, in fact OWL2, and the underlying Description Logic, clarifying the semantics and providing examples to help the understanding of this admittedly complex formalism. In particular, we will discuss common misunderstandings around OWL and OWL2, explain the open world assumption, inferences, and the functionality of reasoners. We will use the RacerPro reasoner to demonstrate the benefit of using reasoning for query answering over ontologies. Scalability issues with respect to expressive ontologies as well as huge assertional knowledge bases are discussed.
Top/Computer_Science/Semantic_Web/Ontologies	VideoLectures.net case study The task in the VideoLectures case study is to develop a software component that will aid the VideoLectures editors in categorizing recorded lectures (i.e. ontology population). This functionality is required due to the rapid growth of the number of hosted lectures as well as due to the fact that the categorization taxonomy is rather fine-grained (200 categories and growing). In addition to aiding the categorization of new lectures, the software will also be used for re-categorization and additional categorization of lectures already categorized. We will show that we were successful in our task as the categorizer is highly accurate it achieves accuracies that stretch 1220% above the baseline and highly robust in terms of missing data. The latter means that a lecture might be missing textual annotations (such as the description and slide titles) but is still categorized correctly. Furthermore, the categorizer has been successfully integrated into the VideoLectures Web site. Categorization suggestions (termed 'quick links') are provided to the author in the categorization panel.
Top/Computer_Science/Semantic_Web/OWL_-_Web_Ontology_Language	OWL
Top/Computer_Science/Semantic_Web/OWL_-_Web_Ontology_Language	An Introduction to OWL Starting with an overview of the requirements for defining and using ontologies on the Web Sean will then outline the main concepts and issues associated with the Web ontology languaages, RDF, RDFS and OWL. By the end of this presentation attendees will have gained a basic understanding of the principles underlying OWL.
Top/Computer_Science/Semantic_Web/OWL_-_Web_Ontology_Language	Reasoning for Ontology Engineering and Usage We will provide a brief introduction to OWL, in fact OWL2, and the underlying Description Logic, clarifying the semantics and providing examples to help the understanding of this admittedly complex formalism. In particular, we will discuss common misunderstandings around OWL and OWL2, explain the open world assumption, inferences, and the functionality of reasoners. We will use the RacerPro reasoner to demonstrate the benefit of using reasoning for query answering over ontologies. Scalability issues with respect to expressive ontologies as well as huge assertional knowledge bases are discussed.
Top/Computer_Science/Semantic_Web/RDF_-_Resource_Description_Framework	Benchmark Suites for Improving the RDF(S) Importers and Exporters of Ontology Development Tools
Top/Computer_Science/Semantic_Web/RDF_-_Resource_Description_Framework	Research 16: A Relaxed Approach to RDF Querying
Top/Computer_Science/Semantic_Web/RDF_-_Resource_Description_Framework	Research 8: Extending faceted navigation for RDF data
Top/Computer_Science/Semantic_Web/RDF_-_Resource_Description_Framework	Research 8: Fresnel: A Browser-Independent Presentation Vocabulary for RDF
Top/Computer_Science/Semantic_Web/RDF_-_Resource_Description_Framework	Web Content Mining with Human Language Technologies: Coreference resolution on RDF Graphs generated from Information Extraction: first results
Top/Computer_Science/Semantic_Web/RDF_-_Resource_Description_Framework	Workshop: GRDF
Top/Computer_Science/Semantic_Web/RDF_-_Resource_Description_Framework	Industry 2: Integrating Enterprise Data with Semantic Technologies The Semantic Web has reached a level of maturity that allows RDF and OWL to be adopted by large commercial software companies. Many of the products that are based on these standards promise the ability to provide more effective solutions to the increasing IT complexity that many industries are facing. This presentation will describe Oracle's interest in being an early adopter of Semantic Web technologies, as well as providing a technical overview of the RDF Data Model in the latest release of the Oracle Database. Experiences gained from implementing Semantic Web technologies will also be provided.
Top/Computer_Science/Semantic_Web/RDF_-_Resource_Description_Framework	RDFa - Bridging the Web of Documents and the Web of Data RDFa is the bridge between the Web of Documents, targeting at human users, and the Web of Data, focusing on machines. Not only due to the recent uptake of RDFa (Digg, Yahoo!, etc.), learning how and where to use RDFa is essential. This tutorial will introduce the usage of RDFa in real-world use cases and will enable the attendees to work with RDFa both on the client as on the server side. We will create, publish and consume RDFa-marked-up data in the course of the tutorial and discuss advanced aspects, such as dynamic content handling. There are no pre-requisites for participation in the tutorial other than a familiarity with the basics of the (Semantic) Web such as URIs, RDF, XHTML, and SPARQL.
Top/Business/Venture_Capital	Bio entrepeneur - Boot Camp for beginners Goal 1 : Understand the expectations of their future financial partners Venture Capitalists come to explain their ways of working, as well as what makes a 'good' business plan. In the past, we have worked with EVCA and EASD representatives, in addition to independent VCs from European countries. Goal 2 : Share experiences with entrepreneurs of the country Biotech managers come to describe their 'start up' experience, explaining both the easy and more complex aspects of business planning. Goal 3 : Provide TOOLS Learn how to use a platform of tools, including the BioBootCamp software (a program specially developed in the Excel and Microsoft Office environment), so that participants may generate their own business plans after the workshop. Participants may use it following the training to create unlimited simulations to assess the feasibility of a project at no cost and to follow the economics and financials of their new company during the 3 first years. They will learn how to write a convincing Executive Summary to open the doors of investors' offices. Plus of The 'Bioentrepreneur Boot Camp for beginners' Participants also get a set of 'Notes', which includes information about Venture Capital and web sites relating to Bio business. These web sites allow bio-entrepreneurs involved in Bioentrepreneur Boot Camp to continue networking and to identify other areas of need. At the end of the training workshop, each attendee takes away a personal copy of the BioBootCamp software and information package to work on their own project.
Top/Business/Venture_Capital	Cambridge fenomenon of new technologies transfer into the economy Dr. Smeets will introduce the experiences of the university town of Cambridge, one of the most important European centers for the transfer of new technologies into the economy. He will focus on the development of the 'Cambridge phenomenon of new technologies transfer into the economy' and the conditions that influenced the success of this concept. On the basis of the above listed facts he will discuss the mechanisms for achieving this phenomenon. The lecture is being organized by the Slovenian Technology Park in Ljubljana in cooperation with the Jozef Stefan Institute and the British Embassy in Slovenia.
Top/Business/Venture_Capital	Interview with Yoav Andrew Leitersdorf - Venture Capital
Top/Computer_Science/Machine_Learning/Inductive_Logic_Programming	Application of Inductive Logic Programming to Structure-Based Drug Design
Top/Computer_Science/Machine_Learning/Inductive_Logic_Programming	ILP Invited Panel - Structured Machine Learning: The Next 10 Years
Top/Computer_Science/Machine_Learning/Inductive_Logic_Programming	Relational Data Mining and ILP
Top/Computer_Science/Machine_Learning/Inductive_Logic_Programming	The next 10 years of ILP
Top/Computer_Science/Machine_Learning/Inductive_Logic_Programming	Statistical Modeling of Relational Data KDD has traditionally been concerned with mining data from a single relation. However, most applications involve multiple interacting relations, either explicitly (in relational databases) or implicitly (in semi-structured and multimodal data). Examples include link analysis, social networks, bioinformatics, information extraction, security, ubiquitous computing, etc. Mining such data has become a topic of keen interest in the KDD community in recent years. The key difficulty is that data in relational domains is no longer i.i.d. (independent and identically distributed), greatly complicating statistical modeling. However, research has now advanced to the point where robust, easy-to-use, general-purpose techniques and languages for mining non-i.i.d. data are available. The goal of this tutorial is to add a sufficient subset of these concepts and techniques to the toolkits of both researchers and practitioners.
Top/Computer_Science/Machine_Learning/Inductive_Logic_Programming	Learning Probabilistic Stochastic Models from Probabilistic Examples
Top/Computer_Science/Machine_Learning/Inductive_Logic_Programming	Statistical Relational Learning - Part 1 Problems that arise from linkage and autocorrelation among objects must be taken into account. Because instances are linked together, classification typically involves complex inference to arrive at 'collective classification' in which the labels predicted for the test instances are determined jointly rather than individually. Unlike iid problems, where the result of learning is a single classifier, relational learning often involves instances that are heterogeneous, where the result of learning is a set of multiple components (classifiers, probability distributions, etc.) that predict labels of objects and logical relationships between objects.
Top/Computer_Science/Machine_Learning/Inductive_Logic_Programming	Statistical Predicate Invention We propose statistical predicate invention as a key problem for statistical relational learning. SPI is the problem of discovering new concepts, properties and relations in structured data, and generalizes hidden variable discovery in statistical models and predicate invention in ILP. We propose an initial model for SPI based on second-order Markov logic, in which predicates as well as arguments can be variables, and the domain of discourse is not fully known in advance. Our approach iteratively refines clusters of symbols based on the clusters of symbols they appear in atoms with (e.g., it clusters relations by the clusters of the ob jects they relate). Since different clusterings are better for predicting different subsets of the atoms, we allow multiple cross-cutting clusterings. We show that this approach outperforms Markov logic structure learning and the recently introduced infinite relational model on a number of relational datasets.
Top/Computer_Science/Machine_Learning/Inductive_Logic_Programming	Mining Relational Model Trees Multi-Relational Data Mining (MRDM) refers to the process of discovering implicit, previously unknown and potentially useful information from data scattered in multiple tables of a relational database. MRDM is necessary to face the substantial complexity added to data mining tasks when properties of units of analysis to be investigated are potentially affected by attributes of related units of analysis eventually of different types and naturally modeled to yield as many tables as the number of object types. Regression is a fundamental task in MRDM where the goal is to examine samples of past experience with known continuous answers (response) and generalize future cases throughan inductive process. Following the mainstream of MRDM research, Mr-SMOTI resorts to the structural approach in order to recursively partition data stored in a tightly-coupled database and build a multi-relational model tree that captures the linear dependence between the response variable and one or more explanatory variables of both the reference objects and task-relevant objects. The model tree is top-down induced by choosing, at each step, either to partition the training space (split nodes) or to introduce a regression variable in the linear models to be associated with the leaves (regression nodes). Internal regression nodes contribute to the definition of multiple models and capture global effects, while straight-line regressions with leaves capture only local effects. The tight-coupling with the database makes the knowledge on data structures (e.g., foreign keys) available free of charge to guide the search in the multi-relational pattern space.
Top/Computer_Science/Machine_Learning/Inductive_Logic_Programming	Fast Inference in Infinite Hidden Relational Models Relational learning is an area of growing interest in machine learning (Dzeroski & Lavrac, 2001; Friedman et al., 1999; Raedt & Kersting, 2003). Xu et al. (2006) introduced the infinite hidden relational model (IHRM) which views relational learning in context of the entity-relationship database model with entities, attributes and relations (compare also (Kemp et al., 2006)). In the IHRM, for each entity a latent variable is introduced. The latent variable is the only parent of the other entity attributes and is a parent of relationship attributes. The number of states in each latent variable is entity class specific. Therefore it is sensible to work with Dirichlet process (DP) mixture models in which each entity class can optimize its own representational complexity in a self-organized way. For our discussion it is sufficient to say that we integrate a DP mixture model into the IHRM by simply letting the number of hidden states for each entity class approach infinity. Thus, a natural outcome of the IHRM is a clustering of the entities providing interesting insight into the structure of the domain.
Top/Computer_Science/Machine_Learning/Inductive_Logic_Programming	Statistical Relational Learning - Part 2 Statistical machine learning is in the midst of a 'relational revolution'. After many decades of focusing on independent and identically-distributed (iid) examples, many researchers are now studying problems in which the examples are linked together into complex networks. These networks can be a simple as sequences and 2-D meshes (such as those arising in part-of-speech tagging and remote sensing) or as complex as citation graphs, the world wide web, and relational data bases.
Top/Computer_Science/Machine_Learning/Inductive_Logic_Programming	An Introduction to Statistical Relational Learning
Top/Computer_Science/Machine_Learning/Neural_Networks	Exploring Spatially Embedded Artificial Neural Networks
Top/Computer_Science/Machine_Learning/Neural_Networks	Implications of decoding for theories of neural representation
Top/Computer_Science/Machine_Learning/Neural_Networks	Convolutional Object Finder, A Neural Architecture for Fast and Robust Object Detection
Top/Computer_Science/Machine_Learning/Neural_Networks	Mixtures of Neural Nets
Top/Computer_Science/Machine_Learning/Neural_Networks	Dance evolution The only submission from undergraduates, this unique video challenges AI to learn how to dance by demonstrating how neuroevolution can be used to (interactively) evolve dancing techniques.
Top/Computer_Science/Machine_Learning/Neural_Networks	Stochastic Learning
Top/Computer_Science/Machine_Learning/Neural_Networks	The Dynamics of Viral Marketing We present an analysis of a person-to-person recommendation network, consisting of 4 million people who made 16 million recommendations on half a million products. We observed the propagation of recommendations and the cascade sizes, which can be explained by a stochastic model. We then established how the recommendation network grows over time and how effective it is from the viewpoint of the sender and receiver of the recommendations. While on average recommendations are not very effective at inducing purchases and do not spread very far, there are product and pricing categories for which viral marketing seems to be very effective. This is a joint work with Lada Adamic and Bernardo Huberman from HP Labs.
Top/Computer_Science/Machine_Learning/Neural_Networks	Tandem Connectionist Feature Extraction for Conversational Speech Recognition Multi-Layer Perceptrons (MLPs) can be used in automatic speech recognition in many ways. A particular application of this tool over the last few years has been the Tandem approach, as described by Hermansky et al in a number of publications. Here we discuss the characteristics of the MLP-based features used for the Tandem approach, and conclude with a report on their application to conversational speech recognition. The paper shows that MLP transformations yield variables that have regular distributions, which can be further modified by using logarithm to make the distribution easier to model by a Gaussian-HMM. Two or more vectors of these features can easily be combined without increasing the feature dimension. We also report recognition results that show that MLP features can significantly improve recognition performance for the NIST 2001 Hub-5 evaluation set with models trained on the Switchboard Corpus, even for complex systems incorporating MMIE training and other enhancements.
Top/Computer_Science/Machine_Learning/Neural_Networks	Real Time GPU-Based Fuzzy ART Skin Recognition
Top/Computer_Science/Machine_Learning/Human_Language_Technology	Human Language technology for the Semantic Web In this talk I will present an overview of Human Language Technology (HLT) and its use in Semantic Web development. HLT is concerned with automatic linguistic processing towards the semantic analysis and extraction of information from textual data. In the context of the Semantic Web the use of HLT is in knowledge markup of web documents for ontology population and text mining for ontology evolution (extension and modification of ontology models). The talk will include examples of both as currently developed in the context of the SmartWeb project on 'Mobile Broadband Access to the Semantic Web' - http://www.smartweb-projekt.de/
Top/Computer_Science/Machine_Learning/Human_Language_Technology	Human Language Technology for the Semantic Web
Top/Computer_Science/Machine_Learning/Human_Language_Technology	Human Language Technology for the Semantic Web
Top/Computer_Science/Machine_Learning/Human_Language_Technology	Information Retrieval and Language Technology The course will give an overview of how statistical learning can help organize and access information that is represented in textual form. In particular, it will cover tasks like text classification, information retrieval, information extraction, topic detection, and topic tracking. The course will introduce the basic techniques for representing text and analyze their statistical properties. An emphasis of the course will be on giving an overview of interesting learning problems in this area, providing starting points for future research.
Top/Computer_Science/Machine_Learning/Human_Language_Technology	Language Technologies
Top/Computer_Science/Machine_Learning/Human_Language_Technology	Language Technologies This tutorial covers the use of Human Language Technologies for the Semantic Web and Web Services. It includes sections on HLT and Text Mining for the Semantic Web, various forms of Information Extraction, Ontology Population and Semantic Metadata Creation, and Evaluation. The tutorial begins with an introduction to Human Language Technology, looking at both its background and development, and then situating it within the context of text mining and other tasks involving knowledge discovery from large collections of unstructured text, which are necessary for the development of the semantic web. The second section concerns information extraction, a major component of text mining. Information extraction involves extracting facts and structured information from unstructured data. We contrast this with Information retrieval, which concerns extracting documents from large text collections, and with data mining, which concerns discoveing patterns in structured data. We introduce GATE, and architecture for language engineering, and its resources for information extraction, and then expand the idea of traditional information extraction to focus on semantic web-enabled technology such as ontology population and semantic metadata creation, both of which involve the use of information extraction based on ontologies. We look at some current state-of-the-art semantic annotation systems such as KIM, Magpie, MnM and OntoMat. In the third section, we discuss evaluation methods for such technology, based on the idea that traditional methods are insufficient when applied to semantic web technology, due to the presence of hierarchical (ontological) information rather than flat structures. We also take a brief look at usability issues of annotation systems. Finally, the tutorial gives demonstrations of two examples of HLT in use for the semantic web. First we present RichNews, which aims to automate the annotation of news programs, segmenting, describing and classifying news broadcasts from transcripts. Second, we present work on ontology-based and mixed initiative information extraction carried out in the context of SEKT.
Top/Computer_Science/Machine_Learning/Human_Language_Technology	Language Technologies
Top/Computer_Science/Machine_Learning/Human_Language_Technology	Language Technologies
Top/Computer_Science/Machine_Learning/Human_Language_Technology	Metadata Extraction: Human Language technology and the Semantic Web
Top/Computer_Science/Machine_Learning/Human_Language_Technology	Metadata Extraction: Human Language technology and the Semantic Web - Part 1
Top/Computer_Science/Machine_Learning/Human_Language_Technology	Metadata Extraction: Human Language technology and the Semantic Web - Part 3
Top/Computer_Science/Machine_Learning/Human_Language_Technology	Metadata Extraction: Human Language technology and the Semantic Web - Part 4
Top/Computer_Science/Machine_Learning/Human_Language_Technology	Metadata Extraction: Human Language technology and the Semantic Web - Part 5
Top/Computer_Science/Machine_Learning/Human_Language_Technology	Metadata Extraction: Human Language technology and the Semantic Web - Part 6
Top/Computer_Science/Machine_Learning/Human_Language_Technology	Metadata Extraction: Human Language technology and the Semantic Web part 7
Top/Computer_Science/Machine_Learning/Human_Language_Technology	Semantic Web(s) and Language Technology
Top/Computer_Science/Machine_Learning/Human_Language_Technology	Web Content Mining with Human Language Technologies: Acquiring Ontological Relationships from Wikipedia Using RMRS
Top/Computer_Science/Machine_Learning/Human_Language_Technology	Web Content Mining with Human Language Technologies: Concept-Instance Relation Extraction from Simple Noun Sequences Using a Full-Text Search Engine
Top/Computer_Science/Machine_Learning/Human_Language_Technology	Web Content Mining with Human Language Technologies: Constructing Dictionaries for Named Entity Recognition on Specific Domains from the Web
Top/Computer_Science/Machine_Learning/Human_Language_Technology	Web Content Mining with Human Language Technologies: Coreference resolution on RDF Graphs generated from Information Extraction: first results
Top/Computer_Science/Machine_Learning/Human_Language_Technology	Web Content Mining with Human Language Technologies: Instance Classification using Co-Occurrences on the Web
Top/Computer_Science/Machine_Learning/Human_Language_Technology	Web Content Mining with Human Language Technologies: Mining and Assessing Discussions on the Web through Speech Act Analysis
Top/Computer_Science/Machine_Learning/Human_Language_Technology	Web Content Mining with Human Language Technologies: Welcoming and Introduction
Top/Computer_Science/Machine_Learning/Human_Language_Technology	Language Models for Information Retrieval
Top/Computer_Science/Machine_Learning/Human_Language_Technology	Thoughts on Language Models in Machine Translation
Top/Computer_Science/Machine_Learning/Human_Language_Technology	Adventures with Camille A computational simulation of a minimalist language learner
Top/Computer_Science/Machine_Learning/Human_Language_Technology	Grammatical Inference: a Tutorial The leactures will introduce the key ideas of grammatical inference and concentrate specially on the algorithmic aspects. Some algorithms that will be described are: The 'State merging' family : Gold, Rpni, Edsm... The 'Window' languages : Local and k-testable Learning with queries.
Top/Computer_Science/Machine_Learning/Human_Language_Technology	The Long Road from Text to Meaning Computers have given us a new way of thinking about language. Given a large sample of language, or corpus, and computational tools to process it, we can approach language as physicists approach forces and chemists approach chemicals. This approach is noteworthy for missing out what, from a language-user's point of view, is important about a piece of language: its meaning. I shall present this empiricist approach to the study of language and show how, as we develop accurate tools for lemmatisation, part-of-speech tagging and parsing, we move from the raw input -- a character stream -- to an analysis of that stream in increasingly rich terms: words, lemmas, grammatical structures, Fillmore-style frames. Each step on the journey builds on a large corpus accurately analysed at the previous levels. A distributional thesaurus provides generalisations about lexical behaviour which can then feed into an analysis at the frames' level. The talk will be illustrated with work done within the Sketch Engine' tool. For much NLP and linguistic theory, meaning is a given. Thus formal semantics assumes meanings for words, in order to address questions of how they combine, and WSD (word sense disambiguation) typically takes a set of meanings (as found in a dictionary) as a starting point and sets itself the challenge of identifying which meaning applies. But, since the birth of philosophy, meaning has been problematic. In our approach meaning is an eventual output of the research programme, not an input.
Top/Computer_Science/Machine_Learning/Human_Language_Technology	The Work of a Professional Translator
Top/Computer_Science/Machine_Learning/Human_Language_Technology	Learning to Distinguish Valid Textual Entailments This paper proposes a new architecture for textual inference in which finding a good alignment is separated from evaluating entailment. Current approaches to semantic inference in question answering and textual entailment have approximated the entailment problem as that of computing the best alignment of the hypothesis to the text, using a locally decomposable matching score. While this formulation is adequate for representing local (word-level) phenomena such as synonymy, it is incapable of representing global interactions, such as that between verb negation and the addition/removal of qualifiers, which are often critical for determining entailment.
Top/Computer_Science/Machine_Learning/Human_Language_Technology	An introduction to grammars and parsing From MJ: 'The following is a fairly advanced summary of the material I'll be covering in my talk. Don't be dismayed if you find it hard to understand now; I hope that after my talk it will be much clearer!
Top/Computer_Science/Machine_Learning/Human_Language_Technology	Learning Probabilistic Stochastic Models from Probabilistic Examples
Top/Computer_Science/Machine_Learning/Human_Language_Technology	GATE APIs, CREOLE lifecycle, JAVA for JAPE
Top/Computer_Science/Machine_Learning/Human_Language_Technology	Automatic Labeling of Multinomial Topic Models Multinomial distributions over words are frequently used to model topics in text collections. A common, major challenge in applying all such topic models to any text mining problem is to label a multinomial topic model accurately so that a user can interpret the discovered topic. So far, such labels have been generated manually in a subjective way. In this paper, we propose probabilistic approaches to automatically labeling multinomial topic models in an objective way. We cast this labeling problem as an optimization problem involving minimizing Kullback-Leibler divergence between word distributions and maximizing mutual information between a label and a topic model. Experiments with user study have been done on two text data sets with different genres. The results show that the proposed labeling methods are quite effective to generate labels that are meaningful and useful for interpreting the discovered topic models. Our methods are general and can be applied to labeling topics learned through all kinds of topic models such as PLSA, LDA, and their variations.
Top/Computer_Science/Machine_Learning/Human_Language_Technology	Statistical Machine Translation
Top/Computer_Science/Machine_Learning/Human_Language_Technology	Robust Textual Inference Using Diverse Knowledge Sources
Top/Computer_Science/Machine_Learning/Human_Language_Technology	Language Modelling
Top/Computer_Science/Machine_Learning/Human_Language_Technology	Speech-to-Speech Translation Services for the Olympic Games 2008
Top/Computer_Science/Machine_Learning/Human_Language_Technology	Structural Prediction in Statistical Alignment and Translation
Top/Computer_Science/Machine_Learning/Human_Language_Technology	Optimizing Local Probability Models for Statistical Parsing - Best Student Paper Runner-up
Top/Computer_Science/Machine_Learning/Human_Language_Technology	A Simpler, Intuitive Approach to Morpheme Induction
Top/Computer_Science/Machine_Learning/Human_Language_Technology	Detecting Action Items in Multi-Party Meetings: Annotation and Initial Experiments
Top/Computer_Science/Machine_Learning/Human_Language_Technology	Automated Text Summarization using MEAD: Experience with the IMF Staff Reports
Top/Computer_Science/Machine_Learning/Human_Language_Technology	Development eco-system: building components, usage in Eclipse, unit tests
Top/Computer_Science/Machine_Learning/Human_Language_Technology	Evaluation of MT and CLITIA
Top/Computer_Science/Machine_Learning/Human_Language_Technology	Going beyond bag-of-words: dealing with a text as a graph of triples
Top/Computer_Science/Machine_Learning/Human_Language_Technology	Machine Learning for Sequential Data: A Comparative Study with Applications to Natural Language Processing
Top/Computer_Science/Machine_Learning/Human_Language_Technology	The use of machine translation tools for cross-lingual text-mining
Top/Computer_Science/Machine_Learning/Human_Language_Technology	Personalized Web Search Engine for Mobile Devices
Top/Computer_Science/Machine_Learning/Human_Language_Technology	Tandem Connectionist Feature Extraction for Conversational Speech Recognition Multi-Layer Perceptrons (MLPs) can be used in automatic speech recognition in many ways. A particular application of this tool over the last few years has been the Tandem approach, as described by Hermansky et al in a number of publications. Here we discuss the characteristics of the MLP-based features used for the Tandem approach, and conclude with a report on their application to conversational speech recognition. The paper shows that MLP transformations yield variables that have regular distributions, which can be further modified by using logarithm to make the distribution easier to model by a Gaussian-HMM. Two or more vectors of these features can easily be combined without increasing the feature dimension. We also report recognition results that show that MLP features can significantly improve recognition performance for the NIST 2001 Hub-5 evaluation set with models trained on the Switchboard Corpus, even for complex systems incorporating MMIE training and other enhancements.
Top/Computer_Science/Machine_Learning/Graphical_Models	Graphical Models for HIV Vaccine Design I will discuss two applications of graphical models to HIV vaccine design. The first helps determine how strongly our immune system fights HIV. The second helps identify which parts of HIV can be successfully attacked by our immune system. I will also discuss how these applications have exposed a weakness in the process of learning graphical models from data---namely, the inability to quantify how many arcs in a learned graphical model are spurious. I will offer a solution based on the False Discovery Rate.
Top/Computer_Science/Machine_Learning/Graphical_Models	Bounds and estimates for BP convergence on binary undirected graphical models Belief Propagation (BP) has become a popular method for inference on graphical models. Accurate approximations for intractable quantities (e.g. single-node marginals) can be obtained within rather modest computation times. However, for large interaction strengths (i.e. potentials that are highly dependent on their arguments) or densely connected graphs, BP can fail to converge. This can be remedied by damping the iteration equations, using sequential update schemes or using entirely different algorithms such as double-loop algorithms, that directly minimize the Bethe free energy. However, the value of this approach is questionable, since there is empirical evidence that failure of convergence of BP often indicates low quality of the Bethe approximation.
Top/Computer_Science/Machine_Learning/Graphical_Models	Discriminative Graphical Models for Protein Quaternary Structure Motif Detection
Top/Computer_Science/Machine_Learning/Graphical_Models	Estimating MAP-configurations in graphical models by exploiting structure The max-product algorithm can be used to obtain approximate MAP-assignments of the probability distribution defined by a graphical model. On tree-structured graphical models the MAP-assignment is exact and the max-product algorithm is equivalent to the Viterbi-algorithm. On general models one may run into the following problems: 1. The algorithm does not converge; 2. the single node marginals are not unique. The first problem can be solved by using a provably convergent double loop algorithm. In the second case it is not straightforward how to obtain a global assignment from the locally defined marginals, due to loops in the graph. An obvious solution to the second problem is to use the approximate marginals for pairs (or any tractable number) of nodes and use the correlations to estimate a global MAP-assignment. A simple strategy is to define a satisfiability problem which entails that the global MAP-assignment should be a MAP-assignment of each local marginal. This should in principle solve the problem of non-unique marginals. However, this satisfiability problem is not guaranteed to have a solution. The existence of a solution depends critically on the nature of the interactions between the nodes in the graphical model.
Top/Computer_Science/Machine_Learning/Graphical_Models	Graphical Models and Variational Methods In this course I will discuss how exponential families, a standard tool in statistics, can be used with great success in machine learning to unify many existing algorithms and to invent novel ones quite effortlessly. In particular, I will show how they can be used in feature space to recover Gaussian Process classification for multiclass discrimination, sequence annotation (via Conditional Random Fields), and how they can lead to Gaussian Process Regression with heteroscedastic noise assumptions.
Top/Computer_Science/Machine_Learning/Graphical_Models	Graphical Models for Structural Pattern Recognition In the 'structural' paradigm for visual pattern recognition, or what some call 'strong' pattern recognition, one is not satisfied with simply assigning a class label to an input object, but instead we aim at finding exactly which parts of the template object correspond to which parts of the scene. This is a much harder problem in principle, because it is inherently combinatorial on the number of parts (features) involved, both in the template object and in the scene. This talk describes a summary of our research efforts in setting this as a mathematical optimization problem and solving it efficiently by exploiting geometric constraints. The key insight involves encoding geometric constraints as conditional independency assumptions in a probabilistic graphical model. Due to some geometric facts, it is possible to show that such models are very well behaved: they allow for exact probabilistic inference in polynomial time. The result is a unified framework for structural visual pattern recognition that is able to handle in a principled way a variety of problems, including point pattern matching in its many instances: invariant to translations, isometries, scalings, affine or projective transformations. Attributed graph matching problems, such as matching road networks, can also be solved within such framework. Limitations and future directions will be discussed.
Top/Computer_Science/Machine_Learning/Graphical_Models	Graphical Models, Variational Methods, and Message-Passing
Top/Computer_Science/Machine_Learning/Graphical_Models	L1-based relaxations for sparsity recovery and graphical model selection in the high-dimensional regime The problem of estimating a sparse signal embedded in noise arises in various contexts, including signal denoising and approximation, as well as graphical model selection. The natural optimization-theoretic formulation of such problems involves 'norm' constraints (i.e., penalties on the number of non-zero coefficients), which leads to NP-hard problems in general. A natural approach is to consider the use of the -norm as a computationally tractable surrogate, as has been pursued in both signal processing and statistics.
Top/Computer_Science/Machine_Learning/Graphical_Models	Learning Causal Graphical Models with Latent Variables
Top/Computer_Science/Machine_Learning/Graphical_Models	Machine Learning, Probability and Graphical Models
Top/Computer_Science/Machine_Learning/Graphical_Models	Probabilistic Graphical Models My lectures will cover the basics of graphical models, also known as Bayes(ian) (Belief) Net(work)s. We will cover the basic motivations for using probabilities to represent and reason about uncertain knowledge in machine learning, and introduce graphical models as a qualitative and quantitative specification of large joint probability distributions. We will see how many common classification, regression and clustering models can be cast in this framework. We will cover the basic algorithm (called belief propagation) for inference in graphical model structures. We will also cover the major approaches to learning models from data (parameter estimation). The course will focus on directed models and the basic algorithms, but time and student desire permitting, I will also try to give some preliminary explanations of undirected models, approximate inference and learning, structure discovery and current applications.
Top/Computer_Science/Machine_Learning/Graphical_Models	Three New Graphical Models for Statistical Language Modelling The supremacy of n-gram models in statistical language modelling has recently been challenged by parametric models that use distributed representations to counteract the difficulties caused by data sparsity. We propose three new probabilistic language models that define the distribution of the next word in a sequence given several preceding words by using distributed representations of those words. We show how real-valued distributed representations for words can be learned at the same time as learning a large set of stochastic binary hidden features that are used to predict the distributed representation of the next word from previous distributed representations. Adding connections from the previous states of the binary hidden features improves performance as does adding direct connections between the real-valued distributed representations. One of our models significantly outperforms the very best ngram models.
Top/Computer_Science/Machine_Learning/Graphical_Models	Undirected Graphical Models for Text & Image
Top/Computer_Science/Machine_Learning/Graphical_Models	Graphical models An introduction to directed and undirected probabilistic graphical models, including inference (belief propagation and the junction tree algorithm), parameter learning and structure learning, variational approximations, and approximate inference. - Introduction to graphical models: (directed, undirected and factor graphs; conditional independence; d-separation; plate notation) - Inference and propagation algorithms: (belief propagation; factor graph propagation; forward-backward and Kalman smoothing; the junction tree algorithm) - Learning parameters and structure: maximum likelihood and Bayesian parameter learning for complete and incomplete data; EM; Dirichlet distributions; score-based structure learning; Bayesian structural EM; brief comments on causality and on learning undirected models) - Approximate Inference: (Laplace approximation; BIC; variational Bayesian EM; variational message passing; VB for model selection) - Bayesian information retrieval using sets of items: (Bayesian Sets; Applications) - Foundations of Bayesian inference: (Cox Theorem; Dutch Book Theorem; Asymptotic consensus and certainty; choosing priors; limitations)
Top/Computer_Science/Machine_Learning/Graphical_Models	Transductive Rademacher complexities for learning over a graph Recent investigations indicate the use of a probabilistic learning perspective of tasks defined on a single graph, as opposed to the traditional algorithmical computational point of view. This note discusses the use of Rademacher complexities in this setting, and illustrates the use of Kruskals algorithm for transductive inference based on a nearest neighbor rule.
Top/Computer_Science/Machine_Learning/Graphical_Models	Generative Latent Space Models for Text and Image
Top/Computer_Science/Machine_Learning/Graphical_Models	Learning CRFs with Hierarchical Features: An Application to Go
Top/Computer_Science/Machine_Learning/Graphical_Models	Graphical Models In the last decade probabilistic graphical models -- in particular Bayes networks and Markov networks -- became very popular as tools for structuring uncertain knowledge about a domain of interest and for building knowledge-based systems that allow sound and efficient inferences about this domain. The lecture gives a brief introduction into the core ideas underlying graphical models, starting from their relational counterparts and highlighting the relation between independence and decomposition. Furthermore, the basics of model construction and evidence propagation are discussed, with an emphasis on join/junction tree propagation. A substantial part of the lecture is then devoted to learning graphical models from data, in which quantitative learning (parameter estimation) as well as the more complex qualitative or structural learning (model selection) are studied.
Top/Computer_Science/Machine_Learning/Graphical_Models	An SMO-like algorithm for Kernel Conditional Random Fields
Top/Computer_Science/Machine_Learning/Graphical_Models	Graphical models This presentations provide an introduction to graphical models together with more advanced topics on inference, propagation and learning structure.
Top/Computer_Science/Machine_Learning/Graphical_Models	Inference in Graphical Models This short course will cover the basics of inference in graphical models. It will start by explaining the theory of probabilistic graphical models, including concepts of conditional independence and factorisation and how they arise in both Markov random fields and Bayesian Networks. He will then present the fundamental methods for performing exact probabilistic inference in such models, which include algorithms like variable elimination, belief propagation and Junction Trees. He will also briefly discuss some of the current methods for performing approximate inference when exact inference is not feasible. Finally, he will illustrate a range of real problems whose solutions can be formulated as inference in graphical models.
Top/Computer_Science/Machine_Learning/Graphical_Models	Grouping Using Factor Graphs: an Approach for Finding Text with a Camera Phone We introduce a new framework for feature grouping based on factor graphs, which are graphical models that encode interactions among arbitrary numbers of random variables. The ability of factor graphs to express interactions higher than pairwise order (the highest order encountered in most graphical models used in computer vision) is useful for modeling a variety of pattern recognition problems. In particular, we show how this property makes factor graphs a natural framework for performing grouping and segmentation, which we apply to the problem of finding text in natural scenes. We demonstrate an implementation of our factor graph-based algorithm for finding text on a Nokia camera phone, which is intended for eventual use in a camera phone system that finds and reads text (such as street signs) in natural environments for blind users.
Top/Computer_Science/Machine_Learning/Graphical_Models	Probabilistic Graphical Models and Structured Prediction
Top/Computer_Science/Information_Extraction	Bootstrapping Ontology Evolution with Multimedia Information Extraction
Top/Computer_Science/Information_Extraction	Elie: A two-level boundary classification approach to adaptive information extraction
Top/Computer_Science/Information_Extraction	Evaluating Machine Learning for Information Extraction
Top/Computer_Science/Information_Extraction	Information extraction
Top/Computer_Science/Information_Extraction	Onthology Based Information Extraction
Top/Computer_Science/Information_Extraction	Onthology Based Information Extraction and Gate
Top/Computer_Science/Information_Extraction	Pascal Challenge on Evaluating Machine Learning for Information Extraction: Goals, Results and Conclusions
Top/Computer_Science/Information_Extraction	Research 3: Towards Knowledge Acquisition from Information Extraction
Top/Computer_Science/Information_Extraction	Research 5: Ontology-driven Information Extraction with OntoSyphon
Top/Computer_Science/Information_Extraction	Selective Sampling for Information Extraction with a Committee of Classifiers
Top/Computer_Science/Information_Extraction	Template Sampling for Leveraging Domain Knowledge in Information Extraction
Top/Computer_Science/Information_Extraction	Text Information Extraction
Top/Computer_Science/Information_Extraction	Web Content Mining with Human Language Technologies: Coreference resolution on RDF Graphs generated from Information Extraction: first results
Top/Computer_Science/Information_Extraction	Information extraction
Top/Computer_Science/Information_Extraction	Robust Textual Inference Using Diverse Knowledge Sources
Top/Computer_Science/Information_Extraction	System for extracting data (facts) from large amount of unstructured documents
Top/Computer_Science/Information_Extraction	Extracting Semantic Relations from Query Logs In this paper we study a large query log of more than twenty million queries with the goal of extracting the semantic relations that are implicitly captured in the actions of users submitting queries and clicking answers. Previous query log analyses were mostly done with just the queries and not the actions that followed after them. We first propose a novel way to represent queries in a vector space based on a graph derived from the query-click bipartite graph. We then analyze the graph produced by our query log, showing that it is less sparse than previous results suggested, and that almost all the measures of these graphs follow power laws, shedding some light on the searching user behavior as well as on the distribution of topics that people want in the Web. The representation we introduce allows to infer interesting semantic relationships between queries. Second, we provide an experimental analysis on the quality of these relations, showing that most of them are relevant. Finally we sketch an application that detects multitopical URLs.
Top/Computer_Science/Information_Extraction	Going beyond bag-of-words: dealing with a text as a graph of triples
Top/Computer_Science/Information_Extraction	A Self-Organizing Map for Relation Extraction from Wikipedia using Structured Data Representations
Top/Computer_Science/Information_Extraction	The GATE GUI
Top/Computer_Science/Information_Extraction	GATE and IBMs UIMA - interoperability layer
Top/Computer_Science/Information_Extraction	Finite state transduction for Information Extraction and other tasks: ANNIE, JAPE - Part 1
Top/Computer_Science/Information_Extraction	Corpora, evaluation tools
Top/Computer_Science/Information_Extraction	Industry 3: How Co-Occurrence can Complement Semantics? Analysis of texts is an obvious way for semantic annotation and extraction of structured knowledge. A basic task is the recognition of references to entities (people, locations, organizations, etc). A next step is relation extraction, e.g. identifying that an organization is located in a particular city. Automatic extraction of such relations is a tough linguistic problem - the solutions are either very partial, expensive to implement, or slow. On the other hand, relationships are crucial for the usability of the extracted knowledge for navigation and search purposes. We demonstrate how efficient co-occurrence analysis, performed on top of semantic annotation, can be used for several purposes: relation extraction, faceted search, and popularity timelines. The faceted search interface allows an easy way for augmenting full-text search by means of entity references, derived through co-occurrence profiling and semantic relationships. Although this sort of analytics can be used in virtually any domain, their development within the KIM platform was driven by the requirements for news analysis and research. We demonstrate the usage of these interfaces on top of 1 million news articles - a corpus of the major international news for the last five years. This sort of co-occurrence analysis has the potential of aiding identity resolution, which is recognized to be a crucial problem for several tasks: cross-document co-reference resolution, record linkage, object linking, and data integration.
Top/Computer_Science/Information_Extraction	Research 1: Extracting Relations in Social Networks from Web using Similarity between Collective Contexts
Top/Computer_Science/Information_Extraction	Webpage Understanding: an Integrated Approach Recent work has shown the effectiveness of leveraging layout and tag-tree structure for segmenting webpages and labeling HTML elements. However, how to effectively segment and label the text contents inside HTML elements is still an open problem. Since many text contents on a webpage are often text fragments and not strictly grammatical, traditional natural language processing techniques, that typically expect grammatical sentences, are no longer directly applicable. In this paper, we examine how to use layout and tag-tree structure in a principled way to help understand text contents on webpages. We propose to segment and label the page structure and the text content of a webpage in a joint discriminative probabilistic model. In this model, semantic labels of page structure can be leveraged to help text content understanding, and semantic labels of the text phrases can be used in page structure understanding tasks such as data record detection. Thus, integration of both page structure and text content understanding leads to an integrated solution of webpage understanding. Experimental results on research homepage extraction show the feasibility and promise of our approach.
Top/Computer_Science/Information_Extraction	Beyond String Search: Fast and Accurate Retrieval of Entities and Dependencies
Top/Computer_Science/Information_Extraction	Finite state transduction for Information Extraction and other tasks: ANNIE, JAPE - Part 2
Top/Computer_Science/Information_Extraction	Ontology-based Information Extraction for Business Intelligence Business Intelligence (BI) requires the acquisition and aggregation of key pieces of knowledge from multiple sources in order to provide valuable information to customers or feed statistical BI models and tools. The massive amount of information available to business analysts makes information extraction and other natural language processing tools key enablers for the acquisition and use of that semantic information. We describe the application of ontology-based extraction and merging in the context of a practical e-business application for the EU MUSING Project where the goal is to gather international company intelligence and country/region information. The results of our experiments so far are very promising and we are now in the process of building a complete end-to-end solution.
Top/Computer_Science/Information_Extraction	Learning, Information Extraction and the Web
Top/Computer_Science/Information_Extraction	Using the Web to Reduce Data Sparseness in Pattern-based Information
Top/Business/Virtual_Enterprises	A Formal Theory Of Bm Virtual Enterprises Structures
Top/Business/Virtual_Enterprises	Efficiently Managing Virtual Organizations Through Distributed Innovation Management Processes
Top/Business/Virtual_Enterprises	EVCN Engeneer Virtual Community Network - PVC for Local Developement
Top/Business/Virtual_Enterprises	ICT support for Virtual Organization Management
Top/Business/Virtual_Enterprises	Knowledge Based Virtual Communities for Collaborative work in the EU Vehicle Repair Works
Top/Business/Virtual_Enterprises	Professional Virtual Communities / Concepts, expected results, intermediate results
Top/Business/Virtual_Enterprises	Social protocols in Professional Virtual Communitie
Top/Business/Virtual_Enterprises	The typed domain - a recipe for creating Virtual Enterprises
Top/Business/Virtual_Enterprises	Virtual Breeding Environment to Support Collaboration in the Aeronautical Supplie Chain
Top/Business/Virtual_Enterprises	Virtual Organizations Breeding Environment - Concepts, expected results, intermediate results
Top/Business/Virtual_Enterprises	Virtual Organizations Management - Concepts, expected results, intermediate results
Top/Business/Virtual_Enterprises	Virtual reality for VE
Top/Business/Virtual_Enterprises	Agent technologies for VE + SW demonstrations: MAS Tutorial
Top/Business/Virtual_Enterprises	Virtual organizations management
Top/Business/Virtual_Enterprises	CNO Base Concepts
Top/Business/Virtual_Enterprises	PVC basic concepts & typologies
Top/Business/Virtual_Enterprises	VO Breeding Environments & competency management
Top/Business/Virtual_Enterprises	Searching, Finding, Buying, Selling: Story of a Long tail Online Marketplace In this talk we will present the personality of an Online Marketplace. This Online Marketplace has all the characteristics of an Offline Marketplace and more because of the simultaneous anonymity and familiarity of the buyers and sellers. The long tail nature of the transactions combined with differing capabilities, motives, and trust leads to a vibrant Social Commerce Network. We will present interesting technical challenges and opportunities in Finding, Classification, merchandizing and reputation systems.
Top/Business/Virtual_Enterprises	Knowledge technologies for network organisations This lecture presents the current research work at JSI on Knowledge technologies and potentials these technologies have to solve problems in Networked organisations. Some real prototypes and solutions has been presented as well as some visionary plans.
Top/Business/Virtual_Enterprises	Extended Products in VOs
Top/Business/Virtual_Enterprises	Performance Management in VOs
Top/Society/Gender_Issues	Gender dimension in research projects : Influence of the team structure on the project results
Top/Society/Gender_Issues	Gender issue in ICT : Dealing with educational obstacles in mathematical education for ICT
Top/Society/Gender_Issues	Gender issues in user interfaces Most areas of the computing sciences consider themselves to be either gender neutral or they aim at acknowledging physiologically-based differences between women and men. While the first standpoint draws on the traditional ideal of science as a rational, objective and value-free project, the second one does not refer to gender, but to sex differences a general tendency that is supported by recent interpretations of brain research in popular media.
Top/Society/Gender_Issues	Round table: Gender Issues
Top/Society/Gender_Issues	CEC-WYS : Central European Center for Women and Youth in Science
Top/Society/Gender_Issues	women@CL : Women in computing research and academic leadership
Top/Society/Gender_Issues	Women in Science in Slovenia, the new Member State Slovenia is a Central European country and has in a history always been between the East and West of Europe, which determined many of the state political and social activities. These reflect also in the situation of women in science.
Top/Society/Scientific_Communication	Introduction to writing
Top/Society/Scientific_Communication	Writing as a reflective experience
Top/Society/Scientific_Communication	Writing reveals skills
Top/Society/Scientific_Communication	Feeling about
Top/Society/Scientific_Communication	Communicating is the doing of Science
Top/Society/Scientific_Communication	Scientific communication is ordinary communication
Top/Society/Scientific_Communication	Career Development
Top/Society/Scientific_Communication	CiteSeerX & ChemXSeer: Lessons for Cyber-infrastructure and Web E-science or cyberinfrastructure have become crucial for scientific progress and open source systems have greatly facilitated design and implementation. CiteSeer, a search engine and digital library for academic documents in computer and information science, was one of the first cyberinfrastructure projects to show the promise of improved search and access for scientific information. For chemistry we propose the ChemXSeer (funded by NSF Chemistry) architecture, a portal for academic researchers in environmental chemistry, which integrates the scientific literature and search with experimental, analytical and simulation datasets.
Top/Mathematics/Statistics	Basics of probability and statistics
Top/Mathematics/Statistics	On the Borders of Statistics and Computer Science Machine learning in computer science and prediction and classification in statistics are essentially equivalent fields. I will try to illustrate the relation between theory and practice in this huge area by a few examples and results. In particular I will try to address an apparent puzzle: Worst case analyses, using empirical process theory, seem to suggest that even for moderate data dimension and reasonable sample sizes good prediction (supervised learning) should be very difficult. On the other hand, practice seems to indicate that even when the number of dimensions is very much higher than the number of observations, we can often do very well. We also discuss a new method of dimension estimation and some features of cross validation.
Top/Mathematics/Statistics	Information Geometry This tutorial will focus on entropy, exponential families, and information projection. We'll start by seeing the sense in which entropy is the only reasonable definition of randomness. We will then use entropy to motivate exponential families of distributions which include the ubiquitous Gaussian, Poisson, and Binomial distributions, but also very general graphical models. The task of fitting such a distribution to data is a convex optimization problem with a geometric interpretation as an 'information projection': the projection of a prior distribution onto a linear subspace (defined by the data) so as to minimize a particular information-theoretic distance measure. This projection operation, which is more familiar in other guises, is a core optimization task in machine learning and statistics. We'll study the geometry of this problem and discuss two popular iterative algorithms for it.
Top/Mathematics/Statistics	Sequential Monte Carlo methods Parts 4 and 5 of this lecture are presented in [[mlss07_davy_smcmc|//Manuel Davy's// '%title']]
Top/Mathematics/Statistics	Interpreting Covariance Functions & Classification
Top/Mathematics/Statistics	Parameter Estimation of ODE's with Regression Splines: Application to Biological Networks The construction and the estimation of quantitative models of gene regulatory networks and metabolic networks is one of the task of Systems Biology. Such models are useful because they provide tools for simulating and predicting biological systems. Various approaches have been proposed, such as graphical models , Bayesian dynamical models or Ordinary Differential Equations (ODE's) . For the latter, one can also expect to derive parameters that often have a meaningful biological sense. We focus on the estimation of a parameter theta indexing a (vector) ODE, from an observed time series (concentration profiles) which may be nonlinear (e.g. due to the use of Michaelis-Menten dynamics or mass action law). Even when the likelihood is simple (in the case of Gaussian error noise), the computation of the Maximum Likelihood Estimator remains hard because of the burden of the optimization step. Indeed, the implicit definition of the model necessitates the integration of the ODE for each evaluation of the likelihood. Moreover, the likelihood may have numerous local maxima we need to avoid, hence the exploration of the parameter space may be computer-intensive. We propose then an alternative (frequentist) estimator of theta based on a preliminary spline estimator of the solution of the ODE. We use a simple characterization of theta that enables to derive a learning algorithm avoiding the integration of the ODE, and that can split the estimation of a vector differential equation in several estimations of scalar differential equations. We illustrate this algorithm with different models used in Systems Biology and we sketch how it can be adapted to various settings encountered by the practitioner. Joint work with Chris Klaassen and Florence d'Alch-Buc.
Top/Mathematics/Statistics	Hierarchical Maximum Entropy Density Estimation We study the problem of simultaneously estimating several densities where the datasets are organized into overlapping groups, such as a hierarchy. For this problem, we propose a maximum entropy formulation, which systematically incorporates the groups and allows us to share the strength of prediction across similar datasets. We derive general performance guarantees, and show how some previous approaches, such as hierarchical shrinkage and hierarchical priors, can be derived as special cases. We demonstrate the proposed technique on synthetic data and in a realworld application to modeling the geographic distributions of species hierarchically grouped in a taxonomy. Specifically, we model the geographic distributions of species in the Australian wet tropics and Northeast New South Wales. In these regions, small numbers of samples per species significantly hinder effective prediction. Substantial benefits are obtained by combining information across taxonomic groups.
Top/Mathematics/Statistics	Estimating Parameters and Hidden Variables in a Non-linear State-space Model of Regulatory Networks Understanding and identifying biological complex systems at work in the cell requires to develop models able to capture the stochastic nature of biological processes as well as their dynamics. Focusing on gene regulatory networks, we propose a new quantitative model in the form of a dynamical Bayesian network that allows to represent both genes and proteins in the same framework. We start from the nonlinear differential equations of Michaelis-Menten which are the gold-standard to represent biochemical interactions and develop a discrete-time and probabilistic model from these equations. Compared to previous works such as Nachman et al [1], our model takes into account the dependency between the regulatory proteins and the genes that code for them as well as protein-protein interactions and protein degradations. In the resulting nonlinear dynamical system, the proteins concentrations are hidden while gene expressions are observed. In order to learn the model's parameters, we first construct a discrete-time probabilistic model corresponding to our continuous-time state-space model and then derive a Kalman smoother algorithm based on the unscented transformation [2] to recursively estimate the parameters and unobserved protein activities. The generality of the learning method opens the door to various adaptations of the model if required by the biology. Numerical results on parameter and state estimation for the repressilator [3] and other several small networks are presented and show the relevance of the model.
Top/Mathematics/Statistics	Learning Distance Function by Coding Similarity We consider the problem of learning a similarity function from a set of positive equivalence constraints, i.e. 'similar' point pairs. We define the similarity in information theoretic terms, as the gain in coding length when shifting from independent encoding of the pair to joint encoding. Under simple Gaussian assumptions, this formulation leads to a non-Mahalanobis similarity function which is effcient and simple to learn. This function can be viewed as a likelihood ratio test, and we show that the optimal similaritypreserving pro jection of the data is a variant of Fisher Linear Discriminant. We also show that under some naturally occurring sampling conditions of equivalence constraints, this function converges to a known Mahalanobis distance (RCA). The suggested similarity function exhibits superior performance over alternative Mahalanobis distances learnt from the same data. Its superiority is demonstrated in the context of image retrieval and graph based clustering, using a large number of data sets.
Top/Mathematics/Statistics	Cluster Variation Method: from statistical mechanics to message passing algorithms The cluster variation method (CVM) is a hierarchy of approximate variational techniques for discrete (Ising--like) models in equilibrium statistical mechanics, improving on the mean--field approximation and the Bethe--Peierls approximation, which can be regarded as the lowest level of the CVM. The foundations of the CVM are briefly reviewed, considering different derivations of the method and related techniques, like for instance TAP equations and the cavity method. Issues of realizability and exactness are also addressed.
Top/Mathematics/Statistics	Variational Bayes for Continuous-time Nonlinear State-space Models
Top/Mathematics/Statistics	EEG Coupling, Granger Causality and Multivariate Autoregressive Models
Top/Mathematics/Statistics	Nonparametric Tests between Distributions Reproducing Kernel Hilbert Spaces have been mainly used for estimation. Distributional tests in this area were mainly concerned with tests for independence of random variables. We give concentration of measure bounds for the latter using an easy to compute criterion between spaces of observations. In addition, we show that a similar criterion can be used easily for the purpose of testing the identity between two distributions. In both cases, we prove necessary and sufficient conditions for the tests.
Top/Mathematics/Statistics	Intrinsic bounds on the BH multiple comparison procedure
Top/Mathematics/Statistics	Gene-based bin-analysis of genome-wide association studied With the improvement of genotyping technologies and the exponentially growing number of available markers, case-control genome-wide association studies promise to be a key tool for investigation of complex diseases. However new analytical methods have to be developed to face the problems induced by this data scale-up, such as statistical multiple testing, data quality control, biological interpretation and computational tractability. We present a novel method to analyze genome-wide association studies results. The algorithm is based on a Bayesian model that integrates genotyping errors and genomic structure dependencies. Probability values are assigned to genomic regions termed bins, which are defined from a gene-biased partitioning of the genome, and the false-discovery rate is estimated. We have applied this algorithm to data coming from three genome-wide association studies of Multiple Sclerosis. The method practically overcomes the scale-up problems and permits to identify new putative regions statistically associated with the disease.
Top/Mathematics/Statistics	Regularization: Quadratic Versus Sparsity-enforcing and Deterministic Versus Stochastic Methods
Top/Mathematics/Statistics	Bounding the k-family-wise error-rate using resampling methods
Top/Mathematics/Statistics	MCMC, SMC,... What next ? The Monte Carlo method was initially developed for scientific computing in statistical physics during the early days of the computers. Due to the rapid progress in computer technology and the need for handling large datasets and complex systems, the past two decades have witnessed a strong surge of interest in Monte Carlo methods from the scientific community. Researchers ranging from computational biologist to signal & image processing engineers and to financial econometricians now view Monte Carlo techniques as essential tools for inference. Besides using the popular Markov chain Monte Carlo strategies and adaptive variants of it, various sequential Monte Carlo strategies have recently appeared on the scene, resulting in a wealth of novel and effective inferential and optimization tools. In this talk, we will present what we believe to be the 'state-of-the art' in Monte-Carlo simulations for inference and will try to identify the next challenges.
Top/Mathematics/Statistics	Determining significance in neuroimaging studies using covariate-modulated false discovery rate
Top/Mathematics/Graph_Theory	Strings, graphs, invariants Strings play an important role in various sciences, from computer science, linguistics, social sciences, to various natural sciences, including bioinformatics. Strings, words, or finite sequences are mainly studied in formal language theory and form the basis of logic, mathematics and theoretical computer science. Although strings have a simple linear structure, we may associate a number of invariants to them in particular, via various graphs. This non-technical talk will explain some of these features and survey some of the recent work of the present authors. 123
Top/Mathematics/Graph_Theory	Prediction on a graph We will discuss the problem of robust online learning over a graph. Consider the following game for predicting the labeling of a graph. Nature presents a vertex v1; the learner predicts the label of the vertex y1; nature presents a label y1; nature presents a vertex v2; the learner predicts y2; and so forth. The learners goal is minimize the total number of mistakes. If nature is adversarial, the learner will always mispredict; but if nature is regular or simple, there is hope that a learner may make only a few mispredictions. Thus, a methodological goal is to give learners whose total mispredictions can be bounded relative to the complexity of natures labeling. In this talk, we consider the label cut size as a measure of the complexity of a graphs labeling, where the size of the cut is the number of edges between disagreeing labels. We will give bounds which depend on the cut size and the (resistance) diameter of the graph.
Top/Mathematics/Graph_Theory	Semidefinite ranking on graphs We consider the problem of ranking the vertices of an undirected graph given some preference relation. This ranking on graphs problem has been tackled before using spectral relaxations in [1]. Their approach is strongly related to the spectral relaxation made in spectral clustering algorithms. One problem with spectral relaxations that has been found in clustering is that even on simple toy graphs the spectral solution can be arbitrarily far from the optimal one [2]. It has recently been shown that semidefinite relaxations offer in many cases better solutions than spectral ones for clustering [3] and transductive classification [4]. We therefore investigate semidefinite relaxations of ranking on graphs.
Top/Mathematics/Graph_Theory	Graphs with extremal energy tend to have a small number of distinct eigenvalues The sum of the absolute values of the eigenvalues of a graph is called the energy of the graph. We study the problem of finding graphs with extremal energy within specified sets of graphs. We develop some tools for treating such problems and obtain some partial results. In particular, we show that in many cases the expected extremal graphs with a small number of distinct eigenvalues do not exist and that actual extremal graphs could have a large number of distinct eigenvalues. Zigzag and central circuit structure
Top/Mathematics/Graph_Theory	Independence ratios of nearly planar graphs With purposeful imprecision, a graph is said to be nearly planar if it resembles in some essential way a planar graph. Classic measures of near planarity include the thickness, the crossing number, and the genus. Modern versions of nearly planar graphs include the graphs embedded on surfaces with large width (shortest noncontractible cycle). Geometric graph theory has given us a different notion of locally planar as well as two definitions of quasi-planar graphs. This talk will begin with a retrospective look at independence ratios of locally planar graphs. We consider contrasts between results about the independence ratio and those about the chromatic number. We will then proceed to results and open questions about independence ratios of various classes of nearly planar graphs.
Top/Mathematics/Graph_Theory	Infinite planar tessellations We survey problems involving planar embeddings of locally finite, infinite graphs, especially graphs that have only one infinite component when any finite subgraphs is deleted. Problems considered concern separating double rays, geodetic double rays, facial walks, rate of growth, and vertex-, edge- and face-homogeneity.
Top/Mathematics/Graph_Theory	Geometric intersection graphs Geometric intersection graphs are intensively studied both for their practical motivations and interesting theoretical properties. Many classes allow elegant characterizations, for many of them optimization problems NP-hard in general can be solved in polynomial time. We will present a survey of recent results and old problems in this area, including questions related to colorability, maximum clique or representations of planar and co-planar graphs. Computational complexity of recognition of many of intersection defined classes of graphs will be one of the main topics.
Top/Mathematics/Graph_Theory	Honeycomb tori and Cayley graphs on generalized dihedral groups We investigate a family of graphs known to some people as honeycomb tori. We establish that they all are Cayley graphs on generalized dihedral groups. We then look at hamiltonicity properties for this family of graphs.
Top/Mathematics/Graph_Theory	Representations of graphs A graph is a mathematical structure that is sometimes hard to separate from its visualization. An important branch of graph theory studies graph drawing problems. Recently a mathematical approach to graph visualization has been developed under the name of 'graph representations'. In this tutorial we present an outline of the theory of graph representations.
Top/Mathematics/Graph_Theory	On Social networks with an overview of graph drawing with demo of a system Pajek Network = Graph + Data. The data can be measured or computed/derived from the network. The graph drawing is already well established field with its own conference http://www.gd2005.org/ (started in 1992). In traditional graph drawing the goal is to produce the best layout of given graph. SNA (Social Network Analysis) is a part of data analysis. Its goal is to get insight into the structure and characteristics of given network.
Top/Mathematics/Graph_Theory	Geometry of partial cube graphs Partial cubes are graphs defined by a geometric structure: the graph vertices can be placed on the vertices of a hypercube in such a way that graph distance equals Hamming distance. We survey recent developments in the theory of these graphs that relate them in other ways to geometric structures: lattice embeddings, hyperplane arrangements, systems of translated quadrants in the plane, and flip graphs of triangulations.
Top/Mathematics/Graph_Theory	On Hurwitz theory: enumerating branched surface coverings With a chronological review of Hurwitz theory, we survey some known results on the enumeration of the equivalence classes of several types of branched coverings of a surface. In particular, relations with the enumeration of the equivalence classes of several types of graph coverings and enumerating the isomorphism classes of branched orientable surface coverings of a nonorientable surface will be mentioned. Also we discuss a similar problem for branched coverings having prescribed branched types.
Top/Mathematics/Graph_Theory	Distance-regular graphs and the quantum affine algebra Uq(bsl2) Combinatorial objects, such as graphs, can often be used to construct representations of abstract algebras. In this talk we will consider a graph possessing a high degree of regularity, known as a distance-regularity. For this graph we define an algebra generated by the adjacency matrix and a certain diagonal matrix. There exists a set of elements in this algebra that, under a minor assumption, satisfy some attractive relations. Using these relations we obtain a representation of the quantum affine sl2 algebra.
Top/Mathematics/Graph_Theory	Convergence of the graph Laplacian application to dimensionality estimation and image segmentation Given a sample from a probability measure with support on a submanifold in Euclidean space one can construct a neighborhood graph which can be seen as an approximation of the submanifold. The graph Laplacian of such a graph is used in several machine learning methods like semi-supervised learning, dimensionality reduction and clustering. We will present the pointwise limit of three different graph Laplacians used in the literature as the sample size increases and the neighborhood size approaches zero. We show that for a uniform measure on the submanifold all graph Laplacians have the same limit up to constants. However in the case of a nonuniform measure on the submanifold only the so called random walk graph Laplacian converges to the weighted Laplace-Beltrami operator. We will give two applications of these theoretical results.
Top/Mathematics/Graph_Theory	Probabilistic graph partitioning We consider the problem of Graph Partitioning for applications in Web Mining and Collaborative Filtering. Our approach is based on predicting the presence/absence of a directed link based on a form of probabilistic mixture model. Being based on a generative model of directed graphs, we are able to apply an approximate Bayesian treatment to automatically select an appropriate number of partitions. We will discuss an application in Collaborative Filtering and comment on relations to mixed membership models, Latent Dirichlet Allocation and Probabilistic Latent Semantic Analysis.
Top/Mathematics/Graph_Theory	Introduction, Basic Notions in Graph Theory At the beginning examples and applications of configurations are shown. Basic definitions in graph theory follow.
Top/Mathematics/Graph_Theory	Isomorphism, Matrices and Graph Invariants; Subgraphs and Connectivity in Graphs
Top/Mathematics/Graph_Theory	Basic and Advanced Operations on Graphs
Top/Mathematics/Graph_Theory	Graph Colorings and Matchings
Top/Mathematics/Graph_Theory	Group Actions and Cayley Graphs
Top/Mathematics/Graph_Theory	Graph Matching Algorithms Graph matching plays a key role in many areas of computing from computer vision to networks where there is a need to determine correspondences between the components (vertices and edges) of two attributed structures. In recent years three new approaches to graph matching have emerged as replacements to more traditional heuristic methods. These new methods are: * Least squares - where the optimal correspondence in determined in terms of deriving the best fitting permutation matrix between sets. * Spectral methods - where optimal correspondences are derived via subspace projections in the graph eigenspaces. * Graphical models - where algorithms such as the junction tree algorithm are used to infer the optimal labeling of the nodes of one graph in terms of the other and that satisfy similarity constraints between vertices and edges. In this lecture we review and compare these methods and demonstrate examples where this applies to point set and line matching.
Top/Mathematics/Graph_Theory	Bipartite Graph Matching for Computing the Edit Distance of Graphs In the field of structural pattern recognition graphs constitute a very common and powerful way of representing patterns. In contrast to string representations, graphs allow us to describe relational information in the patterns under consideration. One of the main drawbacks of graph representations is that the computation of standard graph similarity measures is exponential in the number of involved nodes. Hence, such computations are feasible for rather small graphs only. One of the most flexible error-tolerant graph similarity measures is based on graph edit distance. In this paper we propose an approach for the efficient compuation of edit distance based on bipartite graph matching by means of Munkres algorithm, sometimes referred to as the Hungarian algorithm. Our proposed algorithm runs in polynomial time, but provides only suboptimal edit distance results. The reason for its suboptimality is that implied edge operations are not considered during the process of finding the optimal node assignment. In experiments on semi-artificial and real data we demonstrate the speedup of our proposed method over a traditional tree search based algorithm for graph edit distance computation. Also we show that classification accuracy remains nearly unaffected.
Top/Mathematics/Graph_Theory	Graph Fibrations, graph isomorphism and PageRank
Top/Mathematics/Graph_Theory	A Quadratic Programming Approach to the Graph Edit Distance Problem In this paper we propose a quadratic programming approach to computing the edit distance of graphs. Whereas the standard edit distance is defined with respect to a minimum-cost edit path between graphs, we introduce the notion of fuzzy edit paths between graphs and provide a quadratic programming formulation for the minimization of fuzzy edit costs. Experiments on real-world graph data demonstrate that our proposed method is able to outperform the standard edit distance method in terms of recognition accuracy on two out of three data sets.
Top/Mathematics/Graph_Theory	Graph complexity for structure and learning The talk will consider ways of bounding the complexity of a graph as measured by the number of partitions satisfying certain properties. The approach adopted uses Vapnik Chervonenkis dimension techniques. An example of such a bound was given by Kleinberg et al (2004) with an application to network failure detection. We describe a new bound in the same vein that depends on the eigenvalues of the graph Laplacian. We show an application of the result to transductive learning of a graph labelling from examples.
Top/Mathematics/Graph_Theory	Link analysis with pajek Pajek is a program (for Windows) for large network analysis and visualization. It is freely available for noncommercial use at [[http://vlado.fmf.uni-lj.si/pub/networks/pajek/|http://vlado.fmf.uni-lj.si/pub/networks/pajek/]] Besides ordinary networks Pajek supports also multi-relational and temporal networks. In large network analysis we are often interested in important parts of given network. There are several ways how to determine them. The islands approach is based on an importance measure of vertices or lines. Let (V,L,p) be a network with vertex property p : V ? R and let t be a real number. If we delete all vertices (and corresponding links) with the property value less than t, we get subnetwork called vertex-cut at level t. The number and sizes of its components depend on t. Often we consider only components of size at least k and not exceeding K. The components of size smaller than k are discarded as noninteresting, while the components of size larger than K are cut again at some higher level. Vertex-island is a connected subnetwork which vertices have greater property value than the vertices in its neighborhood. It is easy to see that the components of vertex-cuts are all vertex-islands. We developed an efficient algorithm that identifies all maximal vertex-islands of sizes in the interval k..K in a given network. For networks with weighted lines we can similarly define line-islands. The line-islands algorithm is based on line-cuts. Both algorithms are very general - they can be applied for any vertex/line importance measure. Their complexity is for sparse networks subquadratic - they can be applied to very large networks. We will illustrate them applying different importance measures on selected (large) networks. We will also present the use of pattern searching in analysis of genealogies and some approaches to analysis of (multi-relational) temporal networks.
Top/Mathematics/Graph_Theory	Graph-based Methods for Retinal Mosaicing and Vascular Characterization In this paper, we propose a highly robust point-matching method (Graph Transformation Matching - GTM) relying on finding the consensus graph emerging from putative matches. Such method is a two- phased one in the sense that after finding the consensus graph it tries to complete it as much as possible. We successfully apply GTM to image registration in the context of finding mosaics from retinal images. Feature points are obtained after properly segmenting such images. In addition, we also introduce a novel topological descriptor for quantifying disease by characterizing the arterial/venular trees. Such descriptor relies on diffusion kernels on graphs. Our experiments have showed only statistical signifficance for the case of arterial trees, which is consistent with previous findings.
Top/Mathematics/Graph_Theory	Mining Large Graphs: Laws and Tools
Top/Mathematics/Graph_Theory	On automorphism groups of vertex-transitive graphs One of the most fundamental questions one can ask about a vertex-transitive graph is what is the full automorphism group Aut() of ? This is usually a difficult question, as is evidenced by determining Aut() should allow for the solution of many other problems concerning . For example, is edge-transitive, 1/2-transitive, or normal? Additionally, one should be able to solve the isomorphism problem for . In this talk, we will report on recent progress in determining Aut(), for in certain classes of vertex-transitive graphs, especially Cayley graph of certain abelian groups.
Top/Mathematics/Graph_Theory	Chinese Rings and Hanoi Tower Graphs
Top/Mathematics/Graph_Theory	Frequent graph mining - what is the question? The objective of data mining is to find regularities, or interesting patterns in large data sets, such as business transactions. More recently, there has been great interest in extending this work to structured data, such as graphs. The domain could be a database of molecular graphs, or the web graph, and the question could be to find subgraphs which occur frequently in the data. Algorithms usually list frequent subgraphs or other patterns. There are many different formulations of this problem. At this stage of the development of the field, it appears to be of some interest to put together a general picture of the different variants. In this talk we present an attempt towards this direction.
Top/Mathematics/Graph_Theory	Generating Graphs with Predefined k-Core Structure The modeling of realistic networks is of great importance for complex systemsresearch. Previous procedures typically model the natural growth of networks byiteratively adding nodes, use geometric positioning information, define linkconnectivity with preference for nearest neighbors or already highly connectednodes, or combine several of these approaches. Our novel model is based on the well-know concept of 'k'-cores, originally introduced in social network analysis. Recent studies exposed the significant 'k'-core structure of several real world systems, e.g. the AS network of theInternet. We present two algorithms for generating networks which strictlyadhere to the sizes of a given 'k'-core structure but also exhibit adaptationto various use cases. We showcase this in a comparative evaluation with twowell-known AS network generators.
Top/Mathematics/Graph_Theory	Graph Clustering With Network Structure Indices Graph clustering has become ubiquitous in the study of relational data sets. We examine two simple algorithms: a new graphical adaptation of the k -medoids algorithm and the Girvan-Newman method based on edge betweenness centrality. We show that they can be effective at discovering the latent groups or communities that are defined by the link structure of a graph. However, both approaches rely on prohibitively expensive computations, given the size of modern relational data sets. Network structure indices (NSIs) are a proven technique for indexing network structure and efficiently finding short paths. We show how incorporating NSIs into these graph clustering algorithms can overcome these complexity limitations. We also present promising quantitative and qualitative evaluations of the modified algorithms on synthetic and real data sets.
Top/Mathematics/Graph_Theory	Graph methods and geometry of data In recent years graph-based methods have seen success in different machine learning applications, including clustering, dimensionality reduction and semi-supervised learning. In these methods a graph is associated to a data set, after which certain aspects of the graph are used for various machine learning tasks. It is, however, important to observe that such graphs are empirical objects corresponding to a randomly chosen set of data points. In my talk I will discuss some of our work on using spectral graph methods for dimensionality reduction and semi-supervised learning and certain theoretical aspects of these methods, in particular, when data is sampled from a low-dimensional manifold.
Top/Mathematics/Graph_Theory	'Lies, Damn Lies, and Statistics': A Critical Assessment of Preferential Attachment-type Network Models of the Internet Basic Question: Do the available Internet-related connectivity measurements and their analysis support the sort of claims that can be found in the existing complex networks literature? Key Issues: What about data hygiene? What about statistical rigor? What about model validation? Author discusses some of the main problems and challenges associated with measuring, inferring, and modeling various types of Internet-related connectivity structures. To this end, he uses some known examples to illustrate the need to understand the process by which Internet connectivity measurements are obtained, explore the sensitivity of inferred graph properties to known ambiguities in the data, be more critical with respect to the dominant, preferential attachmenttype network modeling paradigm, and be more serious/ambitious when it comes to model validation. Ignoring any of these issues is bound to produce results that are best described by the well-known aphorism 'lies, damned lies, and statistics.'
Top/Mathematics/Graph_Theory	Self-mapping Networks
Top/Mathematics/Graph_Theory	Multiscale analysis on graphs Analysis on graphs has recently been shown to lead to powerful algorithms in learning, in particular for regression, classification andclustering. Eigenfunctions of the Laplacian on a graph are a natural basis for analyzing functions on a graph, as we have seen in presentations of recent work by partecipants to this conference. In this talk we introduce a new flexible set of basis functions, called Diffusion Wavelets, that allow for a multiscale analysis of functions on a graph, very much in the same way classical wavelets perform a multiscale analysis in Euclidean spaces. They allow efficient, representation, compression, denoising of functions on the graph, and are very well-suited for learning, as well as unsupervised algorithms. They are also associated with a multiscale decomposition of the graph, which has applications by itself. We will discuss this construction with several examples, going from signal processing on manifolds and graphs, to some recent preliminary applications to clustering and learning.
Top/Law/Copyrights	How I came across Creative Commons
Top/Law/Copyrights	Sharing the Creative Commons
Top/Law/Copyrights	The Brazilian Experience in the Creative Commons
Top/Law/Copyrights	Presentation of the book Free Culture by Lawrence Lessig About Free Culture Lawrence Lessig could be called a cultural environmentalist. One of Americas most original and influential public intellectuals, his focus is the social dimension of creativity: how creative work builds on the past and how society encourages or inhibits that building with laws and technologies. In his two previous books, CODE and THE FUTURE OF IDEAS, Lessig concentrated on the destruction of much of the original promise of the Internet. Now, in FREE CULTURE, he widens his focus to consider the diminishment of the larger public domain of ideas. In this powerful wake-up call he shows how short-sighted interests blind to the long-term damage theyre inflicting are poisoning the ecosystem that fosters innovation. All creative worksbooks, movies, records, software, and so onare a compromise between what can be imagined and what is possibletechnologically and legally. For more than two hundred years, laws in America have sought a balance between rewarding creativity and allowing the borrowing from which new creativity springs. The original term of copyright set by the Constitution in 1787 was seventeen years. Now it is closer to two hundred. Thomas Jefferson considered protecting the public against overly long monopolies on creative works an essential government role. What did he know that weve forgotten? Lawrence Lessig shows us that while new technologies always lead to new laws, never before have the big cultural monopolists used the fear created by new technologies, specifically the Internet, to shrink the public domain of ideas, even as the same corporations use the same technologies to control more and more what we can and cant do with culture. As more and more culture becomes digitized, more and more becomes controllable, even as laws are being toughened at the behest of the big media groups. Whats at stake is our freedomfreedom to create, freedom to build, and ultimately, freedom to imagine. ;LINKS AND DOWNLOADS: : [[http://www.free-culture.cc/freecontent/|Free Culture / Free Content]] : [[http://www.lessig.org|Lessig.org]] : [[http://www.free-culture.cc/freeculture.pdf|Free Culture - pdf book==]] //FREE CULTURE is available for free under a Creative Commons license. You may redistribute, copy, or otherwise reuse/remix this book provided that you do so for non-commercial purposes and credit Professor Lessig.//
Top/Law/Copyrights	Digital Technology and Legal Challenges to Copyright
Top/Law/Copyrights	Creative Commons License
Top/Law/Copyrights	Creative Commons Italy
Top/Law/Copyrights	Digital Rights Management in the educational sector
Top/Law/Copyrights	Rights Management and Educational Repositories
Top/Law/Copyrights	Authors rights
Top/Law/Copyrights	An empirical case
Top/Law/Copyrights	Copyright laws in todays digital world
Top/Physics	Laser Manipulation of Atoms and Spins Two of the most important themes of modern magnetism are the making of magnetic structures and the switching of magnetization on ultra fast time scale. With the help of the interaction between photons and electrical states in atoms we can achieve both. Nanofabrication with the help of atomic optics can provide a tool for the production of magnetic nanostructures, where femtoseconds laser shots can provide coherent control of magnetization on ultra light picoseconds timescale.
Top/Physics	CERN's 27km Big Bang machine Presentation of the [[http://lhc.web.cern.ch/lhc/|Large Hadron Collider]] and of the [[http://atlasexperiment.org/|Atlas Experiment]], happening under your feet here in Geneva.
Top/Physics	NASA's 'Beyond Einstein' Program: Exploration at the Limits of Space & Time Albert Einstein's General Theory of Relativity predicted results that were so incredible that even he did not accept them: space is expanding from a Big Bang, space itself contains an energy that is pulling the Universe apart from within, and deep chasms of gravity called black holes actually exist. Astonishingly, all of these wild ideas are now known to be true. But now we need to build on Einstein's work to take the next step -- to study the underlying physics of the very phenomena that came out of his theories. NASA's Beyond Einstein program consists of a series of space missions, large and small, that push Einstein's theories to their limits, using increasingly more sensitive probes. The two flagship missions now in development, Constellation-X and LISA, will explore extremes of space, measuring X-rays and gravitational waves. The smaller missions, the Einstein probes, will target specific science questions such as 'What is Dark Energy?' and 'What powered the Big Bang?'
Top/Physics	Is it SUSY? -first steps after an LHC discovery A missing energy discovery is possible at the LHC in the first year of running. The origin of such a signal could be any of a huge number of models of supersymmetry, or non-supersymmetric models with extra dimensions or 'little Higgs'. Recently we have developed a realistic strategy to rapidly narrow the list of candidate theories at, or close to, the moment of discovery. The strategy is based on robust ratios of inclusive counts of simple physics objects. We studied specific cases showing discrimination of look- alike models in simulated data sets that are at least 10 to 100 times smaller than used in previous studies. We discriminate supersymmetry models from non-supersymmetric look-alikes with only 100 pb-1 of simulated data, using combinations of observables that trace back to differences in spin.
Top/Physics	From spinwaves to Giant Magnetoresistance and beyond Peter Grnberg is joint winner of the 2007 Nobel Prize in Physics. The talk is be based on his Nobel lecture in 2007.
Top/Physics	PS10: First attempt to commercial solar energy in the world The field of solar energy research has reached a level of maturity where truly commercial applications can be envisaged. The fascinating story of one of the first prototypes will be presented.
Top/Physics	Prospect of Particle Physics in China The Beijing Electron Positron Collider (BEPC) finished its running July 2005, with great success in both the Tau-Charm physics experiment and the synchrotron radiation light source. The latest Charm physics results from BEPC are reviewed, including the observation of the new resonance of X1835 with a possible explanation of the PPbar bound state. The major upgrade of BEPC into a double ring collider, so called BEPCII, will increase its luminosity by two orders of magnitude. The physics window of BEPCII is mainly the precision measurements in the Charm physics and the search for new phenomena. The construction of BEPCII is finished. The tuning of the storage ring goes smoothly. The synchrotron radiation facility of BEPCII opened to users with high performance since the end of 2006. The new detector BESIII has been moved into the interaction region June, and the joint commissioning started. The non-accelerator experiments in China are promoted with great efforts, including the neutrino physics experiments, the cosmic ray measurements, and the particle astrophysics experiments in Space. The reactor neutrino experiment at Daya Bay could reach the sensitivity of 0.01 on the measurement of the neutrino mixing parameter sin22 13. The medium and long term plan of the Chinese particle physics experiments is also discussed.
Top/Physics	X-rays from comets - a surprising discovery Comets are kilometre-size aggregates of ice and dust, which remained from the formation of the solar system. It was not obvious to expect X-ray emission from such objects. Nevertheless, when comet Hyakutake (C/1996 B2) was observed with the ROSAT X-ray satellite during its close approach to Earth in March 1996, bright X-ray emission from this comet was discovered. This finding triggered a search in archival ROSAT data for comets, which might have accidentally crossed the field of view during observations of unrelated targets. To increase the surprise even more, X-ray emission was detected from four additional comets, which were optically 300 to 30 000 times fainter than Hyakutake. For one of them, comet Arai (C/1991 A2), X-ray emission was even found in data which were taken six weeks before the comet was optically discovered. These findings showed that comets represent a new class of celestial X-ray sources. The subsequent detection of X-ray emission from several other comets in dedicated observations confirmed this conclusion. The talk will summarise the highlights of the series of discoveries and provide an explanation for this unexpected phenomenon.
Top/Physics	The LHC is safe
Top/Physics	Accelerators for Hadrontherapy Hadrontherapy was born in 1938, when neutron beams were used in cancer therapy, but it has become an accepted therapeutical modality only in the last fifteen years. Fast neutrons are still in use, even if their limitations are now apparent. Charged hadron beams are more favourable, since the largest specific energy deposition occurs at the end of their range in matter. The most used hadrons are at present protons and carbon ions, which allow a dose deposition which conforms to the tumour target. Radiobiological experiments and the results of the first clinical trials indicate that carbon ions have, besides this macroscopic property, a different way of interacting with cell at the microscopic level. There are thus solid hopes to use carbon beams of about 4500 MeV to control tumours which are radioresistant both to X-rays and to protons. After discussing these macroscopic and microscopic properties and presenting the work carried out at CERN in the framework of the Proton Ion Medical Machine Study (PIMMS), the hospital-based facilities in the world, running or under construction, will be reviewed.
Top/Physics	Quantum gravity in three dimensions
Top/Physics	String theory and cosmology
Top/Physics	LHC machine status
Top/Physics	Status of the LHC detectors: design, construction, commissioning
Top/Physics	An overview of the United States government's space and science policy-making process A brief overview of the basic elements of the US space and science policy-making apparatus will be presented, focussing on insights into the interactions among the principal organizations, policy-making bodies and individual participants and their respective impact on policy outcomes. Several specific examples will be provided to illustrate the points made, and in the conclusion there will be some observations on current events in the US that may shape the outcome for the near-term future of US space and science policy in several areas.
Top/Physics	ITER: Promises unkept? (1/2)
Top/Physics	Fundamental Constants in Physics and their Time Dependence In the Standard Model of Particle Physics we are dealing with 28 fundamental constants. In the experiments these constants can be measured, but theoretically they are not understood. I will discuss these constants, which are mostly mass parameters. Astrophysical measurements indicate that the finestructure constant is not a real constant, but depends on time. Grand unification then implies also a time variation of the QCD scale. Thus the masses of the atomic nuclei and the magnetic moments of the nuclei will depend on time. I proposed an experiment, which is currently done by Prof. Haensch in Munich and his group. The first results indicate a time dependence of the QCD scale. I will discuss the theoretical implications.
Top/Physics	A New Method for Non-linear and Non-stationary Time Series Analysis: The Hilbert Spectral Analysis A new method for analysing non-linear and non-stationary data has been developed. The key part of the method is the Empirical Mode Decomposition method with which any complicated data set can be decomposed into a finite and often small number of Intrinsic Mode Functions (IMF). An IMF is defined as any function having the same numbers of zero crossing and extreme, and also having symmetric envelopes defined by the local maximal and minima respectively. The IMF also admits well-behaved Hilbert transform. This decomposition method is adaptive, and, therefore, highly efficient. Since the decomposition is based on the local characteristic time scale of the data, it is applicable to non-linear and non-stationary processes. With the Hilbert transform, the Intrinsic Mode Functions yield instantaneous frequencies as functions of time that give sharp identifications of imbedded structures. The final presentation of the results is an energy-frequency-time distribution, designated as the Hilbert Spectrum. Classical non-linear system models are used to illustrate the roles played by the non-linear and non-stationary effects in the energy-frequency-time distribution. Examples including Duffy equation, Rossler Equation, and non-linear wind wave data will be discussed to show the new Hilbert view of non-linear and non-stationary systems.
Top/Physics	A Passion for Discovery The human side of doing theoretical physics is explored through stories about the interactions between physicists and about the effects world events can have on scientists' behavior, and even on their interests and style. These stories cluster nicely around certain bigger themes to create an overarching whole. This happens both on account of some interesting narrative structures intrinsic to the science of physics itself and on account of the way physics integrates into the general culture. The stories concern Einstein, Schrdinger, Pauli, Heisenberg, Stueckelberg, Jordan and Fock and also involve some mathematicians like Emmy Noether, Teichmller and Bers and even the psychologist C.G. Jung.
Top/Physics	The Historical Origins and Economic Logic of 'Open Science' Modern 'big science' projects, such as the LHC experiments in physics that are being prepared to run at CERN, embody the distinctive ethos of cooperation and mechanisms of coordination among distributed groups of researchers that are characteristic of 'open science'. Much has been written about the institutions of open science, their supporting social norms, and their effectiveness in generating additions to the stock of reliable knowledge. But from where have these institutions and their supporting ethos come? How robust can we assume them to be in the face of the recent trends for universities and research institutes in some domains of science to seek to appropriate the benefits of new discoveries and inventions by asserting intellectual property claims? A search for the historical origins of the institutions of open science throws some new light on these issues, and the answers may offer some lessons for contemporary science and technology policy-making.
Top/Physics	Challenge in Astrophysics The GRavitational lEnsing Accuracy Testing 2008 (GREAT08) Challenge focuses on a problem that is of crucial importance for future observations in cosmology. The shapes of distant galaxies can be used to determine the properties of dark energy and the nature of gravity, because light from those galaxies is bent by gravity from the intervening dark matter. The observed galaxy images appear distorted, although only slightly, and their shapes must be precisely disentangled from the effects of pixelisation, convolution and noise. The worldwide gravitational lensing community has made significant progress in techniques to measure these distortions via the Shear TEsting Program (STEP). Via STEP, we have run challenges within our own community, and come to recognise that this particular image analysis problem is ideally matched to experts in statistical inference, inverse problems and computational learning. Thus, in order to continue the progress seen in recent years, we are seeking an infusion of new ideas from these communities. This document details the GREAT08 Challenge for potential participants. Please visit [[http://www.great08challenge.info/|www.great08challenge.info]] for the latest information.
Top/Chemistry	Poliamids: BASF chains of added value on the chemical products market
Top/Chemistry	Topology, structure and defects in carbon nanosystems In this talk we will explore how in the last twenty years carbon science has made the fundamental step from the flat world of graphite into the three dimensional world of fullerenes and nanotubes, the building blocks of the carbon nanotechnology revolution. We will look at the history of the discovery of buckminsterfullerene and carbon nanotubes, and explore analogies in diverse fields of biology, architecture and sport. The understanding of defects in nanocarbons is essential in order to control their diverse properties. For example irradiating bundles of carbon nanotubes produces defects which increase their bending strength by a factor of 16. At the same time such defects can store energy and were the cause of the UK Windscale nuclear fire in the 1950s. Recent advances in computational modelling and electron microscopy mean that we now have a much better understanding of the structure, formation and evolution of intrinsic defects, opening up the intriguing possibility of selective spatial creation of defects atomic level defect engineering.
Top/Chemistry	Novel functional magnetic materials based on magneto-structural transitions V zadnjih letih so se v svetu intenzivirale raziskave na podroju materialov z magnetokalorinimi lastnostmi. Ena od pomembnih aplikacij takih materialov je izdelava ekoloko neoporenih magnetnih hladilnih sistemov. V predavanju bodo predstavljeni materiali, ki kaejo gigantski magnetokalorien efekt in magnetni oblikovni spomin. V razpravi bo predstavljena teorija do katere mere narava magneto-strukturne sklopitve vpliva na latentno toploto in spremembo magnetne entropije. Predstavljene bodo metode kako dosei maksimalen magnetokalorien efekt, metode izdelave teh materialov in njihove karakterizacije.
Top/Chemistry	Surface plasmon resonance imagery and single molecule approaches of proteo-nucleic complex self-assembly The presentation will illustrate the use of use surface plasmon resonance imagery and of single molecule approaches based on quantum dot labelling for the analysis of the dynamic and structure of proteo-nucleic assemblies. Semi-synthetic complexes (PDAs), encompassing a protein part embedding electro active redox cofactors and a nucleic acid part allowing highly controlled self-organisation of the protein domains, were designed and constructed. This approach allows the building of large and fully defined chain of electron transport centres with tuneable spatial organization. Association with quantum dot offering single molecule monitoring power opens the route to the use of self-assembled bio molecules as molecular wire for bioelectronics.
Top/Chemistry	Spectroscopy This week we are hosting Stan Terras, the representative of the Ranishw Company from the UK. He is an expert on the field of spectroscopy. He will introduce the latest results with the Raman spectrometer and with the AFM/SEM that are used in nanotechnology.
Top/Chemistry	Liquid Crystal Elastomers Rubber elasticity is a unique feature of polymer networks or elastomers formed from long polymer chains connected to one another by crosslinkages. An implanting of rod-like mesogenic monomer units into the network chains can induce the liquid crystalline state of the elastomers. The interplay between network chain conformation and liquid crystalline phase creates material with remarkable properties. Detailed investigations proved a direct coupling between chain conformation and anisotropic state of order of the liquid crystalline phase. Introducing an anisotropic chain conformation a priori by a suitable synthesis concept, networks can be obtained that are macroscopic uniformly ordered (liquid single crystal elastomers, LSCE). Depending on the liquid crystalline phase structure, LSCE exhibit exceptional properties, e.g. nematic LSCE change their macroscopic dimensions when the state of order is modified by external stimuli. This effect can be applied to novel thermo- and opto-mechanical actuators.
Top/Chemistry	Self-assembly of Nematic colloids Nematic colloids are dispersions of micrometric particles in nematic liquid crystals which show unusual properties of self-assembly into regular geometrical patterns. The reason for this type of behavior is the pressure of the liquid crystal among the particles. These pressures are called structures, and are the consequence of the orientation of the liquid crystals and the deformation of the structure of the crystal near the colloid particles. It has been known for quite a while that the nematic colloids are unifying into one dimensional chain. We also know about two dimensional structures on the border of liquid crystals and isotropic liquids. Latest research has shown progress in the understanding of the nature of self-assembly of nematic colloids; they have shown that nematic colloids can spontaneously make also regular two dimensional structures which are very tightly bounded. With these facts a new path has been laid for the self-assembly of colloids in 3 dimensional crystals. There are also interesting possibilities in the manufacturing of materials with the self arranging of nanoparticles.
Top/Chemistry	In-situ Fabrication, Manipulation and Property Measurements of Single Nanotubes and Nanowires with Near Atomic Resolution Carbon nanotube (CNT) and nanowire materials are important building
Top/Chemistry	How is protein synthesis catalyzed? The Cassiopeia synchrotron stations for protein crystallography at MAX-II.
Top/Chemistry	The simulation of structures in modern materials with the theory of density functional calculations With the theory of density functional calculations we can study the connection between the crystal structure, electronic structure and the properties connected with these. With the comparison of complete energies of different structures we can establish the relative stability of individual structures. With the minimizing of inter-atomic powers we can optimize the position of atoms (ions) in the crystal. With this method we can accurately calculate also the phonic structure in the super cell which contains 60-100 atoms. As an example of use of TGF I will introduce the study of Y2Nb2O7, which can crystallize in the pyrochlore structure and at the same time works as an isolator.
Top/Chemistry	Monodispersed particles in technologies and medicine In the last decades we witnessed the development of different techniques for the manufacturing of monodispersed systems with particle size from few micrometers to few nanometers. These particles are of different sizes and forms, their chemical structure can be complex or simple. At the beginning the focus of research of these systems was in the search of different physical and chemical properties, which can be strongly dependent from the size of the particles and their morphology. Later on the interest was pointed towards their use in the making of materials with specific and repeated properties. This use is specially accelerated by todays trends in miniaturizing in technology and medicine. They enable a large progress in the transmission of laboratory techniques of preparative arrangements in monodispersed colloid systems in the economy. The lecturer will focus on the role of monodispersive colloid systems in the manufacturing of cheramics and pigments, photography, chemical polishing, and especially in the making of monodispersive medicines. He will show how colloid particles are useful in the transmission of medicines with delayed effect on the in advance chosen part of the human body. They are also used in diagnostics, especially in x-ray examinations. Their use is different because of the differences in the properties of nano and micro dispersions.
Top/Chemistry	Speciation analysis using ICPMS
Top/Medicine	Functional 4D visualisation of proteoliosys: the influence of the micro environment
Top/Medicine	Is Semantic Web technology ready for Healthcare?
Top/Medicine	Interactions between antibodies and receptors of imune responses: from basic science to medicine use
Top/Medicine	Sculpting Implants in situ: Light-Adjustable Intraocular Lens Ko se nae oesne lee z leti starajo, se pogosto razvije oesna mrena. Vsako leto se opravi preko 14 milijonov operacij, kjer se oesna mrena odstrani in se vsadijo umetne lee. S trenutno uveljavljenimi leami e vedno tretjina pacientov z oesno mreno po posegu potrebuje oala za optimalen vid. Da bi dosegli eljeni cilj brez dodatnih korekcij, bi morale biti lee sposobne prilagoditve po konani terapiji. Novi materiali, ki so jih razvili na Caltech intitutu, omogoajo post-operativne neinvazivne korekcije le. V predavanju bo profesor Julia Kornfield predstavila skupne raziskave s prof. Bob Grubbsom iz Odseka za kemijo in oesnim kirurgom prof. Dan Schwartzem iz UCSF. Skupaj so razvili material, ki omogoa in vivo prilagoditve le. Svetlobno-prilagodljive lee so se izkazale za uspene v klininih testih saj so rezultati ponovljivi in napovedljivi.
Top/Medicine	Expression Profiling of Developmental Processes in the Social Amoeba Dictyostelium
Top/Medicine	Assesment of food product by time domain NMR and MRI V predavanju bodo predstavljene napredne preiskave ivil z neinvazivnimi metodami jedrske magnetne resonance in slikanja z magnetno resonanco. Predavanje bo s konkretnimi zgledi osredotoeno na preuevanje vpliva postopkov, ki jih razvija prehrambena tehnologija, na rok trajanja, strukturo ivil in sposobnost ivil, da im hitreje veejo vodo.
Top/Medicine	How the Body Fights Infection Miniaturized battles are waged continuously by heroic micro-warriors that protect us from viruses, bacteria, fungi and parasites. Join Dr. Richard Locksley for a look at how these unseen victories (and occasional defeats) are played out, and how vaccination stacks the deck in our favor.
Top/Medicine	Development of Implantable SAW Probe for Epilepsy Prediction
Top/Medicine	Evaluation of non-invasive blood pressure simulators
Top/Medicine	Matching of Tree Structures for Registration of Medical Images Many medical applications require a registration of different images of the same organ. In many cases, such a registration is accomplished by manually placing landmarks in the images. In this paper we propose a method which is able to find reasonable landmarks automatically. To achieve this, nodes of the vessel systems, which have been extracted from the images by a segmentation algorithm, will be assigned by the so-called association graph method and the coordinates of these matched nodes can be used as landmarks for a non-rigid registration algorithm.
Top/Medicine	Matching pursuit and unification in EEG analysis
Top/Medicine	EEG Coupling, Granger Causality and Multivariate Autoregressive Models
Top/Medicine	Ambulatory blood pressure monitoring is highly sensitive for detection of early cardiovascular rsk factors in young adults
Top/Medicine	Wearable Wireless Biopotential Electrode for ECG Monitoring
Top/Medicine	Modern Medical Implants - Use of Advanced Manufacturing Technologies (LENS)
Top/Medicine	The Machine Learning Approach to Brain-Computer Interfacing - Part 1
Top/Medicine	Exploiting temporal delays in interpreting EEG/MEG data in terms of brain connectivity
Top/Medicine	Support Feature Machine for Classification of Abnormal Brain Activity In this study, a novel multidimensional time series classification technique, namely support feature machine (SFM), is proposed. SFM is inspired by the optimization model of support vector machine and the nearest neighbor rule to incorporate both spatial and temporal of the multi-dimensional time series data. This paper also describes an application of SFM for detecting abnormal brain activity. Epilepsy is a case in point in this study. In epilepsy studies, electroencephalograms (EEGs), acquired in multidimensional time series format, have been traditionally used as a gold-standard tool for capturing the electrical changes in the brain. From multi-dimensional EEG time series data, SFM was used to identify seizure pre-cursors and detect seizure susceptibility (pre-seizure) periods. The empirical results showed that SFM achieved over 80% correct classification of per-seizure EEG on average in 10 patients using 5-fold cross validation. The proposed optimization model of SFM is very compact and scalable, and can be implemented as an online algorithm. The outcome of this study suggests that it is possible to construct a computerized algorithm used to detect seizure pre-cursors and warn of impending seizures through EEG classification.
Top/Medicine	Deconstructing The Myth Of AIDS In 1984 we were told that HIV was the cause of AIDS. In his provocative documentary film, Deconstructing the Myth of AIDS, Gary Null, Ph.D., challenges virtually every statement ever made by the American medical industrial complex on the virus - including those of the Centers for Disease Control and Prevention (CDC), the National Institute for Health (NIH) and the Food and Drug Administration (FDA). While presenting the findings of Nobel Prize-winning scientists and leading virologists, the film exposes the political maneuvering, conspiracies and cover-ups that have provided obstacles to the study of this human catastrophe from the start. While presenting the findings of Nobel Prize-winning scientists and leading virologists, the film exposes the political maneuvering, conspiracies and cover-ups that have provided obstacles to the study of this human catastrophe from the start. For example, there are experts who believe that AIDS is the result of multiple factors, including drug use, stress and nutritional deficiency, but that government agencies made a politically strategic decision to de-emphasize these hypotheses and thus discourage certain researchers and their funding. Meanwhile, AZT, an infamously failed treatment for cancer, and now the primary FDA-approved approach to treating AIDS, is highly toxic and can produce the very symptoms of the illness it is prescribed to treat. Deconstructing the Myth of AIDS goes beyond medicine and science to question the very foundation of our reliance on government bureaucracies where it concerns matters of life and death.
Top/Medicine	Ventricular Fibrillation in the Human Heart. Why is it different from Fibrillation in the Dog and Pig Heart? Sudden cardiac death is one of the major health problems in the industrialised world, leading to over 300,000 mortalities in the US alone annually. In most cases, it is caused by a cardiac arrhythmia called ventricular fibrillation (VF). Under normal conditions, the coordinated contraction of the heart leads to an effective pumping of blood through the body. In contrast, during fibrillation coordination of contraction is completely lost, rendering the heart incapable of pumping around blood. Despite the huge socio-economical costs of VF and decades of research its causes and mechanisms still remainpoorly understood. In experimental studies into the mechanisms of VF, pig and dog hearts are considered the best model systems for the human heart given their comparable size. In such studies it is found that fibrillation is caused by highly disorganised electrical wave patterns consisting of 50 or more rotating spiral waves. It has been assumed that a similar organisation underlieshuman VF. However, recent clinical studies suggest that fibrillation inthe human heart may have a far more simple organisation.Modelling studies have played an important role and are playing an increasingly important role in cardiac arrhythmia research from the single ion channel to the whole heart level. However, on the whole heart level, most modelling studies thus far have used phenomenological models or small heart animal models to obtain qualitative insights in VF mechanisms and patterns. Instead, we use a detailed model of the human ventricles, to quantitatively study human VF and why it might be different from VF in the pig and dog heart. We indeed find that human VF has a significantly simpler organisation than VF in the pig and dog heart, with wave patterns consisting of around 10 spiral waves only. We then study the dependence of VF wave pattern complexity on various major parameters of our model (excitability, anisotropy, action potential duration (APD) restitution slope, minimum APD). We find that VF wave pattern complexity is most strongly dependent on minimum APD, a factor that is found to differ between human and pig and dog hearts. We thus propose that differences in minimum action potential duration cause the differences in wave pattern complexity during VF in the human and pig and dog hearts. Both the simpler spatial organisation of human VF and it's suggested cause may have important implications for treating and preventing this dangerous arrhythmia in humans.
Top/Medicine	Formulation of PLGA nanoparticles for intracellular delivery of protein drug Design and formulation of advanced drug delivery systems (DDS), such as nanoscale carriers, presents an attractive research area in the field of drug formulation. A vast contribution is expected in delivery of biopharmaceuticals as is clearly recognized that inadequate delivery is the single most important factor delaying their application in clinical practise. In spite of some successful guidelines, formulation of protein drugs in DDS requires step-by step strategy and methods differing from those used for classical pharmaceutical drugs since proteins are the most delicate ones in term of retaining their biological function. A model protein drug cystatin was selected in our work, having high potential for inactivating cysteine proteases, enzymes involved in processes of tumour invasion and metastasis. Nanoparticles was used as carrier system with the aim to increase the bioavailability of the protein drug by protecting it from premature degradation in biological environment and faciliting its intracellular delivery. Cystatin was incorporated in poly(lactide-co-glycolide) (PLGA) nanoparticles by the water-in-oil-in-water emulsion solvent diffusion method. To preserve its biological activity an optimized technique was developed, adjusting physical and chemical parameters of processes during nanoparticle production. Cystatin-loaded NPs had size of 300-350 nm diameter, and contained 1.6 % (w/w) of cystatin, retaining 85% of its starting activity. To follow cellular uptake of nanoparticles, cystatin was labelled with fluorescent dye (Alexa Fluor 488) prior to its encapsulation into NPs. Image analysis showed rapid internalization of NPs into MCF-10A neoT cells as the fluorescence spots were detected after treatment with NPs. On the other hand, labelled free cystatin was internalised very slowly, suggesting that NPs facilitate the delivery of its cargo into the cells. Cystatin, delivered by NPs, also exerted its inhibitory activity on intracellular target cathepsin B, suggesting that its integrity was preserved throughout the processes of formulation and delivery. On the other hand, free cystatin did not impart proteolityic activity of cathepsin B, when tested under the same conditions using the substate, specific for intracellular cathepsin B. Our results show that protein drug can be formulated in the active form into PLGA NPs, when suitably selecting the process parameters of NP-production. NPs are also able to facilitate delivery of protein drug into the cells, enabling its activity on the intracellular target
Top/Medicine	Measuring Red Blood Cell Velocity with a Keyhole Tracking Algorithm
Top/Medicine	MAHI: Investigation of Social Scaffolding for Reflective Thinking in Diabetes Management We describe the design and deployment of MAHI, a ubicompapplication for individuals with diabetes, and its effect on individuals ownership of their health and disease management.
Top/Medicine	Problems faced after the transition from a film to a DDR Radiology Department
Top/Medicine	E-health
Top/Medicine	Snoring and CT Imaging
Top/Medicine	Novel HIV Inhibitors
Top/Medicine	Optoacoustic imaging, a promising technique for non-invasive diagnosis of cancer Optoacoustics provides high optical contrast without the handicap of poor resolution in imaging of optically turbid tissues. In biomedical optoacoustics, tissue is illuminated with short laser pulses. The light is scattered inside the tissue and heats absorbing structures, such as blood vessels, hidden deeply inside the tissue. Image contrast is therefore provided by light absorbing chromophores, either endogenous (such as oxy- or deoxyhemoglobin) or exogenous (e.g. dyes, nanoparticles or quantum dots). By means of the thermoelastic effect, the inhomogeneous heating generates pressure transients exactly representing the absorbing structures. These acoustic transients propagate to the tissue surface and can be detected with an appropriate ultrasound transducer. In one-dimensional optoacoustic measurements, time delay between the laser pulse and detected pressure transient, its amplitude and temporal profile provide information about the location, strength and spatial dimension of the acoustic source. Three-dimensional images can be reconstructed by scanning the transducer. The image quality depends on a number of factors, including the irradiation geometry and image reconstruction algorithm. The talk will give an overview of the possibilities and limitations of optoacoustic imaging in turbid tissues, especially in terms of image contrast and depth resolution.
Top/Computer_Science/Image_Analysis/fMRI_-_Analysis	Exploring human object-vision with hi-res fMRI and information-based analysis
Top/Computer_Science/Image_Analysis/fMRI_-_Analysis	Classifying single trial fMRI: What can machine learning learn? We describe three experiments combining neuroimaging and machine learning. The first experiment compares the performance of maximum likelihood and neural net classifiers for 'brain reading' of fMRI data in the visual cortex. The second experiment applies the optimal classifier to measure the development of the face region in children and adolescents. While the previous experiments used block designs, the third experiment describes an event-related experiment where the classification algorithm learned something real, but not what was planned. The corroboration and validation of the classification results with brain images will be demonstrated.
Top/Computer_Science/Image_Analysis/fMRI_-_Analysis	fMRI-based decoding of the modified default-mode network in mild cognitive impairment The diagnostic tool to detect early stages of Alzheimers Disease, a progressive neurodegenerative disease, is lacking until today. FDG-PET (Fluorodeoxyglucose-Positron Emission Tomography) shows hypometabolic areas in the brains of pre-demented, i.e. patients suffering from mild cognitive impairment (MCI). The reduced activity may be attributed to disrupted connectivity of the resting-, or default-mode network of the brain [1]. In this contribution, we study the detection of such a network using the framework of blind signal processing, a technique to identify hidden sources within a multivariate mixture using source characteristics such as statistical independence or sparseness. The results are compared to FDG-PET data.
Top/Computer_Science/Image_Analysis/fMRI_-_Analysis	Generative Models for Decoding Real-Valued Natural Experience in FMRI Functional Magnetic Resonance Imaging (FMRI) provides an unprecedented window into the complex functioning of the human brain, typically detailing the activity of thousands of voxels for hundreds of time points. The interpretation of FMRI is complicated, however, because of the unknown connection between the hemodynamic response and neural activity, and the unknown spatiotemporal characteristics of the cognitive patterns themselves. Recent work has exploited techniques from machine learning to find patterns of voxel activity related to brain processes (see e.g., [1]). Many of these techniques involve decoding, inferring the value or category class of a stimulus !S given a pattern of voxel activations !V . Decoding can generally be split into two approaches, discriminative and generative [2]. With a discriminative model one learns the conditional distribution P(!S |!V ) directly by minimizing a loss such as minimum classification error. Alternatively, the generative approach obtains this conditional probability through Bayes rule; one posits and fits models for P(!S ) and P(!V |S) instead. Both approaches can reliably establish the existence of sufficient decoding information.
Top/Computer_Science/Image_Analysis/fMRI_-_Analysis	Hierarchical Gaussian Naive Bayes Classifier for Multiple-Subject fMRI Data The Gaussian Nave Bayes (GNB) [2] classifier has been successfully applied to fMRI data. However, it is not specifically designed to account for data from multiple subjects and is usually applied to data from a single subject (referred to as GNB-indiv). An extension to the GNB classifier has been proposed ([4], referred to as GNB-pooled), in which the data from all the subjects are combined together navely by assuming that they all come from the same subjects. However, this extension ignores subject-specific variations that might exist. Here I describe another extension of the GNB classifierthe hierarchical GNB classifier [3]that can account for subject-specific variations, and in addition, has the flexibility to increase or reduce the weight of the contribution of the data from the other subjects based on the number of examples available from the test subject.
Top/Computer_Science/Image_Analysis/fMRI_-_Analysis	Unsupervised fMRI Analysis Recently machine learning methodology has been used increasing to analyze the relationship between stimulus categories and fMRI responses [2, 14, 15, 11, 13, 8, 9, 1, 12, 7]. Here, we introduce a new unsupervised machine learning approach to fMRI analysis approach, in which the simple categorical description of stimulus type (e.g. type of task) is replaced by a more informative vector of stimulus features. We compared this new approach with a standard Support Vector Machine (SVM) analysis of fMRI data using a categorical description of stimulus type. The following study differs from conventional unsupervised approaches in that we make use of the stimulus characteristics. We use kernel Canonical Correlation Analysis (KCCA) to learn the correlation between the fMRI volume and the corresponding stimulus features presented at a particular time point. CCA can be seen as the problem of finding basis vectors for two sets of variables such that the correlation of the projections of the variables onto these basis vectors are mutually maximised. KCCA first projects the data into a higher dimensional feature space before performing CCA in the new feature space.
Top/Computer_Science/Image_Analysis/fMRI_-_Analysis	Enhancing functional magnetic resonance imaging with supervised learning This paper reports novel applications of supervised learning methods intended to directly impact fMRI technology with the aim of improving data acquisition and analysis.
Top/Computer_Science/Image_Analysis/fMRI_-_Analysis	Challenges and limitations in interpreting learnt classifiers
Top/Computer_Science/Image_Analysis/fMRI_-_Analysis	Overview of decoding of mental states and processes
Top/Computer_Science/Image_Analysis/fMRI_-_Analysis	Multiple hypotheses testing in functional neuroimaging applications
Top/Computer_Science/Image_Analysis/fMRI_-_Analysis	Implications of decoding for theories of neural representation
Top/Computer_Science/Image_Analysis/fMRI_-_Analysis	From functional elements to networks in fMRI
Top/Computer_Science/Image_Analysis/fMRI_-_Analysis	Scanning the brain and probing the mind Functional neuroimaging offers insight into the working of human mind and brain. It is used to study how the brain function changes in different neurological and psychiatric diseases. But can it be also used to explore the mysteries of love, to determine what people like, as a lie detector, or as an interface between the brain and the computer? Are we finally able to read the human mind?
Top/Computer_Science/Image_Analysis/fMRI_-_Analysis	Spatiotemporal classification
Top/Computer_Science/Image_Analysis/fMRI_-_Analysis	What Mental States? Exploring How Dimensionality Reduction Might Contribute to the Refinement of Cognitive Models Questions in cognitive neuroscience are often framed in terms of correspondences between known types: How is brain state X related to cognitive state Y? What are the correlations or mappings between particular structures and functions? Such framings are well suited for confirmatory testing of coarse-grained hypotheses. They are not necessarily informative, however, for the purpose of exploring finer physical and functional structure. To the contrary, physical states are typically aggregated over anatomical regions of interest, while tasks are designed to optimize one or a few functional contrasts of interest rather than to cover a fuller behavioral or cognitive range.
Top/Computer_Science/Image_Analysis/fMRI_-_Analysis	Multimodal Imaging: EEG-fMRI integration
Top/Computer_Science/Image_Analysis/fMRI_-_Analysis	EEG/fMRI correlation analysis. A data and model driven approach
Top/Computer_Science/Image_Analysis/fMRI_-_Analysis	Learning, Information Extraction and the Web
Top/Computer_Science/Image_Analysis/Text_Recognition	Text Recognition Evaluation
Top/Computer_Science/Image_Analysis/Text_Recognition	Videotext Recognition System
Top/Computer_Science/Image_Analysis/Text_Recognition	Book-Adaptive and Book-Dependent Models to Accelerate Digitization of Early Music Optical music recognition (OMR) enables early music collections to be digitized on a large scale. The workflow for such digitisation projects also includes scanning and preprocessing, but the cost of expert human labour to correct automatic recognition errors dominates the cost of these other two steps. To reduce the number of recognition errors in the OMR process, we present an innovative application of maximum a posteriori (MAP) adaptation for hidden Markov models (HMMs) to build book-adaptive models, taking advantage of the new learning data generated from human editing work, which is part of any music digitization project. We also experimented with using the generated data to build book-dependent models from scratch, which sometimes outperform the book-adaptive models after enough corrected data is available. Our experiments show that these approaches can reduce human editing costs by more than half and that they are especially well suited to highly variable sources like early or degraded documents.
Top/Computer_Science/Machine_Learning/Principal_Component_Analysis	Discrete PCA Methods for analysis of principal components in discrete data have existed for some time under various names such as grade of membership modelling, probabilistic latent semantic indexing, genotype inference with admixture, non-negative matrix factorization, latent Dirichlet allocation, multinomial PCA, and Gamma-Poisson models. Statistical methodologies for developing algorithms are equally as varied, although this talk will focus on the Bayesian framework. The most well published application is genetype inference, but text analysis is now increasingly seeing use because the algorithms cope with very large sparse matrices. This talk will present the general model, a discrete version of both PCA and ICA, present alternative representations, and several algorithms (mean field and Gibbs).
Top/Computer_Science/Machine_Learning/Principal_Component_Analysis	Generalized Principal Component Analysis (GPCA) Data segmentation is usually though of as a chicken-and-egg problem. In order to estimate a mixture of models one needs to first segment the data, and in order to segment the data one needs to know the model parameters. Therefore, data segmentation is usually solved in two stages 1. Data clustering and 2. Model fitting. Other iterative methods use, e.g. the Expectation Maximization (EM) algorithm. This talk will show that for a wide class of segmentation problems with multi-linear structure (including clustering subspaces of unknown and varying dimensions), the chicken-and-egg dilemma can be tackled as follows: 1. Fit a set of polynomials to all data points, without clustering the data 2. Obtain the model parameters for each group from the derivatives of these polynomials. Applications of GPCA to image/video/motion segmentation, face clustering, and identification of hybrid dynamical models systems will also be presented.
Top/Computer_Science/Machine_Learning/Principal_Component_Analysis	PERFORMANCE BOUNDS FOR KERNEL PCA
Top/Computer_Science/Machine_Learning/Principal_Component_Analysis	Randomized PCA Algorithms with Regret Bounds that are Logarithmic in the Dimension We design an on-line algorithm for Principal Component Analysis. The instances are projected into a probabilistically chosen low dimensional subspace. The total expected quadratic approximation error equals the total quadratic approximation error of the best subspace chosen in hindsight plus some additional term that grows linearly in dimension of the subspace but logarithmically in the dimension of the instances.
Top/Computer_Science/Machine_Learning/Principal_Component_Analysis	Independent Component Analysis In independent component analysis (ICA), the purpose is to linearly decompose a multidimensional data vector into components that are as statistically independent as possible. For nongaussian random vectors, this decomposition is not equivalent to decorrelation as is done by principal component analysis, but something considerably more sophisticated. ICA allows one to separate nongaussian source signals from their linear mixtures 'blindly', i.e. using no other information than the congaussianity of the source signals. ICA can also be used to extract features from image and sound signals according to the principle of redundancy reduction that has its origins in the neurosciences. In my talks I will review the basic theory and theoretical background of ICA together with some recent theoretical developments.
Top/Computer_Science/Machine_Learning/Principal_Component_Analysis	Independent Component Analysis The course provides an introduction to independent component analysis and source separation. We start from simple statistical principles; examine connections to information theory and to sparse coding; we give an overview of available algorithmics; we also show how several key ideas of ICA are illuminated by information geometry.
Top/Computer_Science/Machine_Learning/Principal_Component_Analysis	Neighbourhood Components Analysis and Metric Learning Say you want to do K-Nearest Neighbour classification. Besides selecting K, you also have to chose a distance function, in order to define nearest. Ill talk about a method for learning from the data itself a distance measure to be used in KNN classification. The learning algorithm, Neighbourhood Components Analysis (NCA) directly maximizes a stochastic variant of the leave-one-out KNN score on the training set. Of course, the resulting classification model is non-parametric, making no assumptions about the shape of the class distributions or the boundaries between them. I will also discuss an variant of the method which is a generalization of Fishers discriminant and defines a convex optimization problem by trying to collapse all examples in the same class to a single point and trying to push examples in other classes infinitely far away. By approximating the metric with a low rank matrix, these learning algorithms, can also be used to obtain a low-dimensional linear embedding of the original input features allowing that can be used for data visualization and very fast classification in high dimensions.
Top/Computer_Science/Machine_Learning/Principal_Component_Analysis	Neighbourhood Components Analysis Say you want to do K-Nearest Neighbour classification. Besides selecting K, you also have to chose a distance function, in order to define 'nearest'. I'll talk about a novel method for *learning* -- from the data itself -- a distance measure to be used in KNN classification. The learning algorithm, Neighbourhood Components Analysis (NCA) directly maximizes a stochastic variant of the leave-one-out KNN score on the training set. It can also learn a low-dimensional linear embedding of labeled data that can be used for data visualization and very fast classification in high dimensions. Of course, the resulting classification model is non-parametric, making no assumptions about the shape of the class distributions or the boundaries between them. If time permits, I'll also talk about newer work on learning the same kind of distance metric for use inside a Gaussian Kernel SVM classifier.
Top/Computer_Science/Machine_Learning/Principal_Component_Analysis	Principal Component and the Long Run
Top/Computer_Science/Machine_Learning/Principal_Component_Analysis	Monte Carlo Simulation methods The course provides an introduction to independent component analysis and source separation. We start from simple statistical principles; examine connections to information theory and to sparse coding; we give an overview of available algorithmics; we also show how several key ideas of ICA are illuminated by information geometry.
Top/Computer_Science/Machine_Learning/Principal_Component_Analysis	Latent Semantic Variable Models In the context of information retrieval and natural language processing, latent variable models are quite useful in modeling and discovering hidden structure that often leads to 'semantic' data representations. This talk will provide an overview of the most popular approaches and discuss the range of possible applications for such models, including language modeling, ad hoc retrieval, text categorization and collaborative filtering.
Top/Computer_Science/Machine_Learning/Principal_Component_Analysis	Machine Learning Flavor of Random Matrices
Top/Computer_Science/Machine_Learning/Principal_Component_Analysis	Some aspects of Latent Structure Analysis Latent structure models involve real, potentially observable variables and latent, unobservable variables. Depending on the nature of these variables, whether they be discrete or continuous, the framework includes various particular types of model, such as factor analysis, latent class analysis, latent trait analysis, latent profile models, mixtures of factor analysers, state-space models and others. The simplest scenario, of a single discrete latent variable, includes finite mixture models, hidden Markov chain models and hidden Markov random field models. The talk will give an overview of the application of maximum likelihood and Bayesian approaches to the estimation of parameters within these models, emphasising especially the fact that computational complexity varies greatly among the different scenarios. In the case of a single discrete latent variable, the issue of assessing its cardinality will be discussed, in the context of questions such as the appropriate number of mixture components to be included in a mixture model, or, in the interests of parsimony, the minimum plausible cardinality of such a latent variable. Techniques such as the EM algorithm, Markov chain Monte Carlo methods and variational approximations will be featured in the talk.
Top/Computer_Science/Machine_Learning/Principal_Component_Analysis	Relational Latent Class Models
Top/Computer_Science/Machine_Learning/Principal_Component_Analysis	Efficient Computation of Recursive Principal Component Analysis
Top/Computer_Science/Machine_Learning/Principal_Component_Analysis	Uncorrelated Multilinear Principal Component Analysis through Successive Variance Maximization Tensorial data are frequently encountered in various machine learning tasks today and dimensionality reduction is one of their most important applications. This paper extends the classical principal component analysis (PCA) to its multilinear version by proposing a novel dimensionality reduction algorithm for tensorial data, named as uncorrelated multilinear PCA (UMPCA). UMPCA seeks a tensor-to-vector projection that captures most of the variation in the original tensorial input while producing uncorrelated features through successive variance maximization. We evaluate the proposed algorithm on a second-order tensorial problem, face recognition, and the experimental results show its superiority, especially in low-dimensional spaces, through the comparison with three other PCA-based algorithms.
Top/Computer_Science/Machine_Learning/Principal_Component_Analysis	Dirichlet Component Analysis: Feature Extraction for Compositional Data We consider feature extraction (dimensionality reduction) for compositional data, where the data vectors are constrained to be positive and constant-sum. In real-world problems, the data components (variables) usually have complicated 'correlations' while their total number is huge. Such scenario demands feature extraction. That is, we shall de-correlate the components and reduce their dimensionality. Traditional techniques such as the Principle Component Analysis (PCA) are not suitable for these problems due to unique statistical properties and the need to satisfy the constraints in compositional data. This paper presents a novel approach to feature extraction for compositional data. Our method first identifies a family of dimensionality reduction projections that preserve all relevant constraints, and then finds the optimal projection that maximizes the estimated Dirichlet precision on projected data. It reduces the compositional data to a given lower dimensionality while the components in the lower-dimensional space are de-correlated as much as possible. We develop theoretical foundation of our approach, and validate its effectiveness on some synthetic and real-world datasets.
Top/Environment	Sea Levels and Climate Change Popular interest and scientific concerns have focussed on the potential for sea level rise and increased risks of coastal flooding in a future warmer world. This talk will review the recent global evidence for changes, including the 2007 IPCC Report and look in detail at changes observed at the principal UK Tide Gauge site, Newlyn, from 1915 to 2005. Changes may include not only mean sea level rises, but also increased meterorological effects on surges, and changes in tidal regimes.
Top/Environment	Sustainability concept of energy, water and environmental systems This review first introduces the historical background for the sustainability concept development. Special reference is given to the energy resource depletion and its forecast. In the assessment of global energy resources attention is focused in on the resource consumption and its relevancy to the future demand. The recent assessment of sustainability is reflecting the normative and strategic dimension of sustainability. Special attention is devoted to the most recent development of the concept of sustainability science. A new field of sustainability science is emerging that seeks to understand the fundamental character of interactions between nature and society. With a view toward promoting research necessary to achieve such advances, an initial set of core questions for sustainability science was proposed.
Top/Environment	Stanford Experts on Climate Change and Carbon Trading Dr. Schneider is one of the world's leading scientific experts of climate change (his name is cited on all those climate change charts and graphs we've seen so far). Dr. Heller has extensive experience with policy and negotiations surrounding climate change and sustainable development. Professor Heller also recently served as Sergey's host at the recent UN Climate Change Conference meeting in Montreal where Prof. Heller proved his indepth knowledge of thenuances of legislative works, such as the Kyoto Protocol, and the mechanisms that are currently being employed.
Top/Environment	UK Minister of Finance - Why Climate Change has to be a priority for the Treasury Department
Top/Environment	UK Minister of Environment - Tackling the Environmental Issue
Top/Environment	Converging the Lisbon Agenda and Sustainable Development
Top/Environment	Environmental consequences of better roads
Top/Environment	Act on CO2
Top/Environment	Nature, not human activity, rules the climate The science is settled: Evidence clearly demonstrates that Carbon dioxide contributes insignificantly to Global Warming and is therefore not a 'pollutant.' This fact has not yet been widely recognized, and irrational Global Warming fears continue to distort energy policies and economic policy. All efforts to curtail CO,,2,, emissions, whether global or at the state level, are pointless -- and in any case, ineffective and very costly. On the whole, a warmer climate is beneficial.
Top/Environment	Energy policy: complex? Energy generation is comparatively simple, energy policy is not. Can complex systems science help us to understand the national and international policies past and present? Can it contribute to a sensible resolution of the problems caused by our low cost energy economies and resulting carbon emissions? If so, how? if not, why not?
Top/Environment	CO2 beyond tomorrow - a fundamental approach
Top/Environment	Hydrogen technology activities in Slovenia
Top/Computer_Science/Semantic_Web/Annotation/Tagging	Semantic Authoring and Annotation: The Dynamics and Semantics of Collaborative Tagging
Top/Computer_Science/Semantic_Web/Annotation/Tagging	Semantic Authoring and Annotation: Semantic Authoring by Tagging with Annotea Social Bookmarks and Topics
Top/Computer_Science/Semantic_Web/Annotation/Tagging	Semantic Authoring and Annotation: Specifying the Collaborative Tagging System
Top/Computer_Science/Semantic_Web/Annotation/Tagging	Social-media blog tagging: Metadata or just more content ? The authoring of tags -- unlike the authoring of traditional metadata -- is highly popular among users. This harbours unprecedented opportunities for organizing content. However, tags are still poorly understood. What do they ''mean'', in what senses are they similar to or different from metadata? Different tags support different communities, but how exactly do they reflect the plurality of opinions,what is the relation to individual differences in authoring and reading? In this paper, we offer a definition and empirical evidence for the claim that ''tags are not metadata, but just more content''. The analysis rests on a multi-annotator classification of a blog corpus using the WordNet domain labels system (WND), the development of a system of text-classification methods using WordNet and WND, and a quantitative and qualitative comparative analysis of these classifications. We argue that the notion of a ''gold standard'' may be meaningless in social media, and we outline possible consequences for labelling and search-engine development.
Top/Computer_Science/Semantic_Web/Annotation/Tagging	Corpora, evaluation tools
Top/Computer_Science/Semantic_Web/Annotation/Tagging	Can Social Bookmarks Improve Web Search? Social bookmarking is a recent phenomenon which has the potential to give us a great deal of data about pages on the web. One major question is whether that data can be used to augment systems like web search. To answer this question, over the past year we have gathered what we believe to be the largest dataset from a social bookmarking site yet analyzed by academic researchers. Our dataset represents about forty million bookmarks from the social bookmarking site del.icio.us. We contribute a characterization of posts to del.icio.us: how many bookmarks exist (about 115 million), how fast is it growing, and how active are the URLs being posted about (quite active). We also contribute a characterization of tags used by bookmarkers. We found that certain tags tend to gravitate towards certain domains, and vice versa. We also found that tags occur in over 50 percent of the pages that they annotate, and in only 20 percent of cases do they not occur in the page text, backlink page text, or forward link page text of the pages they annotate. We conclude that social bookmarking can provide search data not currently provided by other sources, though it may currently lack the size and distribution of tags necessary to make a significant impact.
Top/Computer_Science/Semantic_Web/Annotation/Tagging	Beyond String Search: Fast and Accurate Retrieval of Entities and Dependencies
Top/Computer_Science/Semantic_Web/Annotation/Tagging	Network Structure of Folksonomies Folksonomies can be viewed as three mode graphs or as graphs made up of nodes (tags, users, resources) connected by hyper-edges. I shall report on some network statistical properties of a folksonomy graph based on data collected for the del.icio.us system. Moreover, by introducing a suitable distance between resources based on tag co-occurrence, I shall show that folksonomies embed a meaningful semantic clusterization of resources.
Top/Computer_Science/Machine_Learning/Gaussian_Processes	A Gaussian Approximation for Stochastic Nonlinear Dynamical Processes with Annihilation
Top/Computer_Science/Machine_Learning/Gaussian_Processes	Analysing Gene Expression Data Using Gaussian Processes Complex gene regulatory mechanisms ensure the proper functioning of biological cells. New high-throughput experimental techniques, such as microarrays, provide a snapshot of gene expression levels of thousands of genes at the same time. If repeated on a sample of synchronized cells, time-series profiles of gene activity can be obtained. The aim is to reconstruct the complex gene regulatory network underlying these profiles. Genes often influence each other in a nonlinear fashion and with intricate interaction patterns. Linear models are often unsuited to capture such relationships. Gaussian processes, on the other hand, are ideal for representing nonlinear relationships. A particular attraction is the automatic relevance determination effect, removing unused inputs and resulting in sparse gene networks.
Top/Computer_Science/Machine_Learning/Gaussian_Processes	An Exchanging-based Refinement to Sparse Gaussian Process Regression We propose a backward deletion procedure to Sparse Gaussian Process Regression (SGPR) model, which can be used to refine a number of se- quential forward selection algorithms addressed recently. Some experi- mental results demonstrate the effectiveness of our approach.
Top/Computer_Science/Machine_Learning/Gaussian_Processes	Bayesian Data Fusion with Gaussian Process Priors : An Application to Protein Fold Recognition Various emerging quantitative measurement technologies are producing genome, transcriptome and proteome-wide data collections which has motivated the de- velopment of data integration methods within an inferential framework. It has been demonstrated that for certain prediction tasks within computational biol- ogy synergistic improvements in performance can be obtained via integration of a number of (possibly heterogeneous) data sources. In [1] six different parameter representations of proteins were employed for fold recognition of proteins using Support Vector Machines (SVM).
Top/Computer_Science/Machine_Learning/Gaussian_Processes	Dimensionality Reduction in Gaussian Process Models
Top/Computer_Science/Machine_Learning/Gaussian_Processes	Extensions of Gaussian Processes for Ranking: Semi-Supervised and Active Learning
Top/Computer_Science/Machine_Learning/Gaussian_Processes	Flexible and efficient Gaussian process models I will briefly describe our work on the sparse pseudo-input Gaussian process (SPGP), where we refine the sparse approximation by selecting `pseudo-inputs' using gradient methods. I will then describe several extensions to this framework. Firstly we incorporate supervised dimensionality reduction to deal with high dimensional input spaces. Secondly we develop a version of the SPGP that can handle input-dependent noise. These extensions allow GP methods to be applied to a wider variety of modelling tasks than previously possible.
Top/Computer_Science/Machine_Learning/Gaussian_Processes	Gaussian Process Approximations of Stochastic Differential Equations It is well known that certain classes of Gaussian process arise naturally as solutions to stochastic differential equations, for example the Ornstein-Uhlenbeck process arises as the stationary solution of a simple linear stochastic differential equation. In this work we introduce some initial results on the approximation of the solution of general stochastic differential equations by Gaussian processes. We employ a variational framework, where we seek a Gaussian process approximation to the posterior distribution of the state of a system whose dynamics are governed by a stochastic differential equation. The application for this work is approximate inference within stochastic dynamic models, in particular models used in weather forecasting.
Top/Computer_Science/Machine_Learning/Gaussian_Processes	Gaussian Process Basics How on earth can a plain old Gaussian distribution be useful for sophisticated regression and machine learning tasks?
Top/Computer_Science/Machine_Learning/Gaussian_Processes	Gaussian Processes
Top/Computer_Science/Machine_Learning/Gaussian_Processes	Gaussian Processes for Active Sensor Management In this paper we study the active sensor management problem using continuous optimal experimental design (OED) framework. This task comprises the determination of allocation for a limited number of sensors over the spatial domain and the number of repetitive measurements in these locations in order to improve the overall system performance. We present a principled approach to active sensor management with repetitive measurements for Gaussian Processes (GPs) using a generalised D-optimality criteria and soft margin constrains. The resulting optimum of the convex optimization of the optimal experimental design for GP is generally sparse, in the sense that measurements should be taken at only a limited set of possible sensor locations. We demonstrate the use of our method on articial dataset.
Top/Computer_Science/Machine_Learning/Gaussian_Processes	Gaussian Processes for Monocular 3D People tracking We advocate the use of Gaussian Processes (GPs) to learn prior models of human pose and motion for 3D people tracking. The Gaussian Process Latent variable model (GPLVM) provides a low-dimensional embedding of the human pose, and defines a density function that gives higher probability to poses close to the training data. The Gaussian Process Dynamical Model (GPDM) provides also a complex dynamical model in terms of another GP. With the use of Bayesian model averaging both GPLVM and GPDM can be learned from relatively small amounts of training data, and they generalize gracefully to motions outside the training set. We show that such priors are effective for tracking a range of human walking styles, despite weak and noisy image measurements and a very simple image likelihood. Tracking is formulated in terms of a MAP estimator on short sequences of poses within a sliding temporal window.
Top/Computer_Science/Machine_Learning/Gaussian_Processes	Gaussian Processes for Prediction in Intensive Care In this paper we present the use of Gaussian Processes for regression in the application of prediction in Intensive Care. We propose a preliminary solution to predicting the evolution of a patient's state during his stay in intensive care by means of deffined patient speciffic characteristics.
Top/Computer_Science/Machine_Learning/Gaussian_Processes	Gaussian Processes for Principal Component Analysis We show how the supervised method of Gaussian Processes may be used for Principal Component Analysis using two intuitions about the nature of the rst principal component lters.
Top/Computer_Science/Machine_Learning/Gaussian_Processes	Gaussian Process Implicit Surfaces Many applications in computer vision and computer graphics require the definition of curves and surfaces. Implicit surfaces are a popular choice for this because they are smooth, can be appropriately constrained by known geometry, and require no special treatment for topology changes. In this paper we use Gaussian processes for this and derive a covariance function equivalent to the thin plate spline regularizer which has desirable properties for shape modelling. We demonstrate our approach for both 2D curves and 3D surfaces. The benefit of using a Gaussian process for this is the meaningful probabilistic representation of the function.
Top/Computer_Science/Machine_Learning/Gaussian_Processes	Gaussian Process Model for Inferring the Regulatory Activity of Transcription Factor Proteins Inferring the concentration of transcription factors' proteins from the expression levels of target genes is a very active area of research in computational biology. Usually, the dynamics of the gene expression levels are modelled using differential equations where the transcription factor protein concentrations are treated as parameters, subsequently estimated using MCMC. We show how this inference problem can be solved more elegantly by placing a GP prior over the latent functions, obtaining comparable results to the standard MCMC approach in a fraction of the time.
Top/Computer_Science/Machine_Learning/Gaussian_Processes	Hierarchical Gaussian Naive Bayes Classifier for Multiple-Subject fMRI Data The Gaussian Nave Bayes (GNB) [2] classifier has been successfully applied to fMRI data. However, it is not specifically designed to account for data from multiple subjects and is usually applied to data from a single subject (referred to as GNB-indiv). An extension to the GNB classifier has been proposed ([4], referred to as GNB-pooled), in which the data from all the subjects are combined together navely by assuming that they all come from the same subjects. However, this extension ignores subject-specific variations that might exist. Here I describe another extension of the GNB classifierthe hierarchical GNB classifier [3]that can account for subject-specific variations, and in addition, has the flexibility to increase or reduce the weight of the contribution of the data from the other subjects based on the number of examples available from the test subject.
Top/Computer_Science/Machine_Learning/Gaussian_Processes	How to choose the covariance for Gaussian process regression independently of the basis In Gaussian process regression, both the basis functions and their prior distribution are simultaneously specified by the choice of the covariance function. In certain problems one would like to choose the covariance independently of the basis functions (e. g., in polynomial signal processing or Wiener and Volterra analysis). We propose a solution to this problem that approximates the desired covariance function at a finite set of input points for arbitrary choices of basis functions. Our experiments show that this additional degree of freedom can lead to improved regression performance.
Top/Computer_Science/Machine_Learning/Gaussian_Processes	Improved Fast Gauss Transform
Top/Computer_Science/Machine_Learning/Gaussian_Processes	Inferring Latent Functions with Gaussian Processes in Differential Equations
Top/Computer_Science/Machine_Learning/Gaussian_Processes	In search of Non-Gaussian Components of a High-Dimensional Distribution In high dimensional data analysis, finding non-Gaussian components is an important preprocessing step for efficient information processing. This article proposes a new linear method to identify the non- Gaussian subspace within a very general semi-parametric framework. Our proposed method NGCA (Non-Gaussian Component Analysis) is essentially based on the theoretical fact that, via an arbitrary nonlinear function, a vector which approximately belongs to the low dimensional non-Gaussian subspace can be constructed. Since different nonlinear functions yield different directions, one can obtain an approximate subspace from a set of different nonlinear functions. PCA is then applied to identify the non-Gaussian subspace. A numerical study demonstrates the usefulness of our method.
Top/Computer_Science/Machine_Learning/Gaussian_Processes	Kernels and Gaussian Processes
Top/Computer_Science/Machine_Learning/Gaussian_Processes	Learning to Control an Octopus Arm with Gaussian Process Temporal Difference Methods The Octopus arm is a highly versatile and complex limb. How the Octopus controls such a hyper-redundant arm (not to mention eight of them!) is as yet unknown. Robotic arms based on the same mechanical principles may render present day robotic arms obsolete. In this talk, I will describe how we tackle this problem using an online reinforcement learning algorithm, based on a Bayesian approach to policy evaluation known as Gaussian process temporal difference (GPTD) learning.
Top/Computer_Science/Machine_Learning/Gaussian_Processes	Linear Projections and Gaussian Process Reconstructions
Top/Computer_Science/Machine_Learning/Gaussian_Processes	Modelling Transcriptional Regulation with Gaussian Processes Modelling the dynamics of transcriptional processes in the cell requires the knowledge of a number of key biological quantities. While some of them are relatively easy to measure, such as mRNA decay rates and mRNA abundance levels, it is still very hard to measure the active concentration levels of the transcription factor proteins that drive the process and the sensitivity of target genes to these concentrations. In this paper we show how these quantities for a given transcription factor can be inferred from gene expression levels of a set of known target genes. We treat the protein concentration as a latent function with a Gaussian process prior, and include the sensitivities, mRNA decay rates and baseline expression levels as hyperparameters. We apply this procedure to a human leukemia dataset, focusing on the tumour repressor p53 and obtaining results in good accordance with recent biological studies. Joint work with Guido Sanguinetti and Magnus Rattray.
Top/Computer_Science/Machine_Learning/Gaussian_Processes	Near-Optimal Sensor Placements in Gaussian Processes
Top/Computer_Science/Machine_Learning/Gaussian_Processes	Probabilistic Non-Linear Principal Component Analysis with Gaussian Process Latent Variables It is known that Principal Component Analysis has an underlying probabilistic representation based on a latent variable model. Principal component analysis (PCA) is recovered when the latent variables are integrated out and the parameters of the model are optimised by maximum likelihood. It is less well known that the dual approach of integrating out the parameters and optimising with respect to the latent variables also leads to PCA. The marginalised likelihood in this case takes the form of Gaussian process mappings, with linear Covariance functions, from a latent space to an observed space, which we refer to as a Gaussian Process Latent Variable Model (GPLVM). This dual probabilistic PCA is still a linear latent variable model, but by looking beyond the inner product kernel as a for a covariance function we can develop a non-linear probabilistic PCA.
Top/Computer_Science/Machine_Learning/Gaussian_Processes	Regularization of Kernel Methods by Decreasing the Bandwidth of the Gaussian Kernel
Top/Computer_Science/Machine_Learning/Gaussian_Processes	Sparse Log Gaussian Processes via MCMC for Spatial Epidemiology Log Gaussian processes (LGP) are an attractive manner to construct intensity surfaces for the purposes of spatial epidemiology. The intensity surfaces are naturally smoothed by placing a GP prior over the relative log Poisson rate. In this work a fully independent training conditional (FITC) sparse approximation is used to speed up GP computations. The sampling of the latent values is sped up with transformations taking into account the approximate conditional posterior precision.
Top/Computer_Science/Machine_Learning/Gaussian_Processes	The EM algorithm and Mixtures of Gaussians
Top/Computer_Science/Machine_Learning/Gaussian_Processes	The Empirical Bayes Estimation of an Instantaneous Spike Rate with a Gaussian Process Prior
Top/Computer_Science/Machine_Learning/Gaussian_Processes	The Gaussian Variational Approximation of Stochastic Differential Equations
Top/Computer_Science/Machine_Learning/Gaussian_Processes	Wifi Localization with Gaussian Processes Estimating the location of a mobile device from wireless signal strength is an interesting research problem, especially given the complexity of signal propagation through space in the presence of obstacles such as buildings, walls, or people. Gaussian processes have already been used to solve such signal strength localization problems. We extend this work to indoor WiFi localization and present novel kernel functions which increase the accuracy of the Gaussian process model, especially when faced with sparse training data. We additionally present preliminary results of simultaneous mapping and localization using Gaussian process latent variable modeling.
Top/Computer_Science/Machine_Learning/Gaussian_Processes	A Tutorial Introduction to Stochastic Differential Equations: Continuous-time Gaussian Markov Processes
Top/Computer_Science/Machine_Learning/Gaussian_Processes	Best Paper - Information-Theoretic Metric Learning In this paper, we present an information-theoretic approach to learning a Mahalanobis distance function. We formulate the problem as that of minimizing the differential relative entropy between two multivariate Gaussians under constraints on the distance function. We express this problem as a particular Bregman optimization problem: that of minimizing the LogDet divergence subject to linear constraints. Our resulting algorithm has several advantages over existing methods. First, our method can handle a wide variety of constraints and can optionally incorporate a prior on the distance function. Second, it is fast and scalable. Unlike most existing methods, no eigenvalue computations or semi-definite programming are required. We also present an online version and derive regret bounds for the resulting algorithm. Finally, we evaluate our method on a recent error reporting system for software called Clarify, in the context of metric learning for nearest neighbor classification, as well as on standard data sets.
Top/Computer_Science/Machine_Learning/Gaussian_Processes	Bayesian inference and Gaussian processes
Top/Computer_Science/Machine_Learning/Gaussian_Processes	Interpreting Covariance Functions & Classification
Top/Computer_Science/Machine_Learning/Gaussian_Processes	Eigenfunctions & Approximation Methods
Top/Computer_Science/Machine_Learning/Gaussian_Processes	Learning Human Pose and Motion Models for Animation Computer animation is an extraordinarily labor-intensive process; obtaining high-quality motion models could make the process faster and easier. I will describe methods for learning models of human poses and motion from motion capture data. I will begin with a pose model based on the Gaussian Process Latent Variable Model (GPLVM), and the application of this model to Inverse Kinematics posing. I will then describe the Gaussian Process Dynamical Model (GPDM) for modeling motion dynamics. I may also mention a few other extensions to the GPLVM for modeling motion data. I will discuss the properties of these models (both good and bad) and potential directions for future work.
Top/Computer_Science/Machine_Learning/Gaussian_Processes	Impromptu session
Top/Computer_Science/Machine_Learning/Gaussian_Processes	MarkusSparse Grid Methods The search for interesting variable stars, the discovery of relations between geomorphological properties, satellite observations and mineral concentrations, and the analysis of biological networks all require the solution of a large number of complex learning problems with large amounts of data. A major computational challenge faced in these investigations is posed by the curse of dimensionality. A well known aspect of this curse is the exponential dependence of the size of regular grids on the dimension of the domain. This makes traditional finite element approaches infeasible for high-dimensional domains. It is less known that this curse also affects computations of radial basis function approximations -- in a slightly more subtle way. Sparse grid functions can deal with the major problems of the curse of dimensionality. As they are the superposition of traditional finite element spaces, many well-known algorithms can be generalized to the sparse grid context. Sparse grids have been successfully used to solve partial differential equations in the past and, more recently, have been shown to be competitive for learning problems as well. The talk will provide a general introduction to the major properties of sparse grids and will discuss connections with kernel based methods and parallel learning algorithms. It will conclude with a short review over some recent work on algorithms based on the combination technique.
Top/Computer_Science/Machine_Learning/Gaussian_Processes	Learning with Gaussian Processes This presentation describes the basic foundations and advanced theory of Gaussian processes.
Top/Computer_Science/Machine_Learning/Gaussian_Processes	Probabilistic user interfaces Gaussian process priors have recently been applied to control problems. The GPs bring advantages in their representation of model prediction uncertainty, and because the derivative of a Gaussian process is a Gaussian process, they can also incorporate derivative information, and analytically provide the uncertainty of model derivatives. This can be used to bring a natural regularization of control effort, resulting in appropriately cautious control. It can also be used to provide a quickened display, which takes account of model uncertainty. I will describe our work in ambiguous or probabilistic user interfaces, and demonstrate some of the techniques we have developed for combining probabilistic models with continuous control and for providing feedback of system uncertainty to the user.
Top/Computer_Science/Machine_Learning/Markov_Processes	Constrained Hidden Markov Models for Population-based Haplotyping Analysis of genetic variation in human populations is critical to the understanding of the genetic basis for complex diseases. Although genomes of several species have been sequenced, it is still too expensive to sequence genomes of several individuals to analyze genetic variation. Furthermore, most of the genome is invariant among individuals.
Top/Computer_Science/Machine_Learning/Markov_Processes	Efficient max-margin Markov learning via conditional gradient and probabilistic inference We present a general and efficient optimisation methodology for for max-margin sructured classification tasks. The efficiency of the method relies on the interplay of several techiques: marginalization of the dual of the structured SVM, or max-margin Markov problem; partial decomposition via a gradient formulation; and finally tight coupling of a max-likelihood inference algorithm into the optimization algorithm, as opposed to using inference as a working set maintenance mechanism only.
Top/Computer_Science/Machine_Learning/Markov_Processes	Markov Chain Monte Carlo Methods 0. A fundamental theorem of simulation 1. Markov chain basics 2. Slice sampling 3. Gibbs sampling 4. Metropolis-Hastings algorithms 5. Variable dimension models and reversible jump MCMC 6. Perfect sampling 7. Adaptive MCMC and population Monte Carlo
Top/Computer_Science/Machine_Learning/Markov_Processes	On Max-Margin Markov Networks in Hierarchical Document Classification
Top/Computer_Science/Machine_Learning/Markov_Processes	Parameter Learning for Loopy Markov Random Fields with Structural Support Vector Machines
Top/Computer_Science/Machine_Learning/Markov_Processes	Suffix tree and Hidden Markov techniques for pattern analysis Suffix tree construction. Mention the new linear time array constructions - - using suffix trees for finding motifs with gaps (some new observations: 0.5 - 1 hours). - finding cis-regulatory motifs by comparative genomics (1 hour) - Hidden Markov techniques for haplotyping
Top/Computer_Science/Machine_Learning/Markov_Processes	A Tutorial Introduction to Stochastic Differential Equations: Continuous-time Gaussian Markov Processes
Top/Computer_Science/Machine_Learning/Markov_Processes	Sequential Monte Carlo methods Parts 4 and 5 of this lecture are presented in [[mlss07_davy_smcmc|//Manuel Davy's// '%title']]
Top/Computer_Science/Machine_Learning/Markov_Processes	System Identification of Enzymatic Control Processes Using Population Monte Carlo Methods We demonstrate the superiority of Population Monte Carlo techniques over standard Metropolis Markov Chain Monte Carlo (MCMC) methods for inferring optimal parameters for a particular mechanistic model of a biological process given noisy experimental data. As our understanding of biological processes increases, the proposed models to describe them become more complex. With such potentially large numbers of equations and parameters, it is no longer feasible to hand-pick parameter values and be sure that the most appropriate values have been chosen. Monte Carlo methods are becoming more widely used for estimating parameter values, however we show that the standard Metropolis MCMC approach fails to converge on optimal values for even relatively simple models and that a more sophisticated method, in the form of non-Markovian Population Monte Carlo, may be successfully employed to produce consistent and accurate results. We illustrate the basic problem using the minimal model for the circadian genetic network in Arabidopsis thaliana, which consists of 3 linked differential equations containing a total of 6 parameters, with an additional noise parameter incorporated to estimate the variance of noise in the data. Joint work with Mark Girolami.
Top/Computer_Science/Machine_Learning/Markov_Processes	Path Integral Method for Estimation of Time Series
Top/Computer_Science/Machine_Learning/Markov_Processes	Variational Bayes for Continuous-time Nonlinear State-space Models
Top/Computer_Science/Machine_Learning/Markov_Processes	Machine learning for access and retrieval II
Top/Computer_Science/Machine_Learning/Markov_Processes	S-SEER: A Multimodal Office Activity Recognition System with Selective Perception I will present the use of layered probabilistic representations for modeling the activities of people in a system named S-SEER. I will describe how we use the representation to do sensing, learning and inference at multiple levels of temporal granularity and abstraction. The approach centers on the use of a cascade of Hidden Markov Models (HMMs) named Layered Hidden Markov Models (LHMMs) to diagnose states of a user's activity based on real-time streams of evidence from video, audio and computer (keyboard and mouse) interactions.
Top/Computer_Science/Machine_Learning/Markov_Processes	Variational Inference for Markov Jump Processes Markov jump processes (MJPs) underpin our understanding of many important systems in science and technology. They provide a rigorous probabilistic framework to model the joint dynamics of groups (species) of interacting individuals, with applications ranging from information packets in a telecommunications network to epidemiology and population levels in the environment. These processes are usually non-linear and highly coupled, giving rise to non-trivial steady states (often referred to as emerging properties). Unfortunately, this also means that exact statistical inference is unfeasible and approximations must be made in the analysis of these systems. A traditional approach, which has been very successful throughout the past century, is to ignore the discrete nature of the processes and to approximate the stochastic process with a deterministic process whose behaviour is described by a system of non-linear, coupled ODEs. This approximation relies on the stochastic fluctuations being negligible compared to the average population counts. There are many important situations where this assumption is untenable: for example, stochastic fluctuations are reputed to be responsible for a number of important biological phenomena, from cell differentiation to pathogen virulence. Researchers are now able to obtain accurate estimates of the number of macromolecules of a certain species within a cell, prompting a need for practical statistical tools to handle discrete data.
Top/Computer_Science/Machine_Learning/Markov_Processes	Book-Adaptive and Book-Dependent Models to Accelerate Digitization of Early Music Optical music recognition (OMR) enables early music collections to be digitized on a large scale. The workflow for such digitisation projects also includes scanning and preprocessing, but the cost of expert human labour to correct automatic recognition errors dominates the cost of these other two steps. To reduce the number of recognition errors in the OMR process, we present an innovative application of maximum a posteriori (MAP) adaptation for hidden Markov models (HMMs) to build book-adaptive models, taking advantage of the new learning data generated from human editing work, which is part of any music digitization project. We also experimented with using the generated data to build book-dependent models from scratch, which sometimes outperform the book-adaptive models after enough corrected data is available. Our experiments show that these approaches can reduce human editing costs by more than half and that they are especially well suited to highly variable sources like early or degraded documents.
Top/Computer_Science/Machine_Learning/Markov_Processes	Separating Precision and Mean in Dirichlet-enhanced High-order Markov Models
Top/Computer_Science/Machine_Learning/Markov_Processes	Splice form prediction using Machine Learning Accurate ab initio gene finding is still a major challenge in computational biology. We employ state-of-the-art machine learning techniques based on Hidden Semi-Markov-SVMs to assay and improve the accuracy of genome annotations. We applied our system, called mSplicer, on the Caenorhabditis elegans genome and were able to drastically improve its annotation.
Top/Computer_Science/Machine_Learning/Markov_Processes	On the Adequacy of Baseform Pronunciations and Pronunciation Variants This paper presents an approach to automatically extract and evaluate the ``stability'' of pronunciation variants (i.e., adequacy of the model to accommodate this variability), based on multiple pronunciations of each lexicon words and the knowledge of a reference baseform pronunciation. Most approaches toward modelling pronunciation variability in speech recognition are based on the inference (through an ergodic HMM model) of a pronunciation graph (including all pronunciation variants), usually followed by a smoothing (e.g., Bayesian) of the resulting graph.
Top/Computer_Science/Machine_Learning/Markov_Processes	Bayesian Inference of Mechanistic Systems Models Using Population MCMC We demonstrate how Population Markov Chain Monte Carlo techniques may be used to sample from the complex posterior distributions which arise when estimating parameters over nonlinear mechanistic mathematical models of biological processes given noisy data. Further, we show how the samples obtained may be employed, using a Power Posteriors method, to accurately calculate the marginal likelihoods and Bayes factors over such models.
Top/Computer_Science/Machine_Learning/Markov_Processes	Tandem Connectionist Feature Extraction for Conversational Speech Recognition Multi-Layer Perceptrons (MLPs) can be used in automatic speech recognition in many ways. A particular application of this tool over the last few years has been the Tandem approach, as described by Hermansky et al in a number of publications. Here we discuss the characteristics of the MLP-based features used for the Tandem approach, and conclude with a report on their application to conversational speech recognition. The paper shows that MLP transformations yield variables that have regular distributions, which can be further modified by using logarithm to make the distribution easier to model by a Gaussian-HMM. Two or more vectors of these features can easily be combined without increasing the feature dimension. We also report recognition results that show that MLP features can significantly improve recognition performance for the NIST 2001 Hub-5 evaluation set with models trained on the Switchboard Corpus, even for complex systems incorporating MMIE training and other enhancements.
Top/Computer_Science/Robotics	Architecture for humananoid robots: emulation of biological processes To achieve better understanding of the processing of information with humans, which is processing in the brain, we can help with the construction and study of robotic systems which are human like. Three views to this topic will be presented in this lecture; behaviorist, integration of systems, the organization of flow and processing of information. The suggested architecture is based on a decentralized and cooperative processing of information. These processes can be serial or parallel, with forward and backward links. From here a complex network between interlinked and dependent process elements can be established. On the basis of the sensory input and output movements, the system generates more complex operations and decisions. The results of the research which will be presented during the lecture are based on the research in many other disciplines: humanoid robotics for integration and implementation of the system, neuroscience for the organization of the flux in the processing of information and nevropsychology for the evaluation of the systems behavior.
Top/Computer_Science/Robotics	A service robot named Markovito This shows the Peoplebot Markovito as it delivers messages and objects between offices. It can perform speech communication, face recognization, global localization, uses a probablistic grid map, and is controlled by a Factored MDP.
Top/Computer_Science/Robotics	Autonomous robot cleaning crew De-centralized collaborative planning and simulation demo for coordinating agent/robot tasks.
Top/Computer_Science/Robotics	How to say 'No' to a robot This describes an integrated robotic system for spatial understanding and situated interaction in indoor environments. Robot communication is performed using only natural language, but sometimes it needs more than a 'natural' language to understand.
Top/Computer_Science/Robotics	Morphogenesis: Shaping swarms of intelligent robots * Describes, simulates, and demonstrates in hardware the utility of (rule-based) morphogenesis for shaping robot swarms. * For more videos, pictures, and information on Morphogenesis and Morphology Control, see the following site: [[http://iridia.ulb.ac.be/supp/IridiaSupp2007-003/index.html|Photos, videos and information]] * You can also download a high-quality version of this video in various formats from: [[http://iridia.ulb.ac.be/%7Ealyhne/aaai-07/index.html|HQ version of the video]] * You can check the following web sites if you want to know more about the robots, swarm robotics, and swarm intelligence: [[http://www.swarmanoid.com|The Swarmanoid Project web-page]], [[http://www.swarm-bots.org|The Swarm-bots Project web-page]] ;Authors web pages: :[[http://iridia.ulb.ac.be/%7Ealyhne|Anders Lyhne Christensen's homepage]] :[[http://iridia.ulb.ac.be/%7Erogrady|Rehan O'Grady's homepage]] :[[http://iridia.ulb.ac.be/%7Emdorigo/|Marco Dorigo's homepage]]
Top/Computer_Science/Robotics	Multimodal Interactive Robot Agent (MIRA robot head) A short video demonstrating the speech communication, reasoning abilities, and humour of MIRA.
Top/Computer_Science/Robotics	NERO 2.0: Neuro Evolving Robotic Operatives This demonstrates the NERO real-time strategy game and the capabilities of its agents. The technology involves neuroevolution.
Top/Computer_Science/Robotics	One Class formula - Human Robot interaction
Top/Computer_Science/Robotics	Parallel Manipulators in Biomechanics and Robotics In the lecture we will be shown the newest discoveries on the field of parallel manipulators. Parallel manipulators are mechanisms which contain more closed, parallel, cinematic loops, where only few are driven by motors. The attention will be focused on the influence of different parameters; air at the joints, constructional defects, cinematic singularities and isotropy, the activity of the manipulator especially from the biomechanics and robotic point of view.
Top/Computer_Science/Robotics	Patient-Cooperative Rehabilitation Robotics in Zurich
Top/Computer_Science/Robotics	Robot Swarm localization using trilateration A description and demonstration of a robust approach for ground robot formation movement behaviors.
Top/Computer_Science/Robotics	Sme-Service Networks For Cooperative Operation Of Robot Installations
Top/Computer_Science/Robotics	The Centibots 100 Robot Project The Centibots system was a multi-robotic system developed in part by SRI. Its team of 100 small robots were built from off-the-shelf components. This video describes the distributed robot control software and subsequent demonstration.
Top/Computer_Science/Robotics	Two-on-two robot soccer A short demo of two-on-two robotic soccer, featuring the Cornell team's legacy and current players/robots.
Top/Computer_Science/Robotics	Artificial intelligence: An instance of Aibo ingenuity This describes research related to using RL for, among other tasks, learning behaviors for an Aibo robot.
Top/Computer_Science/Robotics	Autonomous UAV capabilities This short demo summarizes the capabilities of some autonomous unmanned air vehicles/helos, flying outdoors, that do not use GPS navigation techniques.
Top/Computer_Science/Robotics	Autonomous UAV search and rescue This longer demo films the application of some autonomous unmanned air vehicles/helos, flying outdoors, for a search and rescue operation.
Top/Computer_Science/Robotics	Leonardo: Goal assistance with divergent beliefs This demonstrates the robot Leonardo's ability to reason and act competently in situations when the other (in this case, human) agents have different belief states.
Top/Computer_Science/Robotics	Map Building without Localization by Dimensionality Reduction Techniques This paper proposes a new map building framework for mobile robot named Localization-Free Mapping by Dimensionality Reduction (LFMDR). In this framework, the robot map building is interpreted as a problem of reconstructing the 2-D coordinates of ob jects so that they maximally preserve the local proximity of the ob jects in the space of robot's observation history. Not only traditional linear PCA but also recent manifold learning techniques can be used for solving this problem. In contrast to the SLAM framework, LFMDR framework does not require localization procedures nor explicit measurement and motion models. In the latter part of this paper, we will demonstrate 'visibility-only' and 'bearing-only' localization-free mappings which are derived by applying LFMDR framework to the visibility and bearing measurements respectively.
Top/Computer_Science/Robotics	Humanoids for autonomous operations The video describes a Humanoid robotics project at JPL, claiming a first practical application of humanoid robotics.
Top/Computer_Science/Robotics	Motion planning of multiple agents in virtual environments Describes and demonstrates in simulation the use of coordination graphs to avoid collisions of multiple agents in tasks requiring motion of multiple agents.
Top/Computer_Science/Robotics	Distributive Adaptive Control: A Real-world Cognitive Architecture applied to Robots, Spaces and Avatares
Top/Computer_Science/Robotics	Probabilistic inference methods in robotics-filling the gap between high-level reasoning and low-level motion control
Top/Computer_Science/Semantic_Web/Annotation	Research 16: Tree-structured Conditional Random Fields for Semantic Annotation
Top/Computer_Science/Semantic_Web/Annotation	Semantic Annotation in the Alvis Project
Top/Computer_Science/Semantic_Web/Annotation	Semantic Authoring and Annotation: A Semi-Automatic Semantic Annotation and Authoring Tool for a Library Help Desk Service
Top/Computer_Science/Semantic_Web/Annotation	Semantic Desktop and Social Semantic Collaboration: PIMO Population and Semantic Annotation for the Gnowsis Semantic Desktop
Top/Computer_Science/Semantic_Web/Annotation	Semantic Authoring and Annotation: A Browser-based Tool for Collaborative Distributed Annotation for the Semantic Web
Top/Computer_Science/Semantic_Web/Annotation	Semantic Authoring and Annotation: Cross-media Document Annotation and Enrichment
Top/Computer_Science/Semantic_Web/Annotation	Semantic Authoring and Annotation: Using WEESA to Semantically Annotate Cocoon Web Applications
Top/Computer_Science/Semantic_Web/Annotation	Semantic Authoring and Annotation: Welcoming and Introduction
Top/Computer_Science/Semantic_Web/Annotation	Towards Trust for Semantic Web Annotations
Top/Computer_Science/Semantic_Web/Annotation	Tags and Dependencies: An integrated View of Document Annotation
Top/Computer_Science/Semantic_Web/Annotation	Semantic Annotated La Tex
Top/Computer_Science/Semantic_Web/Annotation	SASA - A Semi Automatic Semantic Annotator for Personal Knowledge Management
Top/Computer_Science/Semantic_Web/Annotation	Building blocks for semantic search engines: Ranking and compact indexing in entity-relation graphs We see an evolutionary path to supporting semantic search over text facilitated by 1. extractors and annotators for ever-growing collections of entity and relation types and 2. search systems that exploit a smooth continuum between structured entities and relations on one hand and uninterpreted text on the other. The extractors and annotators will be imperfect and incomplete.
Top/Computer_Science/Semantic_Web/Annotation	Can Social Bookmarks Improve Web Search? Social bookmarking is a recent phenomenon which has the potential to give us a great deal of data about pages on the web. One major question is whether that data can be used to augment systems like web search. To answer this question, over the past year we have gathered what we believe to be the largest dataset from a social bookmarking site yet analyzed by academic researchers. Our dataset represents about forty million bookmarks from the social bookmarking site del.icio.us. We contribute a characterization of posts to del.icio.us: how many bookmarks exist (about 115 million), how fast is it growing, and how active are the URLs being posted about (quite active). We also contribute a characterization of tags used by bookmarkers. We found that certain tags tend to gravitate towards certain domains, and vice versa. We also found that tags occur in over 50 percent of the pages that they annotate, and in only 20 percent of cases do they not occur in the page text, backlink page text, or forward link page text of the pages they annotate. We conclude that social bookmarking can provide search data not currently provided by other sources, though it may currently lack the size and distribution of tags necessary to make a significant impact.
Top/Computer_Science/Semantic_Web/Annotation	An Infrastructure for Acquiring High Quality Semantic Metadata
Top/Computer_Science/Computer_Vision	Exploring human object-vision with hi-res fMRI and information-based analysis
Top/Computer_Science/Computer_Vision	Computer Vision
Top/Computer_Science/Computer_Vision	Graph-Based Perceptual Segmentation of Stereo Vision 3D Images at Multiple Abstraction Levels This paper presents a new technique based on perceptual information for the robust segmentation of noisy 3D scenes acquired by stereo vision. A low-pass geometric lter is rst applied to the given cloud of 3D points to remove noise. The tensor voting algorithm is then applied in order to extract perceptual geometric information. Finally, a graph-based segmenter is utilized for extracting the dierent geometric structures present in the scene through a region-growing procedure that is applied hierarchically. The proposed algorithm is evaluated on real 3D scenes acquired with a trinocular camera.
Top/Computer_Science/Computer_Vision	Machine Learning in Vision
Top/Computer_Science/Computer_Vision	Stereo Vision for Obstacle Detection: a Graph-Based Approach We propose a new approach to stereo matching for obstacle detection in the autonomous navigation framework. An accurate but slow reconstruction of the 3D scene is not needed; rather, it is more important to have a fast localization of the obstacles to avoid them. All the methods in the literature, based on a punctual stereo matching, are ineffective in realistic contexts because they are either computationally too expensive, or unable to deal with the presence of uniform patterns, or of perturbations between the left and right images. Our idea is to face the stereo matching problem as a matching between homologous regions. The stereo images are represented as graphs and a graph matching is computed to find homologous regions. Our method is strongly robust in a realistic environment, requires little parameter tuning, and is adequately fast, as experimentally demonstrated in a comparison with the best algorithms in the literature.
Top/Computer_Science/Computer_Vision	Using computer vision in a rehabilitation method of a human hand
Top/Computer_Science/Computer_Vision	TecnoVision-ROBIN: benchmarking object retrieval algorithms Technovision is a recent program of the French Ministry of Research and Technology that will fund evaluation projects in the area of computer vision. Many vision algorithms have been proposed in the past, but comparing their performance has been difficult owing to the lack of common datasets. Technovision aims to correct this by funding the creation of large, representative image datasets. ROBIN is a Technovision proposal covering the evaluation of object retrieval algorithms
Top/Computer_Science/Computer_Vision	Winning The DARPA Grand Challenge The DARPA grand challenge, technical details enabling Sebastian Thrun's&#160;win, and an introduction to the next phase called 'The Urban Grand Challenge'
Top/Computer_Science/Computer_Vision	Learning and Recognizing Visual Object Categories Over the past few years there has been substantial progress in the development of techniques for recognizing generic categories of objects in images, such as automobiles, bicycles, airplanes, and human faces. Much of this progress can be traced to two underlying technical advances: # detectors for locally invariant features of an image, and # the application of techniques from machine learning. Despite recent successes, however, there are some fundamental concerns about methods that rely heavily on feature detection, because the local image evidence used in detection decisions is often highly ambiguous due to the absence of contextual information. We are taking a different approach to learning and recognizing visual object categories, in which there is no separate feature detection stage. In our approach, objects are modeled as local image patches with spring-like connections that constrain the spatial relations between patches. Such models are intuitively natural, and their use dates back over 30 years. Until recently such models were largely abandoned due to computational challenges that are addressed by our work. Our approach can be used to learn models from weakly labeled training data, without any specification of the location of objects or their parts. The recognition accuracy for such models is better than when using techniques based on feature detection that encode similar forms of spatial constraint.
Top/Computer_Science/Computer_Vision	Topics in image and video processing
Top/Computer_Science/Computer_Vision	Learning Graph Matching As a fundamental problem in pattern recognition, graph matching has found a variety of applications in the field of computer vision. In graph matching, patterns are modeled as graphs and pattern recognition amounts to finding a correspondence between the nodes of different graphs. There are many ways in which the problem has been formulated, but most can be cast in general as a quadratic assignment problem, where a linear term in the objective function encodes node compatibility functions and a quadratic term encodes edge compatibility functions. The main research focus in this theme is about designing efficient algorithms for solving approximately the quadratic assignment problem, since it is NP-hard. In this paper, we turn our attention to the complementary problem: how to estimate compatibility functions such that the solution of the resulting graph matching problem best matches the expected solution that a human would manually provide. We present a method for learning graph matching: the training examples are pairs of graphs and the labels are matchings between pairs of graphs. We present experimental results with real image data which give evidence that learning can improve the performance of standard graph matching algorithms. In particular, it turns out that linear assignment with such a learning scheme may improve over state-of-the-art quadratic assignment relaxations. This finding suggests that for a range of problems where quadratic assignment was thought to be essential for securing good results, linear assignment, which is far more ef icient, could be just sufficient if learning is performed. This enables speed-ups of graph matching by up to 4 orders of magnitude while retaining state-of-the-art accuracy.
Top/Computer_Science/Computer_Vision	Facial expression recognition and emotion recognition from speech The presentation tackles the problem of recognizing the emotions based on video and audio data analysis. A fully automatic facial expression recognition system is based on three components: face detection, facial characteristic point extraction and classification. Face detection is employed by boosting simple rectangle Haar-like features that give a decent representation of the face. These features also allow the differentiation between a face and a non-face. The boosting algorithm is combined with an Evolutionary Search to speed up the overall search time. Facial characteristic points (FCP) are extracted from the detected faces. The same technique applied on faces is utilized for this purpose. Additionally, FCP extraction using corner detection methods and brightness distribution has also been considered. Finally, after retrieving the required FCPs the emotion of the facial expression can be determined.
Top/Computer_Science/Computer_Vision	101 Visual object classes - Introduction
Top/Computer_Science/Computer_Vision	Qualitative Spatial Relationships for Image Interpretation by using Semantic Graph In this paper, a new way to express complex spatial relations is proposed in order to integrate them in a Constraint Satisfaction Problem with bilevel constraints. These constraints allow to build semantic graphs, which can describe more precisely the spatial relations between subparts of a composite object that we look for in an image. For example, it allows to express complex spatial relations such as is surrounded by. This approach can be applied to image interpretation and some examples on real images are presented.
Top/Computer_Science/Computer_Vision	Feature extraction & content description I
Top/Computer_Science/Computer_Vision	Introduction to Multimedia Digital Libraries
Top/Computer_Science/Computer_Vision	Learning shared representations for object recognition
Top/Computer_Science/Computer_Vision	Energy-based models & Learning for Invariant Image Recognition
Top/Computer_Science/Computer_Vision	Learning Visual Distance Function for Object Identification from one Example Comparing images is essential to several computer vision problems, like image retrieval or object identification. The comparison of two images heavily relies on the definition of a good distance function. Standard functions (e.g. the euclidean distance in the original feature space) are too generic and fail to encode the domain specific information. In this paper, we propose to learn a similarity measure specific to a given category (e.g. cars). This distance is learned from a training set of pairs of images labeled same or different, indicating if the two images represent the same object (e.g. same car model) or not. After learning, this measure is used to predict how similar two images of never seen objects are.
Top/Computer_Science/Computer_Vision	Image Classification Using Marginalized Kernels for Graphs We propose in this article an image classification technique based on kernel methods and graphs. Our work explores the possibility of applying marginalized kernels to image processing. In machine learning, performant algorithms have been developed for data organized as real valued arrays; these algorithms are used for various purposes like classification or regression. However, they are inappropriate for direct use on complex data sets. Our work consists of two distinct parts. In the first one we model the images by graphs to be able to represent their structural properties and inherent attributes. In the second one, we use kernel functions to project the graphs in a mathematical space that allows the use of performant classification algorithms. Experiments are performed on medical images acquired with various modalities and concerning di_erent parts of the body.
Top/Computer_Science/Computer_Vision	Adaptive Feature Selection in Image Segmentation Most practical image segmentation algorithms optimize some mathematical similarity criterion derived from several low-level image features. One possible way of combining different types of features, e.g. color- and texture features on different scales and/or different orientations, is to simply stack all the individual measurements into one high-dimensional feature vector. Due to the nature of such stacked vectors, however, only very few components (e.g. those which are defined on a suitable scale) will carry information that is relevant for the actual segmentation task. We present a novel approach to combining segmentation and feature selection that is capable of overcoming this relevance determination problem. It implements a wrapper strategy for feature selection, in the sense that the features are directly selected by optimizing thediscriminative power of the used partitioning algorithm. On the technical side, we present an efficient optimization algorithm with guaranteed local convergence property. All free model parameters of this method are selected by a resampling-based stability analysis. Experiments for both toy examples and real-world images demonstrate that the built-in feature selection mechanism leads to stable and meaningful partitions of the images.
Top/Computer_Science/Computer_Vision	Visual Categorization with Bags of Keypoints We present a novel method for generic visual categorization: the problem of identifying the object content of natural images while generalizing across variations inherent to the object class. This bag of keypoints method is based on vector quantization of affine invariant descriptors of image patches. We propose and compare two alternative implementations using different classifiers: Naive Bayes and SVM. The main advantages of the method are that it is simple, computationally efficient and intrinsically invariant. We present results for simultaneously classifying several semantic visual categories. These results clearly demonstrate that the method is robust to background clutter and produces good categorization accuracy even without exploiting geometric information.
Top/Computer_Science/Computer_Vision	Object Identification by Statistical Methods Numerical data fusion or merging of overlapping data files becomes a hard problem if no global unique identifying keys exist in the corresponding data sets. Typical examples are the linkage of address files supplied from different sources for commercial purposes - a money making area-, the merging of special offers in various media (cf. duplicate detection), or an administrative record census (ARC) as planed in Germany, where several autonomous, heterogeneous registers are to be merged. We present a three-step procedure consisting of the steps conversion of attributes, comparison of values of a pair of objects, and classification ('matching problem') of pairs either as 'same' or 'matched and 'not same' or 'not matched'. We pay special attention to the quality and the efficiency of the methodology. We briefly discuss questions like correctness and completeness as well as pre-selection techniques like 'blocking' to reduce the computational complexity of pairwise comparisons. The approach is illustrated on data from carefully composed benchmark data sets. We assume some basic knowledge in computer science and classification (supervised learning).
Top/Computer_Science/Computer_Vision	Projective Kalman Filter: Multiocular Tracking of 3D Locations Towards Scene Understanding This paper presents a novel approach to the problem of estimating and tracking 3D locations of multiple targets in a scene using measurements gathered from multiple calibrated cameras. Estimation and tracking is jointly achieved by a newly conceived computational process, the Projective Kalman lter (PKF), allowing the problem to be treated in a single, unied framework. The projective nature of observed data and information redundancy among views is exploited by PKF in order to overcome occlusions and spatial ambiguity. To demonstrate the eectiveness of the proposed algorithm, the authors present tracking results of people in a SmartRoom scenario and compare these results with existing methods as well.
Top/Computer_Science/Computer_Vision	Graph Based Shapes Representation and Recognition In this paper, we propose to represent shapes by graphs. Based on graphic primitives extracted from the binary images, attributed relational graphs were generated. Thus, the nodes of the graph represent shape primitives like vectors and quadrilaterals while arcs describing the mutual primitives relations. To be invariant to transformations such as rotation and scaling, relative geometric features extracted from primitives are associated to nodes and edges as attributes. Concerning graph matching, due to the fact of NP-completeness of graph-subgraph isomorphism, a considerable attention is given to different strategies of inexact graph matching. We also present a new scoring function to compute a similarity score between two graphs, using the numerical values associated to the nodes and edges of the graphs. The adaptation of a greedy graph matching algorithm with the new scoring function demonstrates significant performance improvements over traditional exhaustive searches of graph matching.
Top/Computer_Science/Computer_Vision	Visual Categorization with Bags of Keypoints We present a novel method for generic visual categorization: the problem of identifying the object content of natural images while generalizing across variations inherent to the object class. This bag of keypoints method is based on vector quantization of affine invariant descriptors of image patches. We propose and compare two alternative implementations using different classifiers: Nave Bayes and SVM. The main advantages of the method are that it is simple, computationally efficient and intrinsically invariant. We present results for simultaneously classifying several semantic visual categories. These results clearly demonstrate that the method is robust to background clutter and produces good categorization accuracy even without exploiting geometric information.
Top/Computer_Science/Computer_Vision	S-SEER: A Multimodal Office Activity Recognition System with Selective Perception I will present the use of layered probabilistic representations for modeling the activities of people in a system named S-SEER. I will describe how we use the representation to do sensing, learning and inference at multiple levels of temporal granularity and abstraction. The approach centers on the use of a cascade of Hidden Markov Models (HMMs) named Layered Hidden Markov Models (LHMMs) to diagnose states of a user's activity based on real-time streams of evidence from video, audio and computer (keyboard and mouse) interactions.
Top/Computer_Science/Computer_Vision	Toward Learning Mixture-of-Parts Pictorial Structures
Top/Computer_Science/Computer_Vision	Color Image Segmentation: Kernel Do the Feature Space
Top/Computer_Science/Computer_Vision	Graph-based Methods for Retinal Mosaicing and Vascular Characterization In this paper, we propose a highly robust point-matching method (Graph Transformation Matching - GTM) relying on finding the consensus graph emerging from putative matches. Such method is a two- phased one in the sense that after finding the consensus graph it tries to complete it as much as possible. We successfully apply GTM to image registration in the context of finding mosaics from retinal images. Feature points are obtained after properly segmenting such images. In addition, we also introduce a novel topological descriptor for quantifying disease by characterizing the arterial/venular trees. Such descriptor relies on diffusion kernels on graphs. Our experiments have showed only statistical signifficance for the case of arterial trees, which is consistent with previous findings.
Top/Computer_Science/Computer_Vision	Research 17: Integrating and Querying Parallel Leaf Shape Descriptions
Top/Computer_Science/Computer_Vision	Feature extraction & content description II
Top/Computer_Science/Computer_Vision	Computing Homology Group Generators of Images Using Irregular Graph Pyramids We introduce a method for computing homology groups and their generators of a 2D image, using a hierarchical structure i.e. irregular graph pyramid. Starting from an image, a hierarchy of the image is built, by two operations that preserve homology of each region. Instead of computing homology generators in the base where the number of entities (cells) is large, we first reduce the number of cells by a graph pyramid. Then homology generators are computed efficiently on the top level of the pyramid, since the number of cells is small, and a top down process is then used to deduce homology generators in any level of the pyramid, including the base level i.e. the initial image. We show that the new method produces valid homology generators and present some experimental results.
Top/Computer_Science/Computer_Vision	Detecting Motifs Under Uniform Scaling Time series motifs are approximately repeated patterns found within the data. Such motifs have utility for many data mining algorithms, including rule-discovery, novelty-detection, summarization and clustering. Since the formalization of the problem and the introduction of efficient linear time algorithms, motif discovery has been successfully applied to many domains, including medicine, motion capture, robotics and meteorology. In this work we show that most previous applications of time series motifs have been severely limited by the definitions brittleness to even slight changes of uniform scaling, the speed at which the patterns develop. We introduce a new algorithm that allows discovery of time series motifs with invariance to uniform scaling, and show that it produces objectively superior results in several important domains. Apart from being more general than all other motif discovery algorithms, a further contribution of our work is that it is simpler than previous approaches, in particular we have drastically reduced the number of parameters that need to be specified.
Top/Computer_Science/Computer_Vision	Applications to Machine Vision This presentation describes novel approaches to spatial inference problems in vision and image processing. Markov random field models are described for image restoration, foreground segmentation, graph cutting and stereo matching.
Top/Computer_Science/Computer_Vision	Generative Models for Visual Objects and Object Recognition via Bayesian Inference
Top/Computer_Science/Computer_Vision	Learning in Computer Vision This tutorial he will cover some of the core fundamentals in vision and demonstrate how they can be interpreted in terms of machine learning fundamentals. Unbeknownst to most researchers in the field of machine learning, the fundamentals of object registration and tracking such as optical flow, interest descriptors (e.g., SIFT), segmentation and correlation filters are inherently related to the learning topics of regression, regularization, graphical models, generative models and discriminative models. As a result many aspects of vision can be interpreted as applied forms of learning. From this discussion on fundamentals we shall also explore advanced topics in object registration and tracking such as non-rigid object alignment/ tracking and non-rigid structure from motion and how the application of machine learning is continuing to improve these technologies.
Top/Computer_Science/Computer_Vision	Recognising Animals
Top/Computer_Science/Computer_Vision	Grouping Using Factor Graphs: an Approach for Finding Text with a Camera Phone We introduce a new framework for feature grouping based on factor graphs, which are graphical models that encode interactions among arbitrary numbers of random variables. The ability of factor graphs to express interactions higher than pairwise order (the highest order encountered in most graphical models used in computer vision) is useful for modeling a variety of pattern recognition problems. In particular, we show how this property makes factor graphs a natural framework for performing grouping and segmentation, which we apply to the problem of finding text in natural scenes. We demonstrate an implementation of our factor graph-based algorithm for finding text on a Nokia camera phone, which is intended for eventual use in a camera phone system that finds and reads text (such as street signs) in natural environments for blind users.
Top/Computer_Science/Computer_Vision	The UoS LAVA group Approach to Generic Image Categorisation
Top/Computer_Science/Computer_Vision	Learning Sprites A simple and efficient way to model much image and video data is to decompose it into a set of 2-dimensional objects in layers. Each object is characterized by its shape and appearance (as with a 'sprite' in computer graphics). Following earlier work on layer decompositions in computer vision (e.g. Wang and Adelson, 1994), Frey and Jojic (1999) stated the sprite-learning problem in terms of transformation-invariant clustering using mixture models and EM. This was later extended (Jojic and Frey, 2001) to learning multiple sprites/objects from a video sequence. The approach of building in knowledge about allowable transformations into the clustering algorithm is an important way that a machine learning algorithm (clustering) needs to be tailored to the computer vision domain. Frey and Jojic's approach to learning multiple sprites uses variational inference simultaneously on all sprites; we also discuss recent work by Williams and Titsias (2004) who describe a greedy sequential algorithm for this task.
Top/Computer_Science/Computer_Vision	The Future of Image Search There are billions of images on the Internet. Today, searching for a desired image is largely based on textual data such as filename or associated text on the web page; not much use is made of the image content. There are good reasons for this. The field of content-based image retrieval, which emerged during the 1990s, focused primarily on color and texture cues. These were easier to model than shape, but they turned out to be much less useful than originally hoped. I shall review some of the recent developments in the field of visual object recognition in the computer vision community that offer greater promise. Much better image features for characterizing shape, advances in machine learning techniques, and the availability of large amounts of training data lie at the heart of these approaches.
Top/Computer_Science/Computer_Vision	Probabilistic models for understanding images Getting a computer to understand an image is challenging due to the numerous sources of variability that influence the imaging process. The pixels of a typical photograph will depend on the scene type and geometry, the number, shape and appearance of objects present in the scene, their 3D positions and orientations, as well as effects such as occlusion, shading and shadows. The good news is that research into physics and computer graphics has given us a detailed understanding of how these variables affect the resulting image. This understanding can help us to build the right prior knowledge into our probabilistic models of images. In theory, building a model containing all of this knowledge would solve the image understanding problem. In practice, such a model would be intractable for current inference methods. The open challenge for machine learning and machine vision researchers is to create a model which captures the imaging process as accurately as possible, whilst remaining tractable for accurate inference. To illustrate this challenge, I will show how different aspects of the imaging process can be incorporated into models for object detection and segmentation, and discuss techniques for making inference tractable in such models. **//Disclaimer:// Videolectures.Net emphasises that the quality of this video can not be improved, because of low light quality conditions provided in the lecture auditorium.**
Top/Computer_Science/Computer_Vision	Large-Scale Object Recognition Systems This paper introduces recent methods for large scale image search. State-of-the-art methods build on the bag-of-features image representation. We first analyze bag-of-features in the framework of approximate nearest neighbor search. This shows the sub-optimality of such a representation for matching descriptors and leads us to derive a more precise representation based on 1) Hamming embedding (HE) and 2) weak geometric consistency constraints (WGC). HE provides binary signatures that refine the matching based on visual words. WGC filters matching descriptors that are not consistent in terms of angle and scale. HE and WGC are integrated within the inverted file and are efficiently exploited for all images, even in the case of very large datasets. Experiments performed on a dataset of one million of images show a significant improvement due to the binary signature and the weak geometric consistency constraints, as well as their efficiency. Estimation of the full geometric transformation, i.e., a re-ranking step on a short list of images, is complementary to our weak geometric consistency constraints and allows to further improve the accuracy.
Top/Computer_Science/Machine_Learning/Linear_Models	Scalable Training of L1-regularized Log-linear Models The l-bfgs limited-memory quasi-Newton method is the algorithm of choice for optimizing the parameters of large-scale log-linear models with L2 regularization, but it cannot be used for an L1-regularized loss due to its non-differentiability whenever some parameter is zero. Eficient algorithms have been proposed for this task, but they are impractical when the number of parameters is very large. We present an algorithm OrthantWise Limited-memory Quasi-Newton (owlqn), based on l-bfgs, that can eficiently optimize the L1-regularized log-likelihood of log-linear models with millions of parameters. In our experiments on a parse reranking task, our algorithm was several orders of magnitude faster than an alternative algorithm, and substantially faster than lbfgs on the analogous L2-regularized problem. We also present a proof that owl-qn is guaranteed to converge to a globally optimal parameter vector.
Top/Computer_Science/Machine_Learning/Linear_Models	Structured Linear Models Over the last five years, we have been able to extend the theory of linear classifiers to structure prediction problems, combining the benefits of discriminative learning and of structured probabilistic models like hidden Markov models. I will review these models and their learning algorithms, and exemplify their use in text processing, with a focus on information extraction from biomedical text.
Top/Computer_Science/Machine_Learning/Linear_Models	Exponential Families In this introductory course we will discuss how log linear models can be extended to feature space. These log linear models have been studied by statisticians for a long time under the name of exponential family of probability distributions. We provide a unified framework which can be used to view many existing kernel algorithms as special cases. Our framework also allows us to derive many natural generalizations of existing algorithms. In particular, we show how we can recover Gaussian Processes, Support Vector Machines, multi-class discrimination, and sequence annotation (via Conditional Random Fields). We also show to deal with missing data and perform MAP estimation on Conditional Random Fields in feature space. The requisite background for the course will be covered briskly in the first two lectures. Knowledge of linear algebra and familiarity with functional analysis will be helpful.
Top/Computer_Science/Machine_Learning/Linear_Models	Exponential Families in Feature Space In this course I will discuss how exponential families, a standard tool in statistics, can be used with great success in machine learning to unify many existing algorithms and to invent novel ones quite effortlessly. In particular, I will show how they can be used in feature space to recover Gaussian Process classification for multiclass discrimination, sequence annotation (via Conditional Random Fields), and how they can lead to Gaussian Process Regression with heteroscedastic noise assumptions.
Top/Computer_Science/Machine_Learning/Linear_Models	Exponential Families in Feature Space In this introductory course we will discuss how log linear models can be extended to feature space. These log linear models have been studied by statisticians for a long time under the name of exponential family of probability distributions. We provide a unified framework which can be used to view many existing kernel algorithms as special cases. Our framework also allows us to derive many natural generalizations of existing algorithms. In particular, we show how we can recover Gaussian Processes, Support Vector Machines, multi-class discrimination, and sequence annotation (via Conditional Random Fields). We also show to deal with missing data and perform MAP estimation on Conditional Random Fields in feature space. The requisite background for the course will be covered briskly in the first two lectures. Knowledge of linear algebra and familiarity with functional analysis will be helpful.
Top/Computer_Science/Machine_Learning/Linear_Models	Exponential Families in Feature Space In this introductory course we will discuss how log linear models can be extended to feature space. These log linear models have been studied by statisticians for a long time under the name of exponential family of probability distributions. We provide a unified framework which can be used to view many existing kernel algorithms as special cases. Our framework also allows us to derive many natural generalizations of existing algorithms. In particular, we show how we can recover Gaussian Processes, Support Vector Machines, multi-class discrimination, and sequence annotation (via Conditional Random Fields). We also show to deal with missing data and perform MAP estimation on Conditional Random Fields in feature space. The requisite background for the course will be covered briskly in the first two lectures. Knowledge of linear algebra and familiarity with functional analysis will be helpful.
Top/Computer_Science/Machine_Learning/Linear_Models	Exponential Families in Feature Space - Part 5 In this introductory course we will discuss how log linear models can be extended to feature space. These log linear models have been studied by statisticians for a long time under the name of exponential family of probability distributions. We provide a unified framework which can be used to view many existing kernel algorithms as special cases. Our framework also allows us to derive many natural generalizations of existing algorithms. In particular, we show how we can recover Gaussian Processes, Support Vector Machines, multi-class discrimination, and sequence annotation (via Conditional Random Fields). We also show to deal with missing data and perform MAP estimation on Conditional Random Fields in feature space. The requisite background for the course will be covered briskly in the first two lectures. Knowledge of linear algebra and familiarity with functional analysis will be helpful.
Top/Computer_Science/Machine_Learning/Linear_Models	Exponential Families in Feature Space - Part 6 In this introductory course we will discuss how log linear models can be extended to feature space. These log linear models have been studied by statisticians for a long time under the name of exponential family of probability distributions. We provide a unified framework which can be used to view many existing kernel algorithms as special cases. Our framework also allows us to derive many natural generalizations of existing algorithms. In particular, we show how we can recover Gaussian Processes, Support Vector Machines, multi-class discrimination, and sequence annotation (via Conditional Random Fields). We also show to deal with missing data and perform MAP estimation on Conditional Random Fields in feature space. The requisite background for the course will be covered briskly in the first two lectures. Knowledge of linear algebra and familiarity with functional analysis will be helpful.
Top/Computer_Science/Machine_Learning/Linear_Models	Multiplicative Updates for L1-Regularized Linear and Logistic Regression Multiplicative update rules have proven useful in many areas of machine learning. Simple to implement, guaranteed to converge, they account in part for the widespread popularity of algorithms such as nonnegative matrix factorization and Expectation-Maximization. In this paper, we show how to derive multiplicative updates for problems in L1-regularized linear and logistic regression. For L1regularized linear regression, the updates are derived by reformulating the required optimization as a problem in nonnegative quadratic programming (NQP). The dual of this problem, itself an instance of NQP, can also be solved using multiplicative updates; moreover, the observed duality gap can be used to bound the error of intermediate solutions. For L1regularized logistic regression, we derive similar updates using an iteratively reweighted least squares approach. We present illustrative experimental results and describe efficient implementations for large-scale problems of interest (e.g., with tens of thousands of examples and over one million features).
Top/Computer_Science/Computer_Graphics	Adaptive Mesh Compression in 3D Computer Graphics using Multiscale Manifold Learning This paper investigates compression of 3D ob jects in computer graphics using manifold learning. Spectral compression uses the eigenvectors of the graph Laplacian of an object's topology to adaptively compress 3D objects. 3D compression is a challenging application domain: ob ject models can have > 105 vertices, and reliably computing the basis functions on large graphs is numerically challenging. In this paper, we introduce a novel multiscale manifold learning approach to 3D mesh compression using diffusion wavelets, a general extension of wavelets to graphs with arbitrary topology. Unlike the 'global' nature of Laplacian bases, diffusion wavelet bases are compact, and multiscale in nature. We decompose large graphs using a fast graph partitioning method, and combine local multiscale wavelet bases computed on each subgraph. We present results showing that multiscale diffusion wavelets bases are superior to the Laplacian bases for adaptive compression of large 3D ob jects.
Top/Computer_Science/Computer_Graphics	Trust building models and self-organizing systems / complexity theory
Top/Computer_Science/Computer_Graphics	Sparse Geometric Super-Resolution What is the maximum signal resolution that can be recovered from partial noisy or degraded data ? This inverse problem is a central issue, from medical to satellite imaging, from geophysical seismic to HDTV visualization of Internet videos. Increasing an image resolution is possible by taking advantage of 'geometric regularities', whatever it means. Super-resolution can indeed be achieved for signals having a sparse representation which is 'incoherent' relatively to the measurement system. For images and videos, it requires to construct sparse representations in redundant dictionaries of waveforms, which are adapted to geometric image structures. Signal recovery in redundant dictionaries is discussed, and applications are shown in dictionaries of bandlets for image super-resolution.
Top/Computer_Science/Computer_Graphics	3D Video: A Fusion of Graphics and Vision In recent years 3-dimensional video has received a significant attention both in research and in industry. Applications range from special effects in feature films to the analysis of sports events. 3D video is concerned with the computation of virtual camera positions and fly-throughs of a scene given multiple, conventional 2D video streams. The high-quality synthesis of such view-independent video representations poses a variety of technical challenges including acquisition, reconstruction, processing, compression, and rendering. In this talk I will outline the research in this area carried out at ETH over the past years. I will discuss various concepts for passive and active acquisition of 3D video using combinations of multiple cameras and projectors. Furthermore, I will address topics related to the representation and processing of the massive amount data arising from such multiple video streams. I will highlight the underlying technical concepts and algorithms that draw upon knowledge both from graphics and from vision. Finally I will demonstrate some commercial applications targeting at virtual replays for sports broadcasts.
Top/Computer_Science/Computer_Graphics	Exploring Semantic Social Networks using Virtual Reality We present Redgraph, the first generic virtual reality visualization program for Semantic Web data. Redgraph is capable of handling large data-sets, as we demonstrate on social network data from the U.S. Patent Trade Office. We develop a Semantic Web vocabulary of virtual reality terms compatible with GraphXML to map graph visualization into the Semantic Web itself. Our approach to visualizing Semantic Web data takes advantage of user-interaction in an immersive environment to bypass a number of difficult issues in 3-dimensional graph visualization layout by relying on users themselves to interactively extrude the nodes and links of a 2-dimensional graph into the third dimension. When users touch nodes in the virtual reality environment, they retrieve data formatted according to the datas schema or ontology. We applied Redgraph to social network data constructed from patents, inventors, and institutions from the United States Patent and Trademark Office in order to explore networks of innovation in computing. Using this data-set, results of a user study comparing extrusion (3-D) vs. no-extrusion (2-D) are presented. The study showed the use of a 3-D interface by subjects led to significant improvement on answering of fine-grained questions about the data-set, but no significant difference was found for broad questions about the overall structure of the data. Furthermore, inference can be used to improve the visualization, as demonstrated with a data-set of biotechnology patents and researchers.
Top/Computer_Science/Computer_Graphics	Fast Support Vector Machine Training and Classification on Graphics Processors Recent developments in programmable, highly parallel Graphics Processing Units (GPUs) have enabled high performance implementations of machine learning algorithms. We describe a solver for Support Vector Machine training, using Platt's Sequential Minimal Optimization algorithm and an adaptive first and second order working set selection heuristic, which achieves speedups of 9-35x over LIBSVM running on a traditional processor. We also present a GPU-based system for SVM classification which achieves speedups of 81-138x over LibSVM (5-24x over our own CPU-based SVM classifier).
Top/Arts/Literature	On The History of Ugliness In History of Beauty, Umberto Eco explored the ways in which notions of attractiveness shift from culture to culture and era to era. With ON UGLINESS, a collection of images and written excerpts from ancient times to the present, he asks: Is repulsiveness, too, in the eye of the beholder? And what do we learn about that beholder when we delve into his aversions? Selecting stark visual images of gore, deformity, moral turpitude and malice, and quotations from sources ranging from Plato to radical feminists, Eco unfurls a taxonomy of ugliness. As gross-out contests go, its both absorbing and highbrow.
Top/Computers/Programming/Web_Development	The Implications of OpenID OpenID is an emerging standard that provides simple, decentralised authentication for the Web. OpenID follows the Unix philosophy, solving one small problem rather than attempting to tackle the many larger challenges posed by online identity. This talk will explore the implications of OpenID, and explore the best practices required to take advantage of this new technology while avoiding the potential pitfalls. Speaker: Simon Willison is a consultant on OpenID and client- and server-side Web development, and a co-creator of the Django Web framework. Before going frelance Simon worked on Yahoo!'s Technology Development team, and prior to that at the Lawrence Journal-World, an award winning local newspaper in Kansas. Simon maintains a popular Web development weblog at [[http://simonwillison.net/]]
Top/Computers/Programming/Web_Development	Gears and the Mashup Problem Mashups are the most interesting innovation in software development in decades. Unfortunately, the browser's security model did not anticipate this development, so mashups are not safe if there is any confidential information in the page. Since virtually every page has at least some confidential information in it, this is a big problem. Google Gears may lead to the solution. Speaker: Douglas Crockford Douglas Crockford is the world's foremost living authority on JavaScript. He is an architect with Yahoo's Ajax Strike Force. He is the founder of two startups, and was Director of Technology at Lucasfilm Ltd., Director of New Media at Paramount, and a researcher at Atari and SRI.
Top/Computer_Science/Interviews	A discussion about ML
Top/Computer_Science/Interviews	Interview with David Hardoon
Top/Computer_Science/Human_Computer_Interaction	Really Achieving Your Childhood Dreams ***The brick walls are there for a reason: they are not there to keep us out, they let us prove how badly we want something.***
Top/Computer_Science/Human_Computer_Interaction	Special Session: Projects in Multimodal Interaction - VACE
Top/Computer_Science/Human_Computer_Interaction	Psychology in Human-Computer Interaction This course is intended to give newcomers enough background in the field of HCI to make their conference experience much more meaningful. It provides a framework to understand how the various topics are related to research and practice. It is a tried and true introduction and has become a CHI conference tradition.
Top/Computer_Science/Human_Computer_Interaction	MAHI: Investigation of Social Scaffolding for Reflective Thinking in Diabetes Management We describe the design and deployment of MAHI, a ubicompapplication for individuals with diabetes, and its effect on individuals ownership of their health and disease management.
Top/Computer_Science/Human_Computer_Interaction	BlindSight: Eyes-Free Access to Mobile Phones BlindSight allows mobile phone users to access phone calendarand contacts in situations where they cannot look at the screen, such as while talking on the phone or while driving. Based on auditory feedback.
Top/Computer_Science/Human_Computer_Interaction	EdgeWrite with Integrated Corner Sequence Help A help system that informs users of the character shapes in the EdgeWrite text entry system was tested. It can replace printed character charts without hurting novice performances.
Top/Computer_Science/Human_Computer_Interaction	Attention By Proxy? Issues in Audience Awareness for Webcasts to Distributed Groups Presents findings from a study of how classroom instructors pay attention to their students, and implications for displaying images of remote participants in classes with both local and remote students.
Top/Computer_Science/Human_Computer_Interaction	Navigation Techniques for Dual-Display E-Book Readers We present the design and evaluation of a dual-display electronic reader, which supports a wide range of reading activities like lightweight navigation, global search and multi-documentinter actions.
Top/Computer_Science/Human_Computer_Interaction	Probabilistic user interfaces Gaussian process priors have recently been applied to control problems. The GPs bring advantages in their representation of model prediction uncertainty, and because the derivative of a Gaussian process is a Gaussian process, they can also incorporate derivative information, and analytically provide the uncertainty of model derivatives. This can be used to bring a natural regularization of control effort, resulting in appropriately cautious control. It can also be used to provide a quickened display, which takes account of model uncertainty. I will describe our work in ambiguous or probabilistic user interfaces, and demonstrate some of the techniques we have developed for combining probabilistic models with continuous control and for providing feedback of system uncertainty to the user.
Top/Computer_Science/Human_Computer_Interaction	The Science and Art of User Experience at Google Focus on the user and all else will follow. From its inception, Google has focused on providing the best user experience possible. Jen Fitzpatrick will take you through the art and science behind Google's design process and share examples of how design, usability and engineering come together in Google's unique culture to create great products
Top/Computer_Science/Human_Computer_Interaction	Multi-modal interaction involving speech and language technologies
Top/Computer_Science/Human_Computer_Interaction	Computer Science and Human-Computer Interaction This course is intended to give newcomers enough backgroundin the field of HCI to make their conference experience much more meaningful. It provides a framework to understand how the various topics are related to research and practice. It is a tried-andtrue introduction and has become a CHI conference tradition.
Top/Computer_Science/Human_Computer_Interaction	Methods for Fusing Eye Movements and Text Content for Information Retrieval
Top/Architecture	WTC Lecture - collapse of WTC Buildings BYU Physics professor and founder of SCHOLARS FOR 9/11 TRUTH Steven E **Jones** presents his presentation on the collapse of **WTC** Buildings 1,2&#160;and 7 on 9/11. A very informative and scientific presentation that raises serious questions about the official account of the collapse of the World Trade Center Towers and Building 7.&nbsp;&nbsp;
Top/Architecture	Interview with Santiago Calatrava Santiago Calatrava, Architect; Visuals of Milwaukee Art Museum, City of Science, and various buildings/bridges
Top/Education	Outdoctrination: Society, Children, Technology and Self Organisation in Education
Top/Education	Rights Management and Educational Repositories
Top/Education	Management Education in Emerging Markets; What Is the Same, What Is Different?
Top/Law	The rule of law and the role of the courts: The perception of law from within A proper understanding of the role of the courts in safeguarding the rule of law as an independent branch of government is a prerequisite for any viable public appreciation of the law in general and the judicial system in particular. This is notably and quite obviously the case with the perception of the role of the courts from the viewpoint of the two political branches of government as well as the general public, which will presumably be the focus of other talks in the conference; but it is also partly dependent upon the self-positioning of the judiciary, which will be the main focus (or the originating point) of this talk. This task may be addressed in several ways. At a principled level, it is the challenge of reconciling the (internal) need for an independent or autonomous judiciary with the (external) drive for its accountability. At a more concrete level, it is translated into individual choices that fall somewhere between the lines of judicial restraint and judicial activism, epitomized by the now slightly dmod political question doctrine. While the doctrine itself has somewhat disappeared from the legal debate in the recent years, the questions it posed remain just as valid today. Should judges be making policy? Where is the line to be made between what is the proper realm of adjudication and what goes beyond into improper interference with the other branches of government? Can judicial activism ever be defended as proper and, if so, to what extent and under what circumstances? May a certain increased level of judicial activism be justified in transitional societies, if and inasmuch as it is used to further the cause of judicial independence? The talk will address these questions with reference to recent examples from across the globe of their actuality and practical volatility, ranging from the relatively harmless academic questioning of Justice Ginsburgs dissent in the recent Ledbetter v. Goodyear Tire &amp; Rubber case, calling upon Congress to correct the Supreme Courts parsimonious reading of a certain legislation and trying to propel legislative change, to the critical case of the unfortunate illegitimate ousting of absurdly vast numbers of judges in Pakistan as an attempt of stifling judicial autonomy in anticipation of judicial decisions unfavorable to the government.
Top/Law	Experiences of the Bulgarian judiciary
Top/Law	Experiences of the Croatian judiciary
Top/Biology	Bioinformatic, Structural Biology and Structure Based Ligand Design in Drug discovery During the lecture we will be shown the principles on which the modern development and drug discovery with emphasis on optimization of compounds with the use of three dimensional structures of the receptor. Also the use of bioinformatics will be shown and computer supported planning of ligands together with the implications of the understanding of detailed interactions between the protein and the ligand in the transfiguration of the enzyme inhibitors into medicines in the context of modern challenges.
Top/Biology	Biologically Inspired Flint Glass Flint glass is optical glass that has relatively high refractive index and low Abbe number. Flint glasses are arbitrarily defined as having an Abbe number of 50 to 55 or less. The currently known flint glasses have refractive indices ranging between 1.45 and 2.00. A concave lens of flint glass is commonly combined with a convex lens of crown glass to produce an achromatic doublet lens because of their compensating optical properties. With respect to glass, the term 'flint' derives from the flint nodules found in the chalk deposits of southeast England that were used as a source of high purity silica by George Ravenscroft, circa 1662, to produce a potash lead glass that was the predecessor to English lead crystal. Traditionally, flint glasses contain around 4%60% lead oxide; however, the manufacture and disposal of these glasses are sources of pollution. In many modern flint glasses, the lead can be replaced with other additives such as titanium dioxide and zirconium dioxide without significantly altering the optical properties of the glass. Flint glass can be fashioned into rhinestones which are used as diamond simulants.
Top/Biology	Topics in Biology - Organismal Biology
Top/Biology	On Numerical Characterization of DNA, Proteins, Proteomics Maps and Proteome from their Graphical Representations We will outline calculation of a selection of mathematical invariants that can be extracted from matrices associated with various graphical representations of DNA, Proteins, Proteomics Maps and Proteome. In the case with proteome maps one can construct a zigzag line connecting ordered spots in a map, one may construct the partial order graph, one may construct the cluster graph, one may construct the nearest neighbor graph or the sequential neighbor graph, and one may construct the minimal spanning tree.
Top/Biology	Parameter Estimation of ODE's with Regression Splines: Application to Biological Networks The construction and the estimation of quantitative models of gene regulatory networks and metabolic networks is one of the task of Systems Biology. Such models are useful because they provide tools for simulating and predicting biological systems. Various approaches have been proposed, such as graphical models , Bayesian dynamical models or Ordinary Differential Equations (ODE's) . For the latter, one can also expect to derive parameters that often have a meaningful biological sense. We focus on the estimation of a parameter theta indexing a (vector) ODE, from an observed time series (concentration profiles) which may be nonlinear (e.g. due to the use of Michaelis-Menten dynamics or mass action law). Even when the likelihood is simple (in the case of Gaussian error noise), the computation of the Maximum Likelihood Estimator remains hard because of the burden of the optimization step. Indeed, the implicit definition of the model necessitates the integration of the ODE for each evaluation of the likelihood. Moreover, the likelihood may have numerous local maxima we need to avoid, hence the exploration of the parameter space may be computer-intensive. We propose then an alternative (frequentist) estimator of theta based on a preliminary spline estimator of the solution of the ODE. We use a simple characterization of theta that enables to derive a learning algorithm avoiding the integration of the ODE, and that can split the estimation of a vector differential equation in several estimations of scalar differential equations. We illustrate this algorithm with different models used in Systems Biology and we sketch how it can be adapted to various settings encountered by the practitioner. Joint work with Chris Klaassen and Florence d'Alch-Buc.
Top/Biology	Principle of Systems Biology illustrated using the Virtual Heart Highest systems property: The living organism does not really exist in the milieu extrieur but in the liquid milieu intrieur a complex organism should be looked upon as an assemblage of simple organisms that live in the liquid milieu intrieur.
Top/Biology	Scaling Laws in Biology and Other Complex Systems Life is very likely the most complex phenomenon in the Universe manifesting an&#160;extraordinary diversity of form and function over an enormous range. Yet, many of its most fundamental and complex attributes scale with size in a surprisingly simple fashion. For example, metabolic rate (the power required to sustain the system) scales as approximately the 3/4-power of mass over 27 orders of magnitude from molecular levels up to the largest multicellular organisms. Similarly, time-scales, such as lifespans and growth-rates, increase with exponents which are typically simple powers of 1/4. It will be shown how these universal quarter-power scaling laws follow from fundamental generic principles embedded in the dynamics and geometry of underlying networks, leading to a general quantitative theory that captures essential features of many diverse biological systems. Examples will include animal and plant vascular systems, growth, cancer, aging and mortality, sleep, DNA nucleotide substitution rates. These ideas will be extended to discuss social organisations such as cities and firms: to what extent, if at all, can we think of these as very large organisms and therefore as an extension of **biology**? Analogues to metabolic rate and behavioral times in cities scale counter to their behaviour in **biology**. Driven by innovation and the creation of wealth this has dramatic implications for their growth, development, sustainability and pace of life which, left unchecked, potentially sow the seeds for their collapse.
Top/Biology	Dynamics on and of Biological Networks: Case Studies on the Machinery of Life Gene regulation networks and other molecular networks that regulate the processes of life in the living cell are prototypes for dynamical networks that combine the aspects of both, transferring dynamical signaling on the one hand, and being structurally dynamical themselves on the other hand. Both phenomena, while living on vastly different timescales (that of molecular interactions versus the timescale of macroevolution), are closely interwoven and depend on each other. We will take a closer look at this interesting type of complex networks and I will review a few approaches and views from different angles.
Top/Biology/Bionics	Targeted Reinnervation for Improved Myoelectric Prosthesis Control Don't miss the 'it' moment from Pop!Tech 2005, as the world's first non-fictional bionic man maneuvers his prosthetic arm using only his&#160;mind. Jesse Sullivan and his doctor, Todd Kuiken, move every heart in the room with indomitable spirit and astonishing bionics.
Top/Biology/Bionics	Evolutionary Classification of Toxin Mediated Interactions The rock scissors paper game gives atemplate for the structural dynamics of toxin induced oscillations withinbacteria. The first player is the sensitive which is toxified by thesecond, the producer, bearing the metabolic costs for resistance and toxinproduction. The producer is in turn again out-competed by the Resistant, who has no metabolic costs for toxin production. At least the Sensitivewins again, because he has no expenses for resistance. This leads to acoexistive/cyclic dynamics in the population-density of these threespecies. We derive here a scenario for the evolutionary dynamics of three speciesinvolved in this RSP - game. Starting from basic biologicalprinciples we determine evolutionary stable states or pathways in the traitspace of bacteria. For the bacteriocin producing bacteria sample mechanisms aredemonstrated. We derive functional relations between the parameters ofour model necessary for evolutionary dynamics. The process usesadaptive dynamics and we analyze the stability type of the obtained singular point. We investigate this behaviorwith respect to the toxicity of the producer and the yield/intrinsic growthspeed relation of each species. The result can be generalizedto all kinds of toxin or disease interactions of three species. As targetdynamics we get Zeeman class 33, which is permanent and guarantees stablecoexistence of three species by population dynamics and also with respect toadaptive evolution. This implies that all toxin mediated interactions tend toa stable coexistive fixed point given reasonable biological relations betweenthe parameters.
Top/Biology/Bionics	Flocking as a Synchronization Phenomenon with Logistic Agents In this paper, we intend to show that the flocking phenomenon observed in many animal species behaviors, may be modeled as a synchronization process occurring within entity states. Although flocking has been widely studied and simulated in Swarm Intelligence, few works mention synchronization as a key aspect of the problem and model it properly. This paper proposes a modeling in terms of a reactive multi-agent system composed of interacting logistic agents moving in an environment. This specific MAS called Logistic MAS (LMAS) takes actually inspiration from the coupled map lattice field, which provides also many tools toanalyse convergence and stability of the system. We develop our approach in boththeoretical and applied way to demonstrate its relevance.
Top/Biology/Ecology	The State of the Oceans Jeremy Jackson, marine ecologist and environmental advocate, professor of oceanography at the Scripps Institution of Oceanography, describes how overfishing, habitat destruction, global warming and other human-induced activities have contributed to a crisis in the health of the world's oceans
Top/Biology/Ecology	Ecology of bird populations in the Pacific Flyways Hear it straight from California experts about the threat of a global bird-flu epidemic. The following research scientists and public-health experts from UC Davis and other Northern California organizations addressed the gamut of common concerns at an Avian Influenza Symposium held in Davis, Calif., on April 15, 2006. Organized by the Yolo Audubon Society, the program particularly emphasized the role that wild birds might play in spreading avian influenza to humans.
Top/Biology/Ecology	Overview of Environmental applications of Machine Learning
Top/Technology/Nanotechnology	Nanowires and Nanocrystals for Nanotechnology Nanowires and nanocrystals represent important nanomaterials with one-dimensional and zero-dimensional morphology, respectively. Here I will give an overview on the research about how these nanomaterials impact the critical applications in faster transistors, smaller nonvolatile memory devices, efficient solar energy conversion, high-energy battery and nanobiotechnology
Top/Technology/Nanotechnology	Nanotechnology
Top/Technology/Nanotechnology	Manipulation of Nanoscale Charged Polar States in Manganites The complicated interplay among charge, spin and lattice degrees of freedom in manganites is believed to induce the unexpected magnetic and transport phenomena, such as the colossal magnetoresistance (CMR). Manganites display also a variety of useful multiferroic properties such as colossal magnetocapacitance effect and high dielectric constant. In multiferroics ferromagnetic order can be controlled by an electric field, or ferroelectric order can be controlled by a magnetic field. Among them, La1-xSrxMnO3 is the most attractive candidate for multiferroic applications because of a combination of desirable properties. In this work we report the observation of the high contrast of electric field induced charged polar states after the local application of the electric field to the surface of samples via several Scanning Probe Microscopy (SPM) techniques in La0.9Sr0.1MnO3 and La0.89Sr0.11MnO3 single crystals. The electric-field-induced contrast is observed in Kelvin mode (KFM) confirming local modification of the surface properties of manganites. Piezoelectric effect of the induced states is assessed using Piezoresponse Force Microscopy (PFM). These results are complemented by the measurements of piezoresponse hysteresis and surface potential hysteresis loops at some area in standard pulse dc mode. The induced polar charged states relax with characteristic time constant of about 80 hours at room temperature, which exceeds Maxwell relaxation time by many orders of magnitude. The mechanisms of the observed phenomena are discussed along with the possible instrumentation effects. The origin of the effect can be related to the nanoscale charge and spin dynamic inhomogeneities appearing in manganites due to a delicate balance of charge, lattice and magnetic order. The injection of the additional charge carriers in the induced area promotes the appearance of the polar charged states. The long relaxation time for the induced charged state may be explained by the existence of the intrinsic inhomogeneous states. All these results show that the existence of the stable areas with the increased charge concentration is possible and thus it confirms the tendency towards charge segregation in manganites.
Top/Technology/Nanotechnology	Formulation of PLGA nanoparticles for intracellular delivery of protein drug Design and formulation of advanced drug delivery systems (DDS), such as nanoscale carriers, presents an attractive research area in the field of drug formulation. A vast contribution is expected in delivery of biopharmaceuticals as is clearly recognized that inadequate delivery is the single most important factor delaying their application in clinical practise. In spite of some successful guidelines, formulation of protein drugs in DDS requires step-by step strategy and methods differing from those used for classical pharmaceutical drugs since proteins are the most delicate ones in term of retaining their biological function. A model protein drug cystatin was selected in our work, having high potential for inactivating cysteine proteases, enzymes involved in processes of tumour invasion and metastasis. Nanoparticles was used as carrier system with the aim to increase the bioavailability of the protein drug by protecting it from premature degradation in biological environment and faciliting its intracellular delivery. Cystatin was incorporated in poly(lactide-co-glycolide) (PLGA) nanoparticles by the water-in-oil-in-water emulsion solvent diffusion method. To preserve its biological activity an optimized technique was developed, adjusting physical and chemical parameters of processes during nanoparticle production. Cystatin-loaded NPs had size of 300-350 nm diameter, and contained 1.6 % (w/w) of cystatin, retaining 85% of its starting activity. To follow cellular uptake of nanoparticles, cystatin was labelled with fluorescent dye (Alexa Fluor 488) prior to its encapsulation into NPs. Image analysis showed rapid internalization of NPs into MCF-10A neoT cells as the fluorescence spots were detected after treatment with NPs. On the other hand, labelled free cystatin was internalised very slowly, suggesting that NPs facilitate the delivery of its cargo into the cells. Cystatin, delivered by NPs, also exerted its inhibitory activity on intracellular target cathepsin B, suggesting that its integrity was preserved throughout the processes of formulation and delivery. On the other hand, free cystatin did not impart proteolityic activity of cathepsin B, when tested under the same conditions using the substate, specific for intracellular cathepsin B. Our results show that protein drug can be formulated in the active form into PLGA NPs, when suitably selecting the process parameters of NP-production. NPs are also able to facilitate delivery of protein drug into the cells, enabling its activity on the intracellular target
Top/Technology/Nanotechnology	Self-catalysed growth of GaAs nanowires by MBE Semiconductor nanowires (NWs) growth is typically assisted by a metal particle, called the catalyst. The use as the catalyst of a material different from the components of the wire may change the semiconductor properties due to the diffusion of the catalyst in the nanowire body during the growth. Moreover, the most commonly used catalyst is Au, a metal that is incompatible with the Si technology. For the above mentioned reasons it is therefore of importance to develop a technology leading to a catalyst-free growth of semiconductor nanowires. Here we report preliminary results on catalyst-free growth of GaAs NWs by molecular beam epitaxy. The GaAs NWs have been grown on cleaved edges of Si wafers, with no catalyst pre-deposition. The growth lasted 30 minutes and has been performed at 580 T 620 C. Two kinds of nanowires have been obtained. The NWs of the first type are as long as 5 m with a section diameter in the range of tens of nm and have a spherical particle at their end tip. Energy dispersive X-ray spectroscopy (EDS) demonstrates that the spherical particle is composed of Ga, and that the NW body is GaAs. The NWs density depends on the crystallographic orientation of the facets that compose the cleaved edges of the Si-wafers. The second type of NWs are generally characterised by a smaller aspect ratio, clearly faceted lateral and tip surfaces, and no metallic particle on their tip. EDS curves reveal that they are completely made of GaAs. The EDS results suggest that a Ga induced self-catalysed growth occurred on specific surface locations where Ga droplets formed and were trapped during the first stages of the GaAs deposition. Work is in progress to understand the growth process and in particular to understand whether the droplet-less NWs grow through a different process or the absence of a Ga droplet is due to its lost during growth.
Top/Technology/Nanotechnology	History of NANONET-Styria
Top/Technology/Nanotechnology	Molybdenum chalcohalide nanowire mysteries: Structure, rigidity and quantum transport
Top/Technology/Nanotechnology	Crowding effects in Enzymatic Restriction Reactions within DNA Nanostructures We have studied the effect of restriction enzyme digestion reactions (DPN II) within DNA nanostructures on flat gold substrates by Atomic Force Microscopy (AFM). Typically we work with a few patches of Self Assembled Monolayers (SAMs) DNA that are hundred nm in size and are fabricated within alkylthiol SAMs on gold films by means of an AFM based lithographical technique known as Nanografting.1,2 We start by nanografting a few patches of a single stranded DNA (ssDNA) molecule of 44 base pairs (bps) with a recognition sequence of 4 bps (specific for the DPN II enzyme) in the middle. Afterwards, to obtain reaction ready DNA, the nanopatches are hybridized with a complementary ssDNA sequence of the same length. The enzymatic reactions were carried out over nanopatches with different molecular density and different geometries. Using nanopatch height measurements carried out with an AFM we are able to show that the capability of the DPN II enzyme to reach and react at the recognition site significantly depends on the geometry and the molecular density of the nanopatches. In general it was found that the digestion of the DNA by the enzyme it is strongly inhibited at a relatively low dsDNA density. Reference experiments were similarly carried out with nanopatches of a DNA sequence without the recognition site and it was found that in that case the enzymatic reaction didnt lead to digestion of the nanopatches. These findings suggest that, due to the enzyme size, it is possible to tune the efficiency of an enzymatic reaction on a surface by changing the crowding conditions of the DNA in the nanopatches.
Top/Technology/Nanotechnology	Structural and electronic properties of molybdenum chalcohalide nanowires We combine ab initio density functional and quantum transport calculations based on the nonequilibrium Greens function formalism to compare structural, electronic, and transport properties of Mo6S6-xIx nanowires with carbon nanotubes. We find systems with x=2 to be particularly stable and rigid, with their electronic structure and conductance close to that of metallic (13,13) single-wall carbon nanotubes. Mo6S6-xIx nanowires are conductive irrespective of their structure, more easily separable than carbon nanotubes, and capable of forming ideal contact to Au leads through thio groups.
Top/Technology/Nanotechnology	Incorporating optical techniques in electron microscopy for comprehensive characterization of individual nanostructures The purpose of this informal talk is to introduce briefly some new additions to the research group at Peking University and seek more future collaborations with JSI. Optical techniques (e.g., luminescence and Raman spectroscopy) can provide rich information on semiconductor properties (band structure, phonon structure, confinements, etc.), which are complementary to electron microscopy techniques. Initial efforts have been carried out to combine submicron optical techniques and in situ nanoprobe technique in electron microscopy to carry out comprehensive characterization of individual semiconductor nanostructures. In the first approach, we attach individual suspended semiconductor nanowires or nanorods to nanometer sized metal tips, which are compatible for different instruments, such as scanning electron microscope (SEM), transmission electron microscope (TEM) and microphotoluminescence (PL). Thus optical (PL), microstructural (SEM and TEM) and electrical (nanoprobe technique inside SEM) characterization can be carried out on the same 1D nanostructure. Our results on in situ annealed ZnO nanowires show conclusive correlations among defectrelated green emission, redshift of the near band edge emission, carrier density and oxygen deficiency. This highly flexible technique also enables angular dependent microphotoluminescence measurements on individual suspended ZnO nanorods. In the second approach, we combine optical fiber probe with the nanoprobe technique in SEM to achieve comprehensive characterization of optoelectronic nanotructures inside a single chamber. The nanoprobe technique, employing sharp metal tips, is used for nanostructure manipulation and electrical measurement. The fiber probe, coupled to a spectrometer or a laser and controlled by a nanomanipulator, allows local optical detection or excitation. Using in situ light emitter and photodetector based on individual nanostructures, we demonstrate that above technique with high flexibility and efficiency can play an important role in building prototype optoelectronic devices and selectingsuitable nanostructures for device purposes. ----
Top/Technology/Nanotechnology	Developments of high performance n-type carbon nanotube field-effect transistors As scaling down with Moore's law, the modern silicon complementary metaloxidesemiconductor (CMOS) technology is facing great challenges and people are looking for alternatives. Carbon nanotube (CNT), due to its novel structure and properties, has been regarded as one of the most promising building blocks for the future integrated circuits. Since the first CNT fieldeffect transistor (CNTFET) was designed in 1998, device performance hasbeen continually improved. By using palladium (Pd) electrodes and highk materials (which are less prone to current leakage) as gate dielectrics, ptype CNTFETs have now surpassed the capabilities of stateoftheart silicon pMOSFETs. However, the development of ntype CNTFETs has lagged behind. This is mainly due to the difficulty of fabricating a Schottky barrierfree contact between metal electrodes and the conduction band of the CNT. The slow progress in producing nCNTFETs has greatly hindered the development of CNTbased integrated circuits. We Recently discovered that scandium (Sc) can be used to generate an ohmic contact with the conduction band of a CNT and high performance ntype CNTFETs can be easily fabricated. Based on this discovery, we proposed an CNTbased dopingfree CMOS technology and pushed the limits of n type CNTFETs. We also demonstrated a design of whole carbon nanotube circuits by integrating MultiWalled CNTs with the SingleWalled CNTFETs which serve as interconnects. All of our results show a prospective future of CNTbased integrated circuits. ----
Top/Technology/Nanotechnology	Re-thinking scientific teams: competition, conflict and collaboration In this talk, Dr. Gadlin will discuss the organizational barriers and breakdown of collaboration in the scientific community while focusing on recurring themes in collaborative disputes. He then presents means for resolving and responding to conflict confidentially through dispute resolution programs and pre-nuptial agreements between collaborators.
Top/Technology/Nanotechnology	Nanotechnology Innovation--Two Aspects In this talk, Dr. Kesan will discuss the challenges and issues posed by nanotechnology innovation for patent policy and for granting patent rights commensurate with innovation. Second, he will discuss how the insights from value-sensitive design can be applied to vindicate societal choices and preferences in emerging nanotechnologies.
Top/Technology/Nanotechnology	Investment and interpretation: nanotechnology, financial journalism and practical epistemology Studies of the ways in which the media report nanotechnologies, and particularly the ways in which they frame their interpretation, are crucial to an understanding of the formation of public perceptions of what are highly technical areas of scientific endeavour. This talk reports on a research project which, whilst broadly located within this area of concern, looked at the related question of the financial understanding of science: how, in a field characterised by high levels of commercialisation, potential investors get information and make judgments about particular applications, and the significance of the roles played by journalists and other mediators in this process. The focus here is on the practical epistemological strategies that scientific and financial journalists employ to make sense of nanotechnologies. Drawing on interview data, the paper considers the way that these journalists assess claims made about scientific validity and investment potential, and how they negotiate such narrative dilemmas as balancing the need for scepticism in a rhetorically inflated context with the professional requirement to produce an interesting story. It is argued that this analytic focus on journalists as active interpreters and as actors for whom the understanding of nanotechnologies is a pressing practical problem provides an important complement both to studies of the framing effects of journalistic copy, and to studies of public understandings of what remains, for most of the public, a relatively arcane field. Moreover, in focusing on where the action currently is, it may inform our knowledge of not just the commercial development of nanotechnologies, but also the formation and development of public opinion.
Top/Technology/Nanotechnology	Predicting the Future: How Ordinary People Make Sense of Emerging Nanotechnologies This presentation will briefly review quantitative and qualitative data that suggest the general tenor of the current public opinion climate for nanotechnologies, and then identify the key factors that can be expected to affect how people cope with information about any new technology. These include their own underlying values, their levels of trust in key social actors, and the connections they identify with technologies previously encountered, as well as information from media accounts. Public conceptions of potential risks are often broader than those commonly identified in formal risk assessments, encompassing 'social risks' such as disruption, displacement, privacy, distribution, regulation, and so on, as well as risks to human health and environmental integrity. While media are only one influence among many, they are regularly accused of exaggerating some risks while ignoring others. Progress toward developing a theory that might predict when and explain why this occurs will be reviewed.
Top/Technology/Nanotechnology	Perceiving Nanoscale Phenomena: Interpreting and Disseminating Nanoscale Images Scientific imaging techniques have played an increasingly significant role in nanoscale research. But how should the resulting images be interpreted? What kind of information do they offer about their targets (the objects and relations they are about)? And how should that information be understood and disseminated (in particular, how reliable should the information be)? Given the plurality of images found in nanoscale research, it is unlikely that a single unified account can be articulated that accommodates all the images in question. However, this doesnt mean that a general framework that helps us address central issues in the interpretation of nanoscale imaging cannot be provided. In this paper, I offer such a framework. It provides a new conceptualization of nanoscale images and their content, by highlighting the following aspects: (a) In order to understand a nanoscale image, its crucial to determine which kind of image we are dealing with: How was the image obtained? And which sort of information is it intended to convey? (b) We should then examine the sources of bias that the images may contain: What kind of artifacts may be included in the image? (c) Finally, nanoscale images are often used as exemplars in the domain from which they emerged: How can such images be used as inferential devices that allow researchers to generalize the information provided in a particular image to other samples (whether in the same domain or in related ones)? After motivating and articulating such a framework, I provide some case studies illustrating how the proposal works, by considering a variety of nanoscale images from microscopy (including probe and electron microscopy).
Top/Technology/Nanotechnology	Nanotechnology, development and public policy In this talk, Dr. Armstrong will discuss five points to think about in regards to Nanotechnology when dealing with development and public policy issues. Dr. Armstrong focuses on the vagueness of the term nanotechnology, the abstract nature in which it is conceptualized and what level of abstraction would be appropriate when conceptualizing it.
Top/Technology/Nanotechnology	The Culture of the American University in the Age of Neoliberalism Universities in the United States and across the globe are changing. What is the nature of this change? For nearly twenty years, much of the scholarship and work by journalists on the United States has highlighted increases in conflict of interest, secrecy, proprietary research, loss of unbiased public interest analysts, and distortion of research agendas associated with university-industry research relationships. While these concerns are not entirely misplaced, I argue that the focus on what are egregious violations of academic normson dramatic casesfails to capture a deeper and more difficult to police transformation of the US university. Instead, I believe a fundamental transformation of the culture of university life and academic science, especially, is underway. In this paper, I explore the claims of some of the most high profile recent work on the commercialization of the American university, and I point to a set of examples and indicators that suggest we are seeing a deep transformation of academic culture in the United States.
Top/Technology/Nanotechnology	Why Managing Research is Not Managing Science In this talk, Dr. Rjeski will discuss the increase and penetration of new nano products into the market and how this is a measure of its success. He will then address the need for responsibility and education to address the publics desire for full-disclosure, pre-market testing, and third-party testing and research. Finally he will discuss the nanotechnology concerns for the future.
Top/Technology/Nanotechnology	Topology, structure and defects in carbon nanosystems In this talk we will explore how in the last twenty years carbon science has made the fundamental step from the flat world of graphite into the three dimensional world of fullerenes and nanotubes, the building blocks of the carbon nanotechnology revolution. We will look at the history of the discovery of buckminsterfullerene and carbon nanotubes, and explore analogies in diverse fields of biology, architecture and sport. The understanding of defects in nanocarbons is essential in order to control their diverse properties. For example irradiating bundles of carbon nanotubes produces defects which increase their bending strength by a factor of 16. At the same time such defects can store energy and were the cause of the UK Windscale nuclear fire in the 1950s. Recent advances in computational modelling and electron microscopy mean that we now have a much better understanding of the structure, formation and evolution of intrinsic defects, opening up the intriguing possibility of selective spatial creation of defects atomic level defect engineering.
Top/History	Successes, Failures and Learning From Them Over the last eighteen years, the field of knowledge discovery and data mining has matured considerably. Although the field has evolved as a result of synergistic co-operation among researchers in databases, artificial intelligence, statistics and systems, it has maintained its own identity. From a single workshop in 1989, the field can now lay claim to at least 5 major conferences and numerous symposium devoted to its central theme.
Top/History	Successes, Failures and Learning From Them At an abstract level, the theme of the field is concerned with extracting actionable and interpretable knowledge from data in as efficient a manner as possible. The primary purpose of this panel, in the context of this underlying theme, is to consider the following questions. What have been the major successes and breakthroughs that we as a field can point to with pride? What have been the critical mistakes or mis-steps that have been taken along the way? And finally, what can we hope to learn from both our successes and mistakes and how can this knowledge be used to determine how to focus our efforts in the future?
Top/History	From Indifference and Contempt to Love and Hate. The Perception of Franks in Ottoman Culture From the very start of their rise to statehood, the Ottomans have been confronted and exposed to a wide variety of western peoples and cultures: Venetians and Genoese in the 15th, French and Austrian in the 16th, British and Dutch in the 17th centuries Most of these early contacts were characterised by a general feeling of indifference, mixed with a considerable amount of contempt deriving from an imperial sense of superiority and a marked bias against infidels falling outside of Islamic jurisdiction. This general feeling of mistrust did not preclude interaction between Ottomans and Westerners, be it at a diplomatic, institutional, communal, or individual level; yet they remained superficial and sporadic, all the more so if one considers that contacts were almost exclusively one-sided, the Ottomans generally comfortably remaining at the receiving end of such relations. Things began to change in the 18th century as a result of a rapidly growing web of communication between the West and the Empire, and, most of all, due to a gradual change in the rapport de force between the two worlds. Confronted with the first concrete signs of western predominance, the Ottomansespecially members of the ruling elitefelt a greater urge to intensify their contacts with, and understanding of, western peoples and culture. Though not yet westernisation per se, this process gradually paved the way to the extraordinary intensity that relations with the West would acquire during the 19th century. Under these circumstances, it was inevitable that perceptions of the Franks, now redefined as Europeans and/or Westernerswith all the civilisational connotations that came with the termswould change radically: indifference was no longer possible; contempt had lost its justification. The Ottomans moved toward a love/hate relationship with the West, which can still be felt underlying the complex feelings of Turks toward Europe today.
Top/History	The Secret History of Silicon Valley How Stanford the CIA/NSA Built the Valley We Know Today How much does an average Googler know about the history of the place he works in? Silicon Valley. Come and test your knowledge. I have seen this talk and I assure you - even seasoned Silicon Valley veterans will find this story interesting. Silicon Valley entrepreneur Steve Blank will talk about how World War II set the stage for the creation and explosive growth of Silicon Valley, and the role of Frederick Terman and Stanford in working with government agencies (including the CIA and the National Security Agency) to set up companies in this area that sparked the creation of hundreds of other enterprises.
Top/Economics	An Economic Response To Unsolicited Communication
Top/Economics	Money As Debt Paul Grignon's 47-minute animated presentation of 'Money as Debt' tells in very simple and effective graphic terms what money is and how it is being created. It is an entertaining way to get the message out. The Cowichan Citizens Coalition and its 'Duncan Initiative' received high praise from those who previewed it. I recommend it as a painless but hard-hitting educational tool and encourage the widest distribution and use by all groups concerned with the present unsustainable monetary system in Canada and the United States. More at [[http://www.moneyasdebt.net/|moneyasdebt.net]]
Top/Economics	Estimation of the Solution of a Differential Equation: an Inverse Problem
Top/Economics	Machine Learning for Stock Selection In this paper, we propose a new method called Prototype Ranking (PR) designed for the stock selection problem. PR takes into account the huge size of real-world stock data and applies a modified competitive learning technique to predict the ranks of stocks. The primary target of PR is to select the top performing stocks among many ordinary stocks. PR is designed to perform the learning and testing in a noisy stocks sample set where the top performing stocks are usually the minority. The performance of PR is evaluated by a trading simulation of the real stock data. Each week the stocks with the highest predicted ranks are chosen to construct a portfolio. In the period of 1978-2004, PRs portfolio earns a much higher average return as well as a higher risk-adjusted return than Coopers method, which shows that the PR method leads to a clear profit improvement.
Top/Economics	Temporal Causal Modeling with Graphical Granger Methods The need for mining causality, beyond mere statistical correlations, for real world problems has been recognized widely. Many of these applications naturally involve temporal data, which raises the challenge of how best to leverage the temporal information for causal modeling. Recently graphical modeling with the concept of Granger causality, based on the intuition that a cause helps predict its effects in the future, has gained attention in many domains involving time series data analysis. With the surge of interest in model selection methodologies for regression, such as the Lasso, as practical alternatives to solving structural learning of graphical models, the question arises whether and how to combine these two notions into a practically viable approach for temporal causal modeling. In this paper, we examine a host of related algorithms that, loosely speaking, fall under the category of graphical Granger methods, and characterize their relative performance from multiple viewpoints. Our experiments show, for instance, that the Lasso algorithm exhibits consistent gain over the canonical pairwise graphical Granger method. We also characterize conditions under which these variants of graphical Granger methods perform well in comparison to other benchmark methods. Finally, we apply these methods to a real world data set involving key performance indicators of corporations, and present some concrete results.
Top/Economics	The economist as therapist: Behavioural economics and 'light' paternalism We review methodological issues that arise in designing, implementing and evaluating the efficacy of 'light' paternalistic policies. In contrast to traditional 'heavy-handed' approaches to paternalism, light paternalistic policies aim to enhance individual choice without restricting it. Although light paternalism is a 'growth industry' in economics, a number of methodological issues that it raises have not been adequately addressed. The first issue is how a particular pattern of behavior should be judged as a mistake, and, relatedly, how the success of paternalistic policies designed to rectify such mistakes should be evaluated i.e.,the welfare criterion that should be used to judge light paternalistic policies. Second,paternalism, and especially light paternalism, introduces new motives for attempting to understand the psychological processes underlying economic behavior. An enhanced understanding of process can help to explain why people make mistakes in the first place,and, more importantly, provide insights into what types of policies are likely to be effective in correcting the mistakes. Third, there is an acute need for testing different possible policies before implementing them on a large scale, which we argue is best done in the field rather than the lab. Fourth, in addition to methodological issues, there are pragmatic issues concerning who will implement light paternalistic policies, especially when they involve positive expenditures. We discuss how economic interests can be rechanneled to supportendeavors consistent with light paternalism.
Top/Economics	Rule Rationality vs. Act Rationality
Top/Psychology	The Paradox of Choice - Why More Is Less
Top/Psychology	Acquisition of Knowledge About Spatial Location in Rats: the Associative Mechanism
Top/Psychology	The Time Paradox: The New Psychology of Time That Will Change Your Life Your every significant choice -- **every important decision you make** -- is determined by a force operating deep inside your mind: your perspective on time -- your internal, personal time zone. This is the most influential force in your life, yet you are virtually unaware of it. Once you become aware of your personal time zone, you can begin to see and manage your life in exciting new ways. In The Time Paradox, Drs. Zimbardo and Boyd draw on thirty years of pioneering research to reveal, for the first time, how your individual time perspective shapes your life and is shaped by the world around you. Further, they demonstrate that your and every other individual's time zones interact to create national cultures, economics, and personal destinies.
Top/Arts/Music/Performance	Druga Godba Festival- performance of the tango quintet Astorpia In recent times, Slovene musicians have been busy proving that they can successfully test themselves against virtually any style of music, traditional or classical. For several years now, tango has been a part of everyday life here, and its popularity keeps on growing. This new, tight and well-honed ensemble- most of whose members have classical music training- can undoubtedly contribute greatly to increasing the popularity of tango in Slovenia still further, but they also offer much more than just reinterpretations of the old Argentinian standards. Since the name Astorpia itself alludes to one of the most important *new tango* artists, Astor Piazzolla it should come as no surprise that we find five of his compositions on their first album MAR DEL PLATA, among them Libertango and Invierno Porteno, which have already become classics and make up part of a cycle dedicated to the four seasons. In addition to these pieces, which Astorpia manage to imbue with renewed vigour and freshness, their repertoire includes compositions by a number of other artists. They are not averse to playing wittily with the established rhythms of tango, not even to placing them in a Balkan context, in a similar way to composer Milos Simic. In one of their songs, the group take a different direction entirely with a fiery Czardas. Viva el Tango!
Top/Arts/Music/Performance	San Jose jazz festival During the day the KDD 2007 attendees were in the conference rooms, but at night everyone was outside on the streets to listen to jazz on the San Jose Jazz Festival which celebrates: - bringing jazz legends to Silicon Valley - bringing music to schools - supporting local musicians - promoting emerging musicians and new jazz forms
Top/Arts/Music/Performance	Creative Commons Music Performance
Top/Computers/Networking	Introduction to Active Networks The goal of active networking is to create communication networks that reposition static, low-level network operation into dynamic, differentiated, and adaptable behavior. This allows communication hardware to be more fully used given that its operation can be tailored to specific application requirements. This also enables a more flexible and survivable communication network. Active networking decouples the network protocol from its transport by allowing easy insertion of protocols on top of the transport layer. Active networking also minimizes requirements for global agreement; it does not require years of standards negotiation to introduce new protocols. Active networking enables on-the-fly experimentation given easy insertion of new protocols and network applications, thus enabling the rapid deployment of new services and applications. The mechanism for implementing an active network is to enable communication packets to carry network code as well as data. This code may be installed on the fly into low-level network devices as the packet flows throughout the network. Ultimately, the essence and fundamental uniqueness of active networking is the flexibility introduced by a tight integration of code and data in service of the communication network. Both code and data flow within, and change the operation of, the network. Legacy networks have focused on improving the flow of data based on the fundamentals of analyses such as Shannons fundamental insights into entropy as well as analyses in support of moving bits, such as queuing theory. With the tighter integration of code and data in an active network, a broader view of information encompassing both code and data in the form of Kolmogorov complexity is required. In this form of analysis, there is a focus on code and data as a combined entity. Active packets in the Kolmogorov complexity framework may vary from static data, as in legacy networks, to pure executable code; as code, the packets act as small executable models of information. This presentation delves into some of these issues.
Top/Computers/Networking	Wireless Tech & Regulatory Reality: Policy and Fantasy in the 21st Century Sascha Meinrath has been described as a '[[http://www.savetheinternet.com/=coalition|community Internet pioneer]]' and an '[[http://infodev-study.oplan.org/the-study/1-background-and-introduction/copy4_of_1-background-and-introduction|entrepreneurial visionary]]' and is a well-known expert on [[http://en.wikipedia.org/wiki/Wireless_community_network|community wireless networks]] (CWNs) and [[http://en.wikipedia.org/wiki/Municipal_broadband|municipal broadband]]. Leading news sources, including [[http://www.economist.com/science/displayStory.cfm?story_id=3535732|the Economist]], [[http://www.nytimes.com/2006/09/27/technology/circuits/27fon.html?ex=1160539200&en=53c38adbd350e304&ei=5070|the New York Times]], [[http://www.thenation.com/blogs/edcut?pid=77928|the Nation]], and [[http://www.npr.org/templates/story/story.php?storyId=4834612|National Public Radio]], often cite Sascha's work in covering issues related to CWNs. Sascha is the Research Director for the [[http://www.newamerica.net/|New America Foundation's]] [[http://www.spectrumpolicy.org/|Wireless Future Program]]. Additionally, he coordinates the [[http://www.oswc.net/|Open Source Wireless Coalition]], a global partnership of open source wireless integrators, researchers, implementors and companies dedicated to the development of open source, interoperable, low-cost wireless technologies. He is a regular contributor to [[http://www.muniwireless.com/|MuniWireless.com]], the leading source for municipal wireless news and information, and a regular contributor to [[http://govtech.net/digitalcommunities|Government Technology's Digital Communities]], the online portal and comprehensive information resource for the public sector. Sascha has also worked with [[http://www.freepress.net/|Free Press]], the [[http://www.caida.org/|the Cooperative Association for Internet Data Analysis (CAIDA)]], the [[http://www.acornactivemedia.com/| Acorn Active Media Foundation]], the [[http://www.ethoswireless.com/|Ethos Group]], and the [[http://www.cuwin.net/|CUWiN Foundation]]. Sascha holds a Bachelor's Degree from [[http://www.yale.edu/|Yale University]] and a Master's Degree from the [[http://www.psych.uiuc.edu/|University of Illinois at Urbana-Champaign]], both in psychology. He is a Telecommunications Fellow at the University of Illinois in the [[http://www.comm.uiuc.edu/icr/|Institute for Communications Research]], where he is finishing his PhD on community empowerment and the impacts and interactions of participatory media, wireless communications, and emergent technologies. more >>> [[http://www.saschameinrath.com/|saschameinrath.com]]
Top/Computers/Networking	A New Way to look at Networking Today's research community congratulates itself for the success of the internet and passionately argues whether circuits or datagrams are the One True Way. Meanwhile the list of unsolved problems grows. Security, mobility, ubiquitous computing, wireless, autonomous sensors, content distribution, digital divide, third world infrastructure, etc., are all poorly served by what's available from either the research community or the marketplace. I'll use various strained analogies and contrived examples to argue that network research is moribund because the only thing it knows how to do is fill in the details of a conversation between two applications. Today as in the 60s problems go unsolved due to our tunnel vision and not because of their intrinsic difficulty. And now, like then, simply changing our point of view may make many hard things easy.
Top/Computers/Networking	Towards the Future of the Internet
Top/Computer_Science/Algorithmic_Information_Theory/Kolmogorov_Complexity	Introduction to Active Networks The goal of active networking is to create communication networks that reposition static, low-level network operation into dynamic, differentiated, and adaptable behavior. This allows communication hardware to be more fully used given that its operation can be tailored to specific application requirements. This also enables a more flexible and survivable communication network. Active networking decouples the network protocol from its transport by allowing easy insertion of protocols on top of the transport layer. Active networking also minimizes requirements for global agreement; it does not require years of standards negotiation to introduce new protocols. Active networking enables on-the-fly experimentation given easy insertion of new protocols and network applications, thus enabling the rapid deployment of new services and applications. The mechanism for implementing an active network is to enable communication packets to carry network code as well as data. This code may be installed on the fly into low-level network devices as the packet flows throughout the network. Ultimately, the essence and fundamental uniqueness of active networking is the flexibility introduced by a tight integration of code and data in service of the communication network. Both code and data flow within, and change the operation of, the network. Legacy networks have focused on improving the flow of data based on the fundamentals of analyses such as Shannons fundamental insights into entropy as well as analyses in support of moving bits, such as queuing theory. With the tighter integration of code and data in an active network, a broader view of information encompassing both code and data in the form of Kolmogorov complexity is required. In this form of analysis, there is a focus on code and data as a combined entity. Active packets in the Kolmogorov complexity framework may vary from static data, as in legacy networks, to pure executable code; as code, the packets act as small executable models of information. This presentation delves into some of these issues.
Top/Computer_Science/Algorithmic_Information_Theory/Minimum_Description_Length	Universal Modeling: Introduction to modern MDL We give a tutorial introduction to the *modern* Minimum Description Length (MDL) Principle, taking into account the many refinements and developments that have taken place in the 1990s. These do not seem to be widely known outside the information theory community. We will especially emphasize the use of MDL in classification. We also consider the connections between MDL, Bayesian inference, maximum entropy inference and structural risk minimization.
Top/Technology/Electronics	Energy Efficient Transistors Major technological shifts in solid-state device technology have typically been associated with improvements in device energy efficiency. This was true in the transition from vacuum tubes to bipolar transistors in the 1950s, and then again from bipolar transistors to MOSFETs in the 1970s. The next technological revolution will similarly stem from an improvement in energy efficiency at the device or at the circuit level. This lecture will explore ways in which new materials and transistors can extend the performance of electronic systems.
Top/Technology/Electronics	andEuros d.o.o. The andEuros company is active in electronics and software research and development to provide complete solutions on system architecture, sensors, sensor networks, mixed signal hardware designs, distributed software and hardware platforms, embedded systems, FPGA/VHDL system-on-chip design, powerline communications and sensor protocols.
Top/Computer_Science/Text_Mining/Tools	Text Garden
Top/Computer_Science/Text_Mining/Tools	Ontologies and Machine Learninig We address the problem of constructing light-weight ontology from social network data. As an example we use social network of a mid size research institution obtained based on e-mail communication. The main contribution is an architecture consisting from five major steps that enable transformation of the data from a given e-mail transactions recordings to an ontology estimating the structure of the organization. Once having a set of sparse vectors, we apply an approach to semi-automated ontology construction as implemented in the OntoGen tool. The experiments and illustrative evaluation show that our approach is useful and applicable in real life situations where the goal is to model social structures based on communication records.
Top/Computer_Science/Text_Mining/Tools	Ontogen Software Demo We address the problem of constructing light-weight ontology from social network data. As an example we use social network of a mid size research institution obtained based on e-mail communication. The main contribution is an architecture consisting from five major steps that enable transformation of the data from a given e-mail transactions recordings to an ontology estimating the structure of the organization. Once having a set of sparse vectors, we apply an approach to semi-automated ontology construction as implemented in the OntoGen tool. The experiments and illustrative evaluation show that our approach is useful and applicable in real life situations where the goal is to model social structures based on communication records.
Top/Biology/Ethology	Honeybees Moving Home - The Effect of Swarm Size on Decision-making Honeybee swarms are faced with a challenging task: how to decide on a new home. The way in which such a decision is made is a prime example of decentralised decision-making: only a small subset of bees in the swarm are involved in the process, each acting upon local information. Previous work has used individual-based simulations to unravel how the behaviour of the bees engaged in decision-making results in the swarm collectively choosing the best possible nest site. Here we explore how the size of the swarm affects the accuracy of the decision-making process when the presence of a certain number of bees at one nest site (quorum) is used to end the deliberations. As soon as a quorum is reached bees involved in the decision-making process indicate to the other bees on the swarm that a decision has been made and the swarm will prepare for lift-off. Assuming a fixed quorum size, we show that the larger the swarm, the less likely it will be able to select the best nest site as the quorum will be reached early in the process. We also show that the speed of the decision-making process is reduced in small swarms especially when only sites of mediocre quality have been discovered. Our results therefore indicate that large swarms will often make sub-optimal decisions while small swarms will require more time to select a nest site but that small swarms select, on average, better quality sites.
Top/Biology/Ethology	Information Transfer in Moving Animal Groups The movement of animal flocks give us one of the clearest examples of the concept of 'complexity'. Simple interactions between animals lead to patterns that are somehow regular but at the same time difficult to characterise. In this paper we discuss first these models and their predictions about the movement of flocks. We provide novel results about how information is transfered in these systems. We look at recent experiments on locusts and pigeons which show that at least some of the patterns seen in these groups can be explained by the phase transitions and bifurcations that arise from these models. In particular, we look at a phase transition in the marching of locusts and symmetry breaking in the decision-making of pigeons.
Top/Biology/Neuroscience	Biology in Four Dimensions How do early birds get up in time to catch the worms? A clock in our brains helps us maintain daily, or circadian, rhythms. Dr. Joseph S.&#160;Takahashi discusses the natural history of biological rhythms and explains how he and other scientists have unraveled the complex workings of the bodys clocks. Biological clocks are key to understanding jet lag, various sleep disorders, and why teenagers have a hard time rising early.
Top/Biology/Neuroscience	The Origin of the Human Mind: Insights from Brain Imaging and Evolution UCSD cognitive scientist Martin Sereno takes you on a captivating exploration of the brain's structure and function as revealed through&#160;investigations with new advanced imaging techniques and understandings of evolution.
Top/Biology/Neuroscience	The Mental Representation of Music: A Neural Darwinist Perspective In this presentation, I review the perceptual research related to auditory representation for music. The research suggests that multiple representations exist concurrently in the auditory system, and that the dominant representation is shaped by the specific auditory environment. I note that the research is consistent with theories of competitive representations, such as Edelman's neural Darwinist approach. I propose that the difference in predictive accuracy for different representations provides the feedback mechanism by which competing representations are selected. Repercussions for cognitive modeling of music are discussed.
Top/Biology/Neuroscience	Feasibility and Pragmatics of Classifying Working Memory Load with an Electroencephalograph We demonstrate high accuracies classifying working memory load using an electroencephalograph (EEG), even with little temporallag, not much training data, and a small number of EEG channels.
Top/Computer_Science/Machine_Learning/Active_Learning	Active, Semi-Supervised Learning for Textual Information Access
Top/Computer_Science/Machine_Learning/Active_Learning	Active Learning, Model Selection and Covariate Shift
Top/Computer_Science/Machine_Learning/Active_Learning	Agnostic Active Learning The great promise of active learning is that via interaction the number of samples required can be reduced to logarithmic in the number required for standard batch supervised learning methods. To achieve this promise, active learning must be able to cope with noisy data. We show how it is possible to cope with even malicious noise in an active learning setting, removing noise an obstacle to regular application of active learning.
Top/Computer_Science/Machine_Learning/Active_Learning	Hierarchical sampling for active learning We present an active learning scheme that exploits cluster structure in data.
Top/Computer_Science/Machine_Learning/Semi-supervised_Learning	Active, Semi-Supervised Learning for Textual Information Access
Top/Computer_Science/Machine_Learning/Semi-supervised_Learning	Semi-supervised Learning, Manifold Methods
Top/Computer_Science/Machine_Learning/Semi-supervised_Learning	Semi-supervised Learning, Manifold Methods
Top/Computer_Science/Machine_Learning/Semi-supervised_Learning	Semi-supervised Structured Prediction Models
Top/Computer_Science/Machine_Learning/Semi-supervised_Learning	Graphs Regularization for Data Sets and Images: Filtering and Semi-Supervised Classification
Top/Computer_Science/Machine_Learning/Semi-supervised_Learning	Semi-supervised Learning for Text Classification
Top/Computer_Science/Machine_Learning/Semi-supervised_Learning	Semi-supervised learning This presentation is an introduction to semi-supervised learning.
Top/Computer_Science/Machine_Learning/Semi-supervised_Learning	Semisupervised Learning Approaches
Top/Computer_Science/Machine_Learning/Semi-supervised_Learning	A Sample-Complexity Analysis of Learning from Labeled and Unlabeled Data
Top/Computer_Science/Machine_Learning/Semi-supervised_Learning	Machine Reading at Web Scale
Top/Mathematics/Operations_Research	Convex Optimization The lectures will provide an introduction to the theory and applications of convex optimization. ;The emphasis will be on results useful for convex modeling, i.e., recognizing and formulating convex optimization problems in practice. : - The first lecture will introduce some of the fundamental theory of convex sets and functions. : - In lecture 2 we will discuss general properties of convex optimization problems and define important standard classes (linear and quadratic programming, second-order cone programming, semidefinite programming) and their applications. : - In lecture 3 the material will be illustrated with various examples.
Top/Mathematics/Operations_Research	Numerical Methods for Solving Least Squares Problems with Constraints In this talk, we discuss the problem of solving linear least squares problems and Total Least Squares problems with linear constraints and/or a quadratic constraint. We are particularly interested in developing stable numerical methods when the data matrix is singular or near singular. Of particular interest are matrices which are large and sparse and for which iterative methods must be employed. The quadratically constrained problems arise in problems where regularization is required. For such problems, a Lagrange multiplier is required and that calculation may be quite intensive. The method we propose will quickly yield an estimate of the parameter and allow for finding the least squares solution.
Top/Mathematics/Operations_Research	Introduction to convex programming, interior point methods, and semi-definite programming
Top/Mathematics/Operations_Research	Gradient Methods for Machine Learning Gradient methods locally optimize an unknown differentiable function, and thus provide the engines that drive much machine learning. Here we'll take a look under the hood, beginning with brief overview of classical gradient methods for unconstrained optimization: * Steepest descent, * Newton's method * Levenberg-Marquardt * BFGS * Conjugate gradient. To cope with the flood of data we find ourselves in today, stochastic approximation of the gradient from subsamples of data becomes a necessity. Unfortunately the noise this introduces into the gradient is not tolerated well by the classical gradient methods, with the exception of steepest descent, which however is very slow to converge. We'll see how local step size adaptation can be used to accelerate the convergence of stochastic gradient descent, culminating in the recent stochastic meta-descent (SMD) algorithm. SMD requires certain Hessian-vector products which can be computed efficiently via algorithmic (or automatic) differentiation (AD), a set of techniques that help automate the correct implementation of gradient methods in general. We'll discuss the basic concepts of AD, and learn simple ways to implement the forward mode of AD, and with it the fast Hessian-vector product.
Top/Mathematics/Operations_Research	Rapid Stochastic Gradient Descent: Accelerating Machine Learning The incorporation of online learning capabilities into real-time computing systems has been hampered by a lack of efficient, scalable optimization algorithms for this purpose: second-order methods are too expensive for large, nonlinear models, conjugate gradient does not tolerate the noise inherent in online learning, and simple gradient descent, evolutionary algorithms, etc., are unacceptably slow to converge. I am addressing this problem by developing new ways to accelerate stochastic gradient descent, using second-order gradient information obtained through the efficient computation of curvature matrix-vector products. In the stochastic meta-descent (SMD) algorithm, this cheap curvature information is built up iteratively into a stochastic approximation of Levenberg-Marquardt second-order gradient steps, which are then used to adapt individual gradient step sizes. SMD handles noisy, correlated, non-stationary signals well, and approaches the rapid convergence of second-order methods at only linear cost per iteration, thus scaling up to extremely large nonlinear systems. To date it has enabled new adaptive techniques in computational fluid dynamics and computer vision. Our most recent development is a version of SMD operating in reproducing kernel Hilbert space.
Top/Mathematics/Operations_Research	Semi-supervised Learning, Manifold Methods
Top/Mathematics/Operations_Research	Trust Region Newton Methods for Large-Scale Logistic Regression Large-scale logistic regression arises in many applications such as document classification and natural language processing. In this paper, we apply a trust region Newton method to maximize the log-likelihood of the logistic regression model. The proposed method uses only approximate Newton steps in the beginning, but achieves fast convergence in the end. Experiments show that it is faster than the commonly used quasi Newton approach for logistic regression. We also compare it with linear SVM implementations.
Top/Mathematics/Operations_Research	Visualizing Pairwise Similarity via Semidefinite Programming
Top/Mathematics/Operations_Research	Learning the Kernel Matrix in Discriminant Analysis via Quadratically Constrained Quadratic Programming The kernel function plays a central role in kernel methods. In this paper, we consider the automated learning of the kernel matrix over a convex combination of pre-specified kernel matrices in Regularized Kernel Discriminant Analysis (RKDA), which performs linear discriminant analysis in the feature space via the kernel trick. Previous studies have shown that this kernel learning problem can be formulated as a semidefinite program (SDP), which is however computationally expensive, even with the recent advances in interior point methods. Based on the equivalence relationship between RKDA and least square problems in the binary-class case, we propose a Quadratically Constrained Quadratic Programming (QCQP) formulation for the kernel learning problem, which can be solved more efficiently than SDP. While most existing work on kernel learning deal with binary-class problems only, we show that our QCQP formulation can be extended naturally to the multi-class case. Experimental results on both binary-class and multiclass benchmark data sets show the efficacy of the proposed QCQP formulations.
Top/Mathematics/Operations_Research	Large-scale RLSC Learning Without Agony The advances in kernel-based learning necessitate the study on solving a large-scale non-sparse positive definite linear system. To provide a deterministic approach, recent researches focus on designing fast matrixvector multiplication techniques coupled with a conjugate gradient method. Instead of using the conjugate gradient method, our paper proposes to use a domain decomposition approach in solving such a linear system. Its convergence property and speed can be understood within von Neumann's alternating pro jection framework. We will report significant and consistent improvements in convergence speed over the conjugate gradient method when the approach is applied to recent machine learning problems.
Top/Mathematics/Operations_Research	A Quadratic Programming Approach to the Graph Edit Distance Problem In this paper we propose a quadratic programming approach to computing the edit distance of graphs. Whereas the standard edit distance is defined with respect to a minimum-cost edit path between graphs, we introduce the notion of fuzzy edit paths between graphs and provide a quadratic programming formulation for the minimization of fuzzy edit costs. Experiments on real-world graph data demonstrate that our proposed method is able to outperform the standard edit distance method in terms of recognition accuracy on two out of three data sets.
Top/Events	Students performing 'Easy' on the last day of the MLSS
Top/Events	Successes, Failures and Learning From Them Over the last eighteen years, the field of knowledge discovery and data mining has matured considerably. Although the field has evolved as a result of synergistic co-operation among researchers in databases, artificial intelligence, statistics and systems, it has maintained its own identity. From a single workshop in 1989, the field can now lay claim to at least 5 major conferences and numerous symposium devoted to its central theme.
Top/Events	KDD-07 Best Paper Awards
Top/Events	Successes, Failures and Learning From Them
Top/Events	Debate
Top/Events	Closing remarks
Top/Events	Introduction to the Panel Since the 1989 workshop on knowledge discovery in databases, the field has seen sustained growth and interest and has attained significant maturity. The main objectives of this panel will be to reflect on the successes and failures in the field of data mining over the last eighteen years and to examine what insights we can take with us as we move forward.
Top/Events	Welcome from the Program Chairs This proceedings is the published record of the Thirteenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-07) held in San Jose, California on August 1215, 2007. The KDD-07 conference provides a forum for novel research results and important applications in the area of data mining and knowledge discovery. The vibrancy, excitement and breadth of the field are reflected by the strong lineup of research papers, invited talks, tutorials and workshops at the conference. The conference and the proceedings represent the efforts of a large number of people. We would like to thank the Industrial and Government Applications Track Chairs, the members of the Organizing Committee, the members of the Program Committee (including the Research Track Senior Program Committee), the external reviewers, and the student volunteers who helped out at the conference. These individuals contributed many hours of their time to serve their scientific community and help make the conference as successful as it is. We would also like to thank the ACM staff and the conference sponsors for their support.
Top/Events	KDD Cup Winners
Top/Events	Debate about future meetings
Top/Events	Opening
Top/Events	Conclusion and Debate
Top/Events	Introduction to the On-line Trading of Exploration and Exploitation Workshop
Top/Events	Next Generation OSS Workshop
Top/Events	Introduction of SEKT partner - BT, UK
Top/Events	Plenary session-ECML poster highlights
Top/Events	Debate
Top/Events	Customer Viewpoint
Top/Computer_Science/Machine_Learning/Structured_data	Learning on Structured Data Discriminative learning framework is one of the very successful fields of machine learning. The methods of this paradigm, such as Boosting, and Support Vector Machines have significantly advanced the state-of-the-art for classification by improving the accuracy and by increasing the applicability of machine learning methods. One of the key benefits of these methods is their ability to learn efficiently in high dimensional feature spaces, either by the use of implicit data representations via kernels or by explicit feature induction. However, traditionally these methods do not exploit dependencies between class labels where more than one label is predicted. Many real-world classification problems involve sequential, temporal or structural dependencies between multiple labels. We will investigate recent research on generalizing discriminative methods to learning in structured domains. These techniques combine the efficiency of dynamic programming methods with the advantages of the state-of-the-art learning methods.
Top/Computer_Science/Machine_Learning/Structured_data	Correlation Search in Graph Databases Correlation mining has gained great success in many application domains for its ability to capture the underlying dependency between objects. However, the research of correlation mining from graph databases is still lacking despite the fact that graph data, especially in various scientific domains, proliferate in recent years. In this paper, we propose a new problem of correlation mining from graph databases, called Correlated Graph Search (CGS). CGS adopts Pearsons correlation coefficient as a correlation measure to take into consideration the occurrence distributions of graphs. However, the problem poses significant challenges, since every subgraph of a graph in the database is a candidate but the number of subgraphs is exponential. We derive two necessary conditions which set bounds on the occurrence probability of a candidate in the database. With this result, we design an efficient algorithm that operates on a much smaller projected database and thus we are able to obtain a significantly smaller set of candidates. To further improve the efficiency, we develop three heuristic rules and apply them on the candidate set to further reduce the search space. Our extensive experiments demonstrate the effectiveness of our method on candidate reduction. The results also justify the efficiency of our algorithm in mining correlations from large real and synthetic datasets.
Top/Computer_Science/Machine_Learning/Structured_data	Graph Matching Algorithms Graph matching plays a key role in many areas of computing from computer vision to networks where there is a need to determine correspondences between the components (vertices and edges) of two attributed structures. In recent years three new approaches to graph matching have emerged as replacements to more traditional heuristic methods. These new methods are: * Least squares - where the optimal correspondence in determined in terms of deriving the best fitting permutation matrix between sets. * Spectral methods - where optimal correspondences are derived via subspace projections in the graph eigenspaces. * Graphical models - where algorithms such as the junction tree algorithm are used to infer the optimal labeling of the nodes of one graph in terms of the other and that satisfy similarity constraints between vertices and edges. In this lecture we review and compare these methods and demonstrate examples where this applies to point set and line matching.
Top/Computer_Science/Machine_Learning/Structured_data	Fast Direction-Aware Proximity for Graph Mining In this paper we study asymmetric proximity measures on directed graphs, which quantify the relationships between two nodes or two groups of nodes. The measures are useful in several graph mining tasks, including clustering, link prediction and connection subgraph discovery. Our proximity measure is based on the concept of escape probability. This way, we strive to summarize the multiple facets of nodes-proximity, while avoiding some of the pitfalls to which alternative proximity measures are susceptible. A unique feature of the measures is accounting for the underlying directional information. We put a special emphasis on computational efficiency, and develop fast solutions that are applicable in several settings. Our experimental study shows the usefulness of our proposed direction-aware proximity method for several applications, and that our algorithms achieve a significant speedup (up to 50,000x) over straightforward implementations.
Top/Computer_Science/Machine_Learning/Structured_data	Probabilistic Inference for Graph Classification Graph data is getting increasingly popular in, e.g., bioinfor- matics and text processing. A main dificulty of graph data processing lies in the intrinsic high dimensionality of graphs, namely, when a graph is represented as a binary feature vector of indicators of all possible sub- graphs, the dimensionality gets too large for usual statistical methods.
Top/Computer_Science/Machine_Learning/Structured_data	Graph kernels and applications in chemoinformatics Several problems in chemistry can be formulated as classification or regression problems over molecules which, when represented by their planar structure, can be seen as labeled graphs. Several approaches have been proposed recently to define positive definite kernels over labeled graphs, paving the way to the use of powerful kernel methods in chemoinformatics. In this talk I will review some of these approaches and present relevant applications in computational chemistry.
Top/Computer_Science/Machine_Learning/Structured_data	Learning CRFs with Hierarchical Features: An Application to Go
Top/Computer_Science/Machine_Learning/Structured_data	Graph Embedding in Vector Spaces by Means of Prototype Selection The field of statistical pattern recognition is characterized by the use of feature vectors for pattern representation, while strings or, more generally, graphs are prevailing in structural pattern recognition. In this paper we aim at bridging the gap between the domain of feature based and graph based object representation. We propose a general approach for transforming graphs into n-dimensional real vector spaces by means of prototype selection and graph edit distance computation. This method establishes the access to the wide range of procedures based on feature vectors without loosing the representational power of graphs. Through various experimental results we show that the proposed method, using graph embedding and classification in a vector space, outperforms the tradional approach based on k-nearest neighbor classification in the graph domain.
Top/Computer_Science/Machine_Learning/Structured_data	Graph complexity for structure and learning The talk will consider ways of bounding the complexity of a graph as measured by the number of partitions satisfying certain properties. The approach adopted uses Vapnik Chervonenkis dimension techniques. An example of such a bound was given by Kleinberg et al (2004) with an application to network failure detection. We describe a new bound in the same vein that depends on the eigenvalues of the graph Laplacian. We show an application of the result to transductive learning of a graph labelling from examples.
Top/Computer_Science/Machine_Learning/Structured_data	Learning to Combine Distances for Complex Representations The k-Nearest Neighbors algorithm can be easily adapted to classify complex objects (e.g. sets, graphs) as long as a proper dissimilarity function is given over an input space. Both the representation of the learning instances and the dissimilarity employed on that representation should be determined on the basis of domain knowledge. However, even in the presence of domain knowledge, it can be far from obvious which complex representation should be used or which dissimilarity should be applied on the chosen representation. In this paper we present a framework that allows to combine different complex representations of a given learning problem and/or different dissimilarities defined on these representations. We build on ideas developed previously on metric learning for vectorial data. We demonstrate the utility of our method in domains in which the learning instances are represented as sets of vectors by learning how to combine different set distance measures.
Top/Computer_Science/Machine_Learning/Structured_data	Visual Categorization with Bags of Keypoints We present a novel method for generic visual categorization: the problem of identifying the object content of natural images while generalizing across variations inherent to the object class. This bag of keypoints method is based on vector quantization of affine invariant descriptors of image patches. We propose and compare two alternative implementations using different classifiers: Nave Bayes and SVM. The main advantages of the method are that it is simple, computationally efficient and intrinsically invariant. We present results for simultaneously classifying several semantic visual categories. These results clearly demonstrate that the method is robust to background clutter and produces good categorization accuracy even without exploiting geometric information.
Top/Computer_Science/Machine_Learning/Structured_data	Probabilistic Relaxation Labeling by Fokker-Planck Diffusion on a Graph In this paper we develop a new formulation of probabilistic relaxation labeling for the task of data classification using the theory of diffusion processes on graphs. The state space of our process as the nodes of a support graph which represent potential object-label assignments. The edge-weights of the support graph encode data-proximity and label consistency information. The state-vector of the diffusion process represents the object-label probabilities. The state vector evolves with time according to the Fokker-Planck equation.We show how the solution state vector can be estimated using the spectrum of the Laplacian matrix for the weighted support graph. Experiments on various data clustering tasks show effectiveness of our new algorithm.
Top/Computer_Science/Machine_Learning/Structured_data	XML structure mapping A key problem for automating the processing of semi-structured resources is the format heterogeneity among data sources. For dealing with heterogeneous semi-structured data, the correspondence between the different formats has to be established. The multiplicity and the rapid growth of information sources have motivated researchers to develop machine learning technologies for helping to automate those transformations.
Top/Computer_Science/Machine_Learning/Structured_data	SCAN: A Structural Clustering Algorithm for Networks Network clustering (or graph partitioning) is an important task for the discovery of underlying structures in networks. Many algorithms find clusters by maximizing the number of intra-cluster edges. While such algorithms find useful and interesting structures, they tend to fail to identify and isolate two kinds of vertices that play special roles&#160;- vertices that bridge clusters (hubs) and vertices that are marginally connected to clusters (outliers). Identifying hubs is useful for applications such as viral marketing and epidemiology since hubs are responsible for spreading ideas or disease. In contrast, outliers have little or no influence, and may be isolated as noise in the data. In this paper, we proposed a novel algorithm called SCAN (Structural Clustering Algorithm for Networks), which detects clusters, hubs and outliers in networks. It clusters vertices based on a structural similarity measure. The algorithm is fast and efficient, visiting each vertex only once. An empirical evaluation of the method using both synthetic and real datasets demonstrates superior performance over other methods such as the modularity-based algorithms.
Top/Computer_Science/Machine_Learning/Structured_data	Sequence Classification Using Statistical Pattern Recognition Sequence classification is a significant problem that arises in many different real-world applications. The purpose of a sequence classifier is to assign a class label to a given sequence. Also, to obtain the pattern that characterizes the sequence is usually very useful. In this paper, a technique to discover a pattern from a given sequence is presented followed by a general novel method to classify the sequence. This method considers mainly the dependencies among the neighbouring elements of a sequence. In order to evaluate this method, a UNIX command environment is presented, but the method is general enough to be applied to other environments.
Top/Computer_Science/Machine_Learning/Preprocessing	Introduction to feature selection This course covers feature selection fundamentals and applications. The students will first be reminded of the basics of machine learning algorithms and the problem of overfitting avoidance. In the wrapper setting, feature selection will be introduced as a special case of the model selection problem. Methods to derive principled feature selection algorithms will be reviewed as well as heuristic method, which work well in practice. One class will be devoted to feature construction techniques. Finally, a lecture will be devoted to the connections between feature section and causal discovery. The class will be accompanied by several lab sessions. The course will be attractive to students who like playing with data and want to learn practical data analysis techniques. The instructor has ten years of experience with consulting for startup companies in the US in pattern recognition and machine learning. Datasets from a variety of application domains will be made available: handwriting recognition, medical diagnosis, drug discovery, text classification, ecology, marketing.
Top/Computer_Science/Machine_Learning/Preprocessing	A Transductive Framework of Distance Metric Learning by Spectral Dimensionality Reduction Distance metric learning and nonlinear dimensionality reduction are two interesting and active topics in recent years. However, the connection between them is not thoroughly studied yet. In this paper, a transductive framework of distance metric learning is proposed and its close connection with many nonlinear spectral dimensionality reduction methods is elaborated. Furthermore, we prove a representer theorem for our framework, linking it with function estimation in an RKHS, and making it possible for generalization to unseen test samples. In our framework, it suffices to solve a sparse eigenvalue problem, thus datasets with 105 samples can be handled. Finally, experiment results on synthetic data, several UCI databases and the MNIST handwritten digit database are shown.
Top/Computer_Science/Machine_Learning/Preprocessing	Dimensionality Reduction by Feature Selection in Machine Learning Dimensionality reduction is a commonly used step in machine learning, especially when dealing with a high dimensional space of features. The original feature space is mapped onto a new, reduced dimensioanllyity space and the examples to be used by machine learning algorithms are represented in that new space. The mapping is usually performed either by selecting a subset of the original features or/and by constructing some new features. This persentation deals with the first approach, feature subset selection. We provide a brief overview of the feature subset selection techniques that are commonly used in machine learning and give a more detailed description of feature subset selection used in machine learning on text data. Performance of some methods used is document categorization is illustrated by providing experimental comparison on real-world data collected from the Web.
Top/Computer_Science/Machine_Learning/Preprocessing	Adaptive Dimension Reduction Using Discriminant Analysis and K-means Clustering Regularized Kernel Discriminant Analysis (RKDA) performs linear discriminant analysis in the feature space via the kernel trick. The performance of RKDA depends on the selection of kernels. In this paper, we consider the problem of learning an optimal kernel over a convex set of kernels. We show that the kernel learning problem can be formulated as a semidefinite program (SDP) in the binary-class case. We further extend the SDP formulation to the multi-class case. It is based on a key result established in this paper, that is, the multi-class kernel learning problem can be decomposed into a set of binary-class kernel learning problems. In addition, we propose an approximation scheme to reduce the computational complexity of the multi-class SDP formulation. The performance of RKDA also depends on the value of the regularization parameter. We show that this value can be learned automatically in the framework. Experimental results on benchmark data sets demonstrate the efficacy of the proposed SDP formulations.
Top/Computer_Science/Machine_Learning/Preprocessing	Feature Selection and Causality Inference
Top/Computer_Science/Machine_Learning/Preprocessing	Robust Non-linear Dimensionality Reduction using Successive 1-Dimensional Laplacian Eigenmapse Non-linear dimensionality reduction of noisy data is a challenging problem encountered in a variety of data analysis applications. Recent results in the literature show that spectral decomposition, as used for example by the Laplacian Eigenmaps algorithm, provides a powerful tool for non-linear dimensionality reduction and manifold learning. In this paper, we discuss a significant shortcoming of these approaches, which we refer to as the repeated eigendirections problem. We propose a novel approach that combines successive 1dimensional spectral embeddings with a data advection scheme that allows us to address this problem. The proposed method does not depend on a non-linear optimization scheme; hence, it is not prone to local minima. Experiments with artificial and real data illustrate the advantages of the proposed method over existing approaches. We also demonstrate that the approach is capable of correctly learning manifolds corrupted by significant amounts of noise.
Top/Computer_Science/Machine_Learning/Preprocessing	Map Building without Localization by Dimensionality Reduction Techniques This paper proposes a new map building framework for mobile robot named Localization-Free Mapping by Dimensionality Reduction (LFMDR). In this framework, the robot map building is interpreted as a problem of reconstructing the 2-D coordinates of ob jects so that they maximally preserve the local proximity of the ob jects in the space of robot's observation history. Not only traditional linear PCA but also recent manifold learning techniques can be used for solving this problem. In contrast to the SLAM framework, LFMDR framework does not require localization procedures nor explicit measurement and motion models. In the latter part of this paper, we will demonstrate 'visibility-only' and 'bearing-only' localization-free mappings which are derived by applying LFMDR framework to the visibility and bearing measurements respectively.
Top/Computer_Science/Machine_Learning/Preprocessing	The Sparse Grid Method The sparse grid method is a special discretization technique, which allows to cope with the curse of dimensionality to some extent. It is based on a hierarchical basis and a sparse tensor product decompositon. Sparse grids have been successfully used to solve partial differential equations in the past and, more recently, have been shown to be competitive for learning problems as well. The lecture will provide a general introduction to the major properties of sparse grids and present the sparse grid combination technique for classification and regression.
Top/Computer_Science/Machine_Learning/Preprocessing	Multi-task feature learning We present a method for learning a low-dimensional representation which is shared across a set of multiple related tasks. The method builds upon the well-known 1-norm regularization problem using a new regularizer which controls the number of learned features common for all the tasks. We show that this problem is equivalent to a convex optimization problem and develop an iterative algorithm for solving it.
Top/Computer_Science/Machine_Learning/Preprocessing	What Mental States? Exploring How Dimensionality Reduction Might Contribute to the Refinement of Cognitive Models Questions in cognitive neuroscience are often framed in terms of correspondences between known types: How is brain state X related to cognitive state Y? What are the correlations or mappings between particular structures and functions? Such framings are well suited for confirmatory testing of coarse-grained hypotheses. They are not necessarily informative, however, for the purpose of exploring finer physical and functional structure. To the contrary, physical states are typically aggregated over anatomical regions of interest, while tasks are designed to optimize one or a few functional contrasts of interest rather than to cover a fuller behavioral or cognitive range.
Top/Computer_Science/Machine_Learning/Preprocessing	Adaptive Feature Selection in Image Segmentation Most practical image segmentation algorithms optimize some mathematical similarity criterion derived from several low-level image features. One possible way of combining different types of features, e.g. color- and texture features on different scales and/or different orientations, is to simply stack all the individual measurements into one high-dimensional feature vector. Due to the nature of such stacked vectors, however, only very few components (e.g. those which are defined on a suitable scale) will carry information that is relevant for the actual segmentation task. We present a novel approach to combining segmentation and feature selection that is capable of overcoming this relevance determination problem. It implements a wrapper strategy for feature selection, in the sense that the features are directly selected by optimizing thediscriminative power of the used partitioning algorithm. On the technical side, we present an efficient optimization algorithm with guaranteed local convergence property. All free model parameters of this method are selected by a resampling-based stability analysis. Experiments for both toy examples and real-world images demonstrate that the built-in feature selection mechanism leads to stable and meaningful partitions of the images.
Top/Computer_Science/Machine_Learning/Preprocessing	Identifying Feature Relevance using a Random Forest Many feature selection algorithms are limited in that they attempt to identify relevant feature subsets by examining the features individually. This paper introduces a technique for determining feature relevance using the average information gain achieved during the construction of decision tree ensembles. The technique introduces a node complexity measure and a statistical method for updating the feature sampling distribution based upon confidence intervals to control the rate of convergence. Experiments demonstrate the potential of this method for feature selection and subspace identification.
Top/Computer_Science/Machine_Learning/Preprocessing	Optimal Dimensionality of Metric Space for Classification For large-scale classification problems, the training samples can be clustered beforehand as a downsampling pre-process, and then only the obtained clusters are used for training. Motivated by such assumption, we proposed a classification algorithm, Support Cluster Machine (SCM), within the learning framework introduced by Vapnik. For the SCM, a compatible kernel is adopted such that a similarity measure can be handled not only between clusters in the training phase but also between a cluster and a vector in the testing phase. We also proved that the SCM is a general extension of the SVM with the RBF kernel. The experimental results confirm that the SCM is very effective for largescale classification problems due to significantly reduced computational costs for both training and testing and comparable classification accuracies. As a by-product, it provides a promising approach to dealing with privacy-preserving data mining problems.
Top/Computer_Science/Machine_Learning/Preprocessing	Graph Embedding in Vector Spaces by Means of Prototype Selection The field of statistical pattern recognition is characterized by the use of feature vectors for pattern representation, while strings or, more generally, graphs are prevailing in structural pattern recognition. In this paper we aim at bridging the gap between the domain of feature based and graph based object representation. We propose a general approach for transforming graphs into n-dimensional real vector spaces by means of prototype selection and graph edit distance computation. This method establishes the access to the wide range of procedures based on feature vectors without loosing the representational power of graphs. Through various experimental results we show that the proposed method, using graph embedding and classification in a vector space, outperforms the tradional approach based on k-nearest neighbor classification in the graph domain.
Top/Computer_Science/Machine_Learning/Preprocessing	Cleaning Disguised Missing Data: A Heuristic Approach In some applications such as filling in a customer information form on the web, some missing values may not be explicitly represented as such, but instead appear as potentially valid data values. Such missing values are known as disguised missing data, which may impair the quality of data analysis severely, such as causing significant biases and misleading results in hypothesis tests, correlation analysis and regressions. The very limited previous studies on cleaning disguised missing data use outlier mining and distribution anomaly detection. They highly rely on domain background knowledge in specific applications and may not work well for the cases where the disguise values are inliers. To tackle the problem of cleaning disguised missing data, in this paper, we first model the distribution of disguised missing data, and propose the embedded unbiased sample heuristic. Then, we develop an effective and efficient method to identify the frequently used disguise values which capture the major body of the disguised missing data. Our method does not require any domain background knowledge to find the suspicious disguise values. We report an empirical evaluation using real data sets, which shows that our method is effective the frequently used disguise values found by our method match the values identified by the domain experts nicely. Our method is also efficient and scalable for processing large data sets.
Top/Computer_Science/Machine_Learning/Preprocessing	Learning with spectral representations and use of MDL principles
Top/Computer_Science/Machine_Learning/Preprocessing	Landscape Multidimensional Scaling We revisit the problem of representing a high-dimensional data set by a distance-preserving projection onto a two-dimensional plane. This problem is solved by well-known techniques, such as multidimensional scaling. There, the data is projected onto a flat plane and the Euclidean metric is used for distance calculation. In real topographic maps, however, travel distance (or time) is not determined by (Euclidean) distance alone, but also influenced by map features such as mountains or lakes. We investigate how to utilize landscape features for a distance-preserving projection. A first approach with rectangular cylindrical mountains in the MDS landscape is presented.
Top/Computer_Science/Machine_Learning/Preprocessing	Relational Data Pre-Processing Techniques for Improved Securities Fraud Detection Commercial datasets are often large, relational, and dynamic. They contain many records of people, places, things, events and their interactions over time. Such datasets are rarely structured appropriately for knowledge discovery, and they often contain variables whose meanings change across different subsets of the data. We describe how these challenges were addressed in a collaborative analysis project undertaken by the University of Massachusetts Amherst and the National Association of Securities Dealers (NASD). We describe several methods for data preprocessing that we applied to transform a large, dynamic, and relational dataset describing nearly the entirety of the U.S. securities industry, and we show how these methods made the dataset suitable for learning statistical relational models. To better utilize social structure, we first applied known consolidation and link formation techniques to associate individuals with branch office locations. In addition, we developed an innovative technique to infer professional associations by exploiting dynamic employment histories. Finally, we applied normalization techniques to create a suitable class label that adjusts for spatial, temporal, and other heterogeneity within the data. We show how these pre-processing techniques combine to provide the necessary foundation for learning high-performing statistical models of fraudulent activity.
Top/Computer_Science/Machine_Learning/Preprocessing	Grouping Using Factor Graphs: an Approach for Finding Text with a Camera Phone We introduce a new framework for feature grouping based on factor graphs, which are graphical models that encode interactions among arbitrary numbers of random variables. The ability of factor graphs to express interactions higher than pairwise order (the highest order encountered in most graphical models used in computer vision) is useful for modeling a variety of pattern recognition problems. In particular, we show how this property makes factor graphs a natural framework for performing grouping and segmentation, which we apply to the problem of finding text in natural scenes. We demonstrate an implementation of our factor graph-based algorithm for finding text on a Nokia camera phone, which is intended for eventual use in a camera phone system that finds and reads text (such as street signs) in natural environments for blind users.
Top/Computer_Science/Machine_Learning/Preprocessing	What is the Optimal Number of Features? A learning theoretic perspective In this paper we discuss the problem of feature selection for supervised learning from the standpoint of statistical machine learning. We inquire what subset of features will lead to the best classification accuracy. It is clear that if the statistical model is known, or if there are an unlimited number of training samples, any additional feature can only improve the accuracy. However, we explicitly show that when the training set is finite, using all the features may be suboptimal, even if all the features are independent and carry information on the label. We analyze one setting analytically and show how feature selection can increase accuracy. We also find the optimal number of features as a function of the training set size for a few specific examples. This perspective on feature selection is different from the common approach that focuses on the probability that a specific algorithm will pick a completely irrelevant or redundant feature.
Top/Computer_Science/Evolutionary_Computation	Dance evolution The only submission from undergraduates, this unique video challenges AI to learn how to dance by demonstrating how neuroevolution can be used to (interactively) evolve dancing techniques.
Top/Computer_Science/Evolutionary_Computation	Evolutionary Algorithms It has been a century and a half since Darwin provided the first mechanistic explanation for the complexity of the living things we see around us. Only in the last 30 years or so have computational systems been employed to try out natural selection on complex artificial problems. There have been some successes, but the complexity of artificially evolved systems remains a very long way short of the complexity that is easy to find in biology. Why is this? Is our understanding of natural evolution missing something important? How can we improve our artificial problem solving methods to make them work better on large-scale complex problems?
Top/Computer_Science/Evolutionary_Computation	Biological Principles of Swarm Intelligence
Top/Computer_Science/Search_Engines	Experiences with the Nutch search engine Nutch is open-source software that implements a web search engine. It has been used in a variety of applications: vertical search engines, archival web search, search engines that incorporate novel metadata, etc. Nutch is itself implemented using Hadoop, an open-source platform for scalable computing. Hadoop facilitates the development and management of applications that run on large numbers of computers and on very large datasets. Hadoop has been demonstrated on clusters with hundreds of computers and is designed to scale to thousands of computers. This talk will present the architecture, capabilities and current status of these two projects.
Top/Computer_Science/Search_Engines	Building blocks for semantic search engines: Ranking and compact indexing in entity-relation graphs We see an evolutionary path to supporting semantic search over text facilitated by 1. extractors and annotators for ever-growing collections of entity and relation types and 2. search systems that exploit a smooth continuum between structured entities and relations on one hand and uninterpreted text on the other. The extractors and annotators will be imperfect and incomplete.
Top/Computer_Science/Search_Engines	Personalized Web Search Engine for Mobile Devices
Top/Computer_Science/Search_Engines	Some Key Challenges in Web Crawlers and Content-Based Search Engines
Top/Computer_Science/Search_Engines	Can Social Bookmarks Improve Web Search? Social bookmarking is a recent phenomenon which has the potential to give us a great deal of data about pages on the web. One major question is whether that data can be used to augment systems like web search. To answer this question, over the past year we have gathered what we believe to be the largest dataset from a social bookmarking site yet analyzed by academic researchers. Our dataset represents about forty million bookmarks from the social bookmarking site del.icio.us. We contribute a characterization of posts to del.icio.us: how many bookmarks exist (about 115 million), how fast is it growing, and how active are the URLs being posted about (quite active). We also contribute a characterization of tags used by bookmarkers. We found that certain tags tend to gravitate towards certain domains, and vice versa. We also found that tags occur in over 50 percent of the pages that they annotate, and in only 20 percent of cases do they not occur in the page text, backlink page text, or forward link page text of the pages they annotate. We conclude that social bookmarking can provide search data not currently provided by other sources, though it may currently lack the size and distribution of tags necessary to make a significant impact.
Top/Computer_Science/Search_Engines	The Future of Image Search There are billions of images on the Internet. Today, searching for a desired image is largely based on textual data such as filename or associated text on the web page; not much use is made of the image content. There are good reasons for this. The field of content-based image retrieval, which emerged during the 1990s, focused primarily on color and texture cues. These were easier to model than shape, but they turned out to be much less useful than originally hoped. I shall review some of the recent developments in the field of visual object recognition in the computer vision community that offer greater promise. Much better image features for characterizing shape, advances in machine learning techniques, and the availability of large amounts of training data lie at the heart of these approaches.
Top/Mathematics/Game_Theory	Online Learning and Game Theory We consider online learning and its relationship to game theory. In an online decision-making problem, as in Singer's lecture, one typically makes a sequence of decisions and receives feedback immediately after making each decision. As far back as the 1950's, game theorists gave algorithms for these problems with strong regret guarantees. Without making statistical assumptions, these algorithms were guaranteed to perform nearly as well as the best single decision, where the best is chosen with the benefit of hindsight. We discuss applications of these algorithms to complex learning problems where one receives very little feedback. Examples include online routing, online portfolio selection, online advertizing, and online data structures. We also discuss applications to learning Nash equilibria in zero-sum games and learning correlated equilibria in general two-player games.
Top/Mathematics/Game_Theory	Exploration exploitation in Go: UCT for Monte-Carlo Go
Top/Mathematics/Game_Theory	Game theoretic models in molecular biology There are many challenges in computational modeling of biological processes. Few processes such as signaling pathways operate in- dependently of others but rather involve substantial coordination and shared resources. The level of abstraction appropriate for understand- ing different processes, e.g, viewing a pathway as a filter or a molecular cascade, varies by context and the type of predictions sought.
Top/Mathematics/Game_Theory	Learning CRFs with Hierarchical Features: An Application to Go
Top/Mathematics/Game_Theory	Basic Concepts of Game Theory
Top/Mathematics/Game_Theory	Machine Learning, Market Design, and Advertising Given the complexity of preferences in markets such as key word advertising it is hard to believe that the de facto standard, decentralized, local, greedy algorithm (advertisers bid for clicks on keywords) is any where close to being optimal for any reasonable objective (welfare, profit, etc.). In this talk we consider the market design problem from a global perspective. We make connections between machine learning theory and market design theory, where machine learning design problems closely mirror game theoretic design problems. We reduce a general theoretical market design problem to a natural machine learning optimization problem. These theoretical results lead to a number of practical answers to advertising market design questions.
Top/Computer_Science/Machine_Learning/Computational_Learning_Theory	Online Learning and Game Theory We consider online learning and its relationship to game theory. In an online decision-making problem, as in Singer's lecture, one typically makes a sequence of decisions and receives feedback immediately after making each decision. As far back as the 1950's, game theorists gave algorithms for these problems with strong regret guarantees. Without making statistical assumptions, these algorithms were guaranteed to perform nearly as well as the best single decision, where the best is chosen with the benefit of hindsight. We discuss applications of these algorithms to complex learning problems where one receives very little feedback. Examples include online routing, online portfolio selection, online advertizing, and online data structures. We also discuss applications to learning Nash equilibria in zero-sum games and learning correlated equilibria in general two-player games.
Top/Computer_Science/Machine_Learning/Computational_Learning_Theory	Foundations of Learning
Top/Computer_Science/Machine_Learning/Computational_Learning_Theory	Online Learning and Bregman Divergences L 1: Introduction to Online Learning (Predicting as good as the best expert, Predicting as good as the best linear combination of experts, Additive versus multiplicative family of updates) L 2: Bregman divergences and Loss bounds (Introduction to Bregman divergences, Relative loss bounds for the linear case, Nonlinear case & matching losses, Duality and relation to exponential families) L 3: Extensions, interpretations, applications (Online to Batch Conversions, Prior information on the weight vector, Some applications)
Top/Computer_Science/Machine_Learning/Computational_Learning_Theory	How to predict with Bayes, MDL, and Experts Most passive Machine Learning tasks can be (re)stated as sequence prediction problems. This includes pattern recognition, classification, time-series forecasting, and others. Moreover, the understanding of passive intelligence also serves as a basis for active learning and decision making. In the recent past, rich theories for sequence prediction have been developed, and this is still an ongoing process. On the other hand, we are arriving at the stage where some important results are already termed classical. While much of the current Learning Theory is formulated under the assumption of independent and identically distributed (i.i.d.) observations, this lecture series focusses on situations without this prerequisite (e.g. weather or stock-market time-series).
Top/Computer_Science/Machine_Learning/Computational_Learning_Theory	A Domain Adaptation Formal Framework Addressing the Training/Test Distribution Gap
Top/Computer_Science/Machine_Learning/Computational_Learning_Theory	Graph complexity for structure and learning The talk will consider ways of bounding the complexity of a graph as measured by the number of partitions satisfying certain properties. The approach adopted uses Vapnik Chervonenkis dimension techniques. An example of such a bound was given by Kleinberg et al (2004) with an application to network failure detection. We describe a new bound in the same vein that depends on the eigenvalues of the graph Laplacian. We show an application of the result to transductive learning of a graph labelling from examples.
Top/Computer_Science/Machine_Learning/Computational_Learning_Theory	Generalization bounds When a learning algorithm produces a classifier, a natural question to ask is 'How well will it do in the future?' To make statements about the future given the past, some assumption must be made. If we make only an assumption that all examples are drawn independently and identically from some (unknown) distribution, we can answer the question. The answer to this question is directly applicable to classifier testing and confidence reporting. It also provides a simple general explanation of 'overfitting', and influences algorithm design.
Top/Computer_Science/Machine_Learning/Computational_Learning_Theory	Entropy Properties of a Decision Rule Class in Connection with machine learning abilities Many methods of Machine Learning are based on the idea of empirical risk minimisation. It is to find a decision rule or a model from some set which most perfectly fits the data presented in the training set. This idea is based on the large number law: empirical risk converges to real risk, if the training set is large enough. But if the class of decision rules or models is too large (in some sense) one meets the problem of oferfitting, the model perfectly corresponds to the data presented in the training set, but shows large errors on new data. It is due to the fact that only uniform convergence of empirical risk to the real risk guarantees closeness of the optimal model behaviour on the training set and on the new data. We introduce the notion of entropy of a decision rule class over a fixed sample sequence as log of the number of possible classifications of the sequence by the rules of the class. Maximum entropy over sequences of a fixed length l determines sufficient condition of the uniform convergence and corresponding estimates. But only average entropy H(l) behaviour determine necessary and sufficient condition of the uniform convergence. The condition is that H(l) / l (average entropy per symbol) should go to zero when the sequence length goes to infinity. If the condition does not hold then there exists a set of objects with non zero probability measure, such that almost all sequences of arbitrary finite length from this set may be divided in all possible ways by the rules of the class. One can easily see, that in this case overfitting is inevitable. Similar results are found for real dependencies instead of decision rules.
Top/Computer_Science/Machine_Learning/Computational_Learning_Theory	Who is Afraid of Non-Convex Loss Functions? The NIPS community has suffered of an acute convexivitis epidemic: - ML applications seem to have trouble moving beyond logistic regression, SVMs, and exponential-family graphical models; - For a new ML model, convexity is viewed as a virtue; - Convexity is sometimes a virtue; - But it is often a limitation. ML theory has essentially never moved beyond convex models - the same way control theory has not really moved beyond linear systems.
Top/Computer_Science/Machine_Learning/Computational_Learning_Theory	Concentration Inequalities with Machine Learning Applications
Top/Computer_Science/Machine_Learning/Computational_Learning_Theory	A Sample-Complexity Analysis of Learning from Labeled and Unlabeled Data
Top/Computer_Science/Machine_Learning/Computational_Learning_Theory	Exploration - Exploitation for Statistical Software Testing
Top/Computer_Science/Machine_Learning/Computational_Learning_Theory	What is the Optimal Number of Features? A learning theoretic perspective In this paper we discuss the problem of feature selection for supervised learning from the standpoint of statistical machine learning. We inquire what subset of features will lead to the best classification accuracy. It is clear that if the statistical model is known, or if there are an unlimited number of training samples, any additional feature can only improve the accuracy. However, we explicitly show that when the training set is finite, using all the features may be suboptimal, even if all the features are independent and carry information on the label. We analyze one setting analytically and show how feature selection can increase accuracy. We also find the optimal number of features as a function of the training set size for a few specific examples. This perspective on feature selection is different from the common approach that focuses on the probability that a specific algorithm will pick a completely irrelevant or redundant feature.
Top/Computer_Science/Machine_Learning/Computational_Learning_Theory	Presentation of proposed outline challenge
Top/Science/Research_Ethics	Explanation of objective, issue and element of strategy
Top/Computer_Science/Intelligent_Agents	BICA: An idea that can change the world This describes and demonstrates in simulation the capabilities of an agent controlled by a biologically-inspired cognitive architecture.
Top/Computer_Science/Intelligent_Agents	Leonardo: Goal assistance with divergent beliefs This demonstrates the robot Leonardo's ability to reason and act competently in situations when the other (in this case, human) agents have different belief states.
Top/Computer_Science/Intelligent_Agents	Cosmo: The lifelike pedagogical agent
Top/Computer_Science/Intelligent_Agents	Motion planning of multiple agents in virtual environments Describes and demonstrates in simulation the use of coordination graphs to avoid collisions of multiple agents in tasks requiring motion of multiple agents.
Top/Computer_Science/Machine_Learning/Structured_Output	Learning Similarity Metrics with Invariance Properties
Top/Computer_Science/Machine_Learning/Structured_Output	Learning to align: a statistical approach We present a new machine learning approach to the inverse parametric sequence alignment problem: given as training examples a set of correct pairwise global alignments, find the parameter values that make these alignments optimal.We consider the distribution of the scores of all incorrect alignments, then we search for those parameters for which the score of the given alignments is as far as possible from this mean, measured in number of standard deviations. This normalized distance is called the Z-score in statistics. We show that the Z-score is a function of the parameters and can be computed with efficient dynamic programs similar to the Needleman-Wunsch algorithm.We also show that maximizing the Z-score boils down to a simple quadratic program. Experimental results demonstrate the effectiveness of the proposed approach.
Top/Computer_Science/Machine_Learning/Structured_Output	Learning Distance Function by Coding Similarity We consider the problem of learning a similarity function from a set of positive equivalence constraints, i.e. 'similar' point pairs. We define the similarity in information theoretic terms, as the gain in coding length when shifting from independent encoding of the pair to joint encoding. Under simple Gaussian assumptions, this formulation leads to a non-Mahalanobis similarity function which is effcient and simple to learn. This function can be viewed as a likelihood ratio test, and we show that the optimal similaritypreserving pro jection of the data is a variant of Fisher Linear Discriminant. We also show that under some naturally occurring sampling conditions of equivalence constraints, this function converges to a known Mahalanobis distance (RCA). The suggested similarity function exhibits superior performance over alternative Mahalanobis distances learnt from the same data. Its superiority is demonstrated in the context of image retrieval and graph based clustering, using a large number of data sets.
Top/Computer_Science/Machine_Learning/Structured_Output	Learning with Structured Output
Top/Computer_Science/Machine_Learning/Structured_Output	Semi-supervised Structured Prediction Models
Top/Computer_Science/Machine_Learning/Structured_Output	Multi-task feature learning We present a method for learning a low-dimensional representation which is shared across a set of multiple related tasks. The method builds upon the well-known 1-norm regularization problem using a new regularizer which controls the number of learned features common for all the tasks. We show that this problem is equivalent to a convex optimization problem and develop an iterative algorithm for solving it.
Top/Computer_Science/Machine_Learning/Structured_Output	Estimation of gradients and coordinate covariation in classification We introduce an algorithm that simultaneously estimates a classification function as well as its gradient in the supervised learning framework. The motivation for the algorithm is to find salient variables and estimate how they covary. An efficient implementation with respect to both memory and time is given.
Top/Computer_Science/Machine_Learning/Structured_Output	Top-down vs. bottom-up methods for hierarchical classification We deal with hierarchical classification in the general case when an instance could be associated with multiple and/or partial paths in a given taxonomy. We approach the problem from different perspectives: top-down vs. bottom-up, but also on-line vs. batch and theoretical vs. experimental. In this talk, we briefly present our recent research experience on this subject matter.
Top/Computer_Science/Machine_Learning/Structured_Output	Completion of biological networks : the output kernel trees approach Elucidating biological networks appears nowadays as one of the most important challenge in systems biology. Due to the availability of various sources of data, machine learning has to play a major role regarding this issue, given its large spectrum of tools ranging from generative models to concept learning methods. In this work the focus is narrowed on the completion of biological interactions networks for which some of the interactions between variables (usually genes or proteins) are already known.
Top/Computer_Science/Machine_Learning/Structured_Output	Structural Prediction in Statistical Alignment and Translation
Top/Computer_Science/Machine_Learning/Structured_Output	Learning to Compare Examples
Top/Computer_Science/Machine_Learning/Structured_Output	Learning a Distance Metric for Structured Network Prediction Man-made or naturally-formed networks typically exhibit a high degree of structural regularity. In this paper, we introduce the problem of structured network prediction: given a set of n entities and a desired distribution for connectivity, return a likely set of edges connecting the entities together in a network having the specified degree distribution. Prediction is useful for initializing a network, augmenting an existing network, and for filtering existing networks, when the structure of the network is known. In order to capture the inter-dependencies amongst pairwise predictions to learn parameters of our model, we build upon recent structured output models. Novel in our approach is the use of partially labeled training examples, and a network structure sensitive loss function. We present encouraging results of the model predicting equivalence graphs and links in a social network.
Top/Computer_Science/Machine_Learning/Structured_Output	Learning to Combine Distances for Complex Representations The k-Nearest Neighbors algorithm can be easily adapted to classify complex objects (e.g. sets, graphs) as long as a proper dissimilarity function is given over an input space. Both the representation of the learning instances and the dissimilarity employed on that representation should be determined on the basis of domain knowledge. However, even in the presence of domain knowledge, it can be far from obvious which complex representation should be used or which dissimilarity should be applied on the chosen representation. In this paper we present a framework that allows to combine different complex representations of a given learning problem and/or different dissimilarities defined on these representations. We build on ideas developed previously on metric learning for vectorial data. We demonstrate the utility of our method in domains in which the learning instances are represented as sets of vectors by learning how to combine different set distance measures.
Top/Computer_Science/Machine_Learning/Structured_Output	Targeted PDF Learning
Top/Computer_Science/Machine_Learning/Structured_Output	Ranking as Learning Structured Outputs
Top/Computer_Science/Machine_Learning/Structured_Output	Mixtures of Hierarchical Topics with Pachinko Allo cation The four-level pachinko al location model (PAM) (Li & McCallum, 2006) represents correlations among topics using a DAG structure. It does not, however, represent a nested hierarchy of topics, with some topical word distributions representing the vocabulary that is shared among several more specific topics. This paper presents hierarchical PAM -- an enhancement that explicitly represents a topic hierarchy. This model can be seen as combining the advantages of hLD's topical hierarchy representation with PAM's ability to mix multiple leaves of the topic hierarchy. Experimental results show improvements in likelihood of held-out documents, as well as mutual information between automatically-discovered topics and humangenerated categories such as journals.
Top/Computer_Science/Machine_Learning/Structured_Output	Going beyond bag-of-words: dealing with a text as a graph of triples
Top/Computer_Science/Machine_Learning/Structured_Output	XML structure mapping A key problem for automating the processing of semi-structured resources is the format heterogeneity among data sources. For dealing with heterogeneous semi-structured data, the correspondence between the different formats has to be established. The multiplicity and the rapid growth of information sources have motivated researchers to develop machine learning technologies for helping to automate those transformations.
Top/Computer_Science/Machine_Learning/Structured_Output	Learning with Structured Output
Top/Computer_Science/Machine_Learning/Structured_Output	Magic Moments: Moment-based Approaches to Structured Output Prediction
Top/Computer_Science/Machine_Learning/Structured_Output	Ranking Individuals by Group Comparisons We discuss the problem of ranking individuals from their group competition results. Many real-world problems are of this type. For example, ranking players from team games is important in some sports. In machine learning, this is closely related to multi-class classification and probability estimates. We propose new models for estimating individuals' abilities, and hence rankings of individuals. We develop easy and effective solution procedures. Experiments on real bridge records and multi-class classification demonstrate the viability of the proposed models.
Top/Computer_Science/Spatial_Data_Structures	Fast, Exact Nearest Neighbor in Arbitrary Dimensions with a Cover Tree Given only a metric between points, how quickly can the nearest neighbor of a point be found? In the worst case, this time is O(n). When these points happen to obey a dimensionality constraint, more speed is possible. The 'cover tree' is O(n) space datastructure which allows us to answer queries in O(log(n)) time given a fixed intrinsic dimensionality. It is also a very practical algorithm yielding speedups between a factor of 1 and 1000 on all datasets tested. This speedup has direct implications for several learning algorithms, simulations, and some systems
Top/Science/Complexity_Science	Introduction to Complexity Science Continuing advances in information and communications technology (ICT) are increasing the scale and connectivity of today's engineered systems. Managing the resultant complexity is becoming the central challenge for UK industry and government: from software, to cities and even stock exchanges. Across the UK, a wide range of internationally leading research groups are addressing this challenge. In many cases they draw inspiration from biology, which provides innumerable examples of systems that cope with complexity. From cells to ecosystems, biology achieves scalability, adaptability, self-repair, and robustness, often by exploiting 'emergent' system-level behaviours. Achieving equivalent success in engineered systems is the root problem that we face. In the first of our short courses, we introduce the core concepts of complexity in the context of both natural and engineered systems, and explore the ways in which new computational systems, models, and simulations are taking part in complexity science through a series of lectures and workshop activities.
Top/Science/Complexity_Science	Introduction Continuing advances in information and communications technology (ICT) are increasing the scale and connectivity of today's engineered systems. Managing the resultant complexity is becoming the central challenge for UK industry and government: from software, to cities and even stock exchanges. Across the UK, a wide range of internationally leading research groups are addressing this challenge. In many cases they draw inspiration from biology, which provides innumerable examples of systems that cope with complexity. From cells to ecosystems, biology achieves scalability, adaptability, self-repair, and robustness, often by exploiting 'emergent' system-level behaviours. Achieving equivalent success in engineered systems is the root problem that we face. In the first of our short courses, we introduce the core concepts of complexity in the context of both natural and engineered systems, and explore the ways in which new computational systems, models, and simulations are taking part in complexity science through a series of lectures and workshop activities.
Top/Science/Complexity_Science	Chaordic Systems Thinking: An Introduction of Chaos and Complexity in Organisations and Management
Top/Science/Complexity_Science	Adaptive Behaviour and Emergence Continuing advances in information and communications technology (ICT) are increasing the scale and connectivity of today's engineered systems. Managing the resultant complexity is becoming the central challenge for UK industry and government: from software, to cities and even stock exchanges. Across the UK, a wide range of internationally leading research groups are addressing this challenge. In many cases they draw inspiration from biology, which provides innumerable examples of systems that cope with complexity. From cells to ecosystems, biology achieves scalability, adaptability, self-repair, and robustness, often by exploiting 'emergent' system-level behaviours. Achieving equivalent success in engineered systems is the root problem that we face. In the first of our short courses, we introduce the core concepts of complexity in the context of both natural and engineered systems, and explore the ways in which new computational systems, models, and simulations are taking part in complexity science through a series of lectures and workshop activities.
Top/Science/Complexity_Science	Simulation Modeling: Issues - Part 1 Continuing advances in information and communications technology (ICT) are increasing the scale and connectivity of today's engineered systems. Managing the resultant complexity is becoming the central challenge for UK industry and government: from software, to cities and even stock exchanges. Across the UK, a wide range of internationally leading research groups are addressing this challenge. In many cases they draw inspiration from biology, which provides innumerable examples of systems that cope with complexity. From cells to ecosystems, biology achieves scalability, adaptability, self-repair, and robustness, often by exploiting 'emergent' system-level behaviours. Achieving equivalent success in engineered systems is the root problem that we face. In the first of our short courses, we introduce the core concepts of complexity in the context of both natural and engineered systems, and explore the ways in which new computational systems, models, and simulations are taking part in complexity science through a series of lectures and workshop activities.
Top/Science/Complexity_Science	Simulation Modeling: Examples Continuing advances in information and communications technology (ICT) are increasing the scale and connectivity of today's engineered systems. Managing the resultant complexity is becoming the central challenge for UK industry and government: from software, to cities and even stock exchanges. Across the UK, a wide range of internationally leading research groups are addressing this challenge. In many cases they draw inspiration from biology, which provides innumerable examples of systems that cope with complexity. From cells to ecosystems, biology achieves scalability, adaptability, self-repair, and robustness, often by exploiting 'emergent' system-level behaviours. Achieving equivalent success in engineered systems is the root problem that we face. In the first of our short courses, we introduce the core concepts of complexity in the context of both natural and engineered systems, and explore the ways in which new computational systems, models, and simulations are taking part in complexity science through a series of lectures and workshop activities.
Top/Science/Complexity_Science	Engineered Complexity Continuing advances in information and communications technology (ICT) are increasing the scale and connectivity of today's engineered systems. Managing the resultant complexity is becoming the central challenge for UK industry and government: from software, to cities and even stock exchanges. Across the UK, a wide range of internationally leading research groups are addressing this challenge. In many cases they draw inspiration from biology, which provides innumerable examples of systems that cope with complexity. From cells to ecosystems, biology achieves scalability, adaptability, self-repair, and robustness, often by exploiting 'emergent' system-level behaviours. Achieving equivalent success in engineered systems is the root problem that we face. In the first of our short courses, we introduce the core concepts of complexity in the context of both natural and engineered systems, and explore the ways in which new computational systems, models, and simulations are taking part in complexity science through a series of lectures and workshop activities.
Top/Science/Complexity_Science	Science, Models and Theories Continuing advances in information and communications technology (ICT) are increasing the scale and connectivity of today's engineered systems. Managing the resultant complexity is becoming the central challenge for UK industry and government: from software, to cities and even stock exchanges. Across the UK, a wide range of internationally leading research groups are addressing this challenge. In many cases they draw inspiration from biology, which provides innumerable examples of systems that cope with complexity. From cells to ecosystems, biology achieves scalability, adaptability, self-repair, and robustness, often by exploiting 'emergent' system-level behaviours. Achieving equivalent success in engineered systems is the root problem that we face. In the first of our short courses, we introduce the core concepts of complexity in the context of both natural and engineered systems, and explore the ways in which new computational systems, models, and simulations are taking part in complexity science through a series of lectures and workshop activities.
Top/Science/Complexity_Science	Air Traffic Control: Issues
Top/Science/Complexity_Science	Complexity: Scale and Connectivity Continuing advances in information and communications technology (ICT) are increasing the scale and connectivity of today's engineered systems. Managing the resultant complexity is becoming the central challenge for UK industry and government: from software, to cities and even stock exchanges. Across the UK, a wide range of internationally leading research groups are addressing this challenge. In many cases they draw inspiration from biology, which provides innumerable examples of systems that cope with complexity. From cells to ecosystems, biology achieves scalability, adaptability, self-repair, and robustness, often by exploiting 'emergent' system-level behaviours. Achieving equivalent success in engineered systems is the root problem that we face. In the first of our short courses, we introduce the core concepts of complexity in the context of both natural and engineered systems, and explore the ways in which new computational systems, models, and simulations are taking part in complexity science through a series of lectures and workshop activities.
Top/Science/Complexity_Science	Engineered Complexity Continuing advances in information and communications technology (ICT) are increasing the scale and connectivity of today's engineered systems. Managing the resultant complexity is becoming the central challenge for UK industry and government: from software, to cities and even stock exchanges. Across the UK, a wide range of internationally leading research groups are addressing this challenge. In many cases they draw inspiration from biology, which provides innumerable examples of systems that cope with complexity. From cells to ecosystems, biology achieves scalability, adaptability, self-repair, and robustness, often by exploiting 'emergent' system-level behaviours. Achieving equivalent success in engineered systems is the root problem that we face. In the first of our short courses, we introduce the core concepts of complexity in the context of both natural and engineered systems, and explore the ways in which new computational systems, models, and simulations are taking part in complexity science through a series of lectures and workshop activities.
Top/Science/Complexity_Science	Designing a Simulation Model Continuing advances in information and communications technology (ICT) are increasing the scale and connectivity of today's engineered systems. Managing the resultant complexity is becoming the central challenge for UK industry and government: from software, to cities and even stock exchanges. Across the UK, a wide range of internationally leading research groups are addressing this challenge. In many cases they draw inspiration from biology, which provides innumerable examples of systems that cope with complexity. From cells to ecosystems, biology achieves scalability, adaptability, self-repair, and robustness, often by exploiting 'emergent' system-level behaviours. Achieving equivalent success in engineered systems is the root problem that we face. In the first of our short courses, we introduce the core concepts of complexity in the context of both natural and engineered systems, and explore the ways in which new computational systems, models, and simulations are taking part in complexity science through a series of lectures and workshop activities.
Top/Science/Complexity_Science	Working with Systems Continuing advances in information and communications technology (ICT) are increasing the scale and connectivity of today's engineered systems. Managing the resultant complexity is becoming the central challenge for UK industry and government: from software, to cities and even stock exchanges. Across the UK, a wide range of internationally leading research groups are addressing this challenge. In many cases they draw inspiration from biology, which provides innumerable examples of systems that cope with complexity. From cells to ecosystems, biology achieves scalability, adaptability, self-repair, and robustness, often by exploiting 'emergent' system-level behaviours. Achieving equivalent success in engineered systems is the root problem that we face. In the first of our short courses, we introduce the core concepts of complexity in the context of both natural and engineered systems, and explore the ways in which new computational systems, models, and simulations are taking part in complexity science through a series of lectures and workshop activities.
Top/Science/Complexity_Science	Random Walks All life forms rely on information processing to maintain their highly organised state. Macromolecules and supramolecular structures are key to the special properties that set living systems apart from dead matter. The course will adopt an engineering perspective to introduce the molecular biology (proteins, RNA, DNA) and the physics (thermodynamics, kinetics, dynamics) required for understanding the operation of the molecularmachinery at work in living cells. On this basis the role andthe processing of information at the molecular level will be discussed - covering topics such as noise, molecular motors, conformational switching and intracellular networks - leading to decision making in cells (chemotaxis, development). Throughout the course the potential transfer of concepts from nature to artificial systems will be explored (robustness, self-repair, nano-engineering, molecular computing).
Top/Science/Complexity_Science	Understanding Spatiality
Top/Science/Complexity_Science	Simulation Modeling: Hints and Tips Continuing advances in information and communications technology (ICT) are increasing the scale and connectivity of today's engineered systems. Managing the resultant complexity is becoming the central challenge for UK industry and government: from software, to cities and even stock exchanges. Across the UK, a wide range of internationally leading research groups are addressing this challenge. In many cases they draw inspiration from biology, which provides innumerable examples of systems that cope with complexity. From cells to ecosystems, biology achieves scalability, adaptability, self-repair, and robustness, often by exploiting 'emergent' system-level behaviours. Achieving equivalent success in engineered systems is the root problem that we face. In the first of our short courses, we introduce the core concepts of complexity in the context of both natural and engineered systems, and explore the ways in which new computational systems, models, and simulations are taking part in complexity science through a series of lectures and workshop activities.
Top/Science/Complexity_Science	Simulation Modeling - Part 2 Continuing advances in information and communications technology (ICT) are increasing the scale and connectivity of today's engineered systems. Managing the resultant complexity is becoming the central challenge for UK industry and government: from software, to cities and even stock exchanges. Across the UK, a wide range of internationally leading research groups are addressing this challenge. In many cases they draw inspiration from biology, which provides innumerable examples of systems that cope with complexity. From cells to ecosystems, biology achieves scalability, adaptability, self-repair, and robustness, often by exploiting 'emergent' system-level behaviours. Achieving equivalent success in engineered systems is the root problem that we face. In the first of our short courses, we introduce the core concepts of complexity in the context of both natural and engineered systems, and explore the ways in which new computational systems, models, and simulations are taking part in complexity science through a series of lectures and workshop activities.
Top/Science/Complexity_Science	Plains, Brains and Automobiles Continuing advances in information and communications technology (ICT) are increasing the scale and connectivity of today's engineered systems. Managing the resultant complexity is becoming the central challenge for UK industry and government: from software, to cities and even stock exchanges. Across the UK, a wide range of internationally leading research groups are addressing this challenge. In many cases they draw inspiration from biology, which provides innumerable examples of systems that cope with complexity. From cells to ecosystems, biology achieves scalability, adaptability, self-repair, and robustness, often by exploiting 'emergent' system-level behaviours. Achieving equivalent success in engineered systems is the root problem that we face. In the first of our short courses, we introduce the core concepts of complexity in the context of both natural and engineered systems, and explore the ways in which new computational systems, models, and simulations are taking part in complexity science through a series of lectures and workshop activities.
Top/Science/Complexity_Science	Simulation Modeling: Group Work Continuing advances in information and communications technology (ICT) are increasing the scale and connectivity of today's engineered systems. Managing the resultant complexity is becoming the central challenge for UK industry and government: from software, to cities and even stock exchanges. Across the UK, a wide range of internationally leading research groups are addressing this challenge. In many cases they draw inspiration from biology, which provides innumerable examples of systems that cope with complexity. From cells to ecosystems, biology achieves scalability, adaptability, self-repair, and robustness, often by exploiting 'emergent' system-level behaviours. Achieving equivalent success in engineered systems is the root problem that we face. In the first of our short courses, we introduce the core concepts of complexity in the context of both natural and engineered systems, and explore the ways in which new computational systems, models, and simulations are taking part in complexity science through a series of lectures and workshop activities.
Top/Science/Complexity_Science	Interdisciplinarity Continuing advances in information and communications technology (ICT) are increasing the scale and connectivity of today's engineered systems. Managing the resultant complexity is becoming the central challenge for UK industry and government: from software, to cities and even stock exchanges. Across the UK, a wide range of internationally leading research groups are addressing this challenge. In many cases they draw inspiration from biology, which provides innumerable examples of systems that cope with complexity. From cells to ecosystems, biology achieves scalability, adaptability, self-repair, and robustness, often by exploiting 'emergent' system-level behaviours. Achieving equivalent success in engineered systems is the root problem that we face. In the first of our short courses, we introduce the core concepts of complexity in the context of both natural and engineered systems, and explore the ways in which new computational systems, models, and simulations are taking part in complexity science through a series of lectures and workshop activities.
Top/Science/Complexity_Science	Evolution of Sex It has been a century and a half since Darwin provided the first mechanistic explanation for the complexity of the living things we see around us. Only in the last 30 years or so have computational systems been employed to try out natural selection on complex artificial problems. There have been some successes, but the complexity of artificially evolved systems remains a very long way short of the complexity that is easy to find in biology. Why is this? Is our understanding of natural evolution missing something important? How can we improve our artificial problem solving methods to make them work better on large-scale complex problems?
Top/Science/Complexity_Science	Social Dynamics in Age of the Web People have moved to the web: For many of their social and commercial interactions; To satisfy their information and entertainment needs; To generate content on a massive scale. (follows) The emergence of an online collective intelligence that can be tapped. What do these trends portend? And how can we benefit from them?
Top/Science/Complexity_Science	MCMC, SMC,... What next ? The Monte Carlo method was initially developed for scientific computing in statistical physics during the early days of the computers. Due to the rapid progress in computer technology and the need for handling large datasets and complex systems, the past two decades have witnessed a strong surge of interest in Monte Carlo methods from the scientific community. Researchers ranging from computational biologist to signal & image processing engineers and to financial econometricians now view Monte Carlo techniques as essential tools for inference. Besides using the popular Markov chain Monte Carlo strategies and adaptive variants of it, various sequential Monte Carlo strategies have recently appeared on the scene, resulting in a wealth of novel and effective inferential and optimization tools. In this talk, we will present what we believe to be the 'state-of-the art' in Monte-Carlo simulations for inference and will try to identify the next challenges.
Top/Science/Complexity_Science	Science mapping with asymmetric co-occurence analisys We propose new innovative methods in order to reconstruct paradigmatic fields thanks to simple statistics over a scientific content database. We first define an asymmetric paradigmatic proximity between concepts which provides hierarchical structure over the set of concepts. We propose to implement overlapping categorization to describe paradigmatic fields as sets of concepts that may have several different usage and introduce a 2D embedding to represent these sets in a structured way. This enables to have a micro, meso and macro scale approach to our set of concepts. Concepts can also be dynamically clustered providing a high-level description of the evolution of the paradigmatic fields. We apply our set of methods on a case study from the Complex Systems Community through the mapping of the dynamics of more than 400 Complex Systems Science concepts indexed in a database of of several millions of journal papers.
Top/Science/Complexity_Science	Leibniz, Complexity and Incompleteness I will discuss Leibniz's ideas on complexity (Discours de metaphysique, 1686), leading to modern work on program-size complexity, the halting probability and incompleteness. Leibniz's principle of sufficient reason asserts that if anything is true it is true for a reason. But the bits of the numerical value of the halting probability are mathematical truths that are true for no reason. More precisely, as I will explain, they are irreducible mathematical truths, that is, true for no reason simpler than themselves.
Top/Science/Complexity_Science	Publish or Perish Publish-or-Perish Phenomenon - Evaluations of scientists depend on number of papers, positions in lists of authors, and journals impact factors. In Japan, Spain and elsewhere, such assessments have reached formulaic precision. But bureaucrats are not only wholly responsible for these changes - we scientists have enthusiastically colluded. What began as someone elses measure has become our (own) goal P.A. Lawrence, The politics of publication, Nature 422, 259 (2003).
Top/Science/Complexity_Science	Evolution of Complexity It has been a century and a half since Darwin provided the first mechanistic explanation for the complexity of the living things we see around us. Only in the last 30 years or so have computational systems been employed to try out natural selection on complex artificial problems. There have been some successes, but the complexity of artificially evolved systems remains a very long way short of the complexity that is easy to find in biology. Why is this? Is our understanding of natural evolution missing something important? How can we improve our artificial problem solving methods to make them work better on large-scale complex problems?
Top/Science/Complexity_Science	Structure and Dynamics in Complex Networks
Top/Science/Complexity_Science	Dynamics on and of Biological Networks: Case Studies on the Machinery of Life Gene regulation networks and other molecular networks that regulate the processes of life in the living cell are prototypes for dynamical networks that combine the aspects of both, transferring dynamical signaling on the one hand, and being structurally dynamical themselves on the other hand. Both phenomena, while living on vastly different timescales (that of molecular interactions versus the timescale of macroevolution), are closely interwoven and depend on each other. We will take a closer look at this interesting type of complex networks and I will review a few approaches and views from different angles.
Top/Science/Complexity_Science	Strong random correlations in complex systems Complex systems (living organisms, the brain, society, the economy, etc.) seem to depend on a huge number of details which makes them nearly irreducible, so that they cannot be described in terms of a small number of variables. This poses fundamental difficulties for the modeling of such systems and the parametrization or calibration of any model that we may propose to describe them. Furthermore, this irreducibility also implies the existence of strong random correlations between a large number of the components of the system that are not necessarily close neighbours in a geometric sense, or not necessarily linked by strong, direct interactions. This makes the system sensitive to changes in the external control parameters, to boundary conditions, etc., and poses a serious challenge to computer simulations. These ideas are illustrated on some toy models: a spin glass, a random cellular automaton, and a game theoretical model.
Top/Computer_Science/Machine_Learning/Active_Learning/Exploration_vs_Exploitation	Exploration exploitation in Go: UCT for Monte-Carlo Go
Top/Computer_Science/Machine_Learning/Active_Learning/Exploration_vs_Exploitation	Using upper confidence bounds to control exploration and exploitation
Top/Computer_Science/Machine_Learning/Active_Learning/Exploration_vs_Exploitation	Exploration Vs. Explotation
Top/Computer_Science/Machine_Learning/Active_Learning/Exploration_vs_Exploitation	On Efficient Sequential Decision Making in Structured Problems
Top/Computer_Science/Machine_Learning/Active_Learning/Exploration_vs_Exploitation	Introduction to the On-line Trading of Exploration and Exploitation Workshop
Top/Computer_Science/Machine_Learning/Ensemble_Methods	AdaBoost is Universally Consistent We consider the risk, or probability of error, of the classifier produced by AdaBoost, and in particular the stopping strategy to be used to ensure universal consistency. (A classification method is universally consistent if the risk of the classifiers it produces approaches the Bayes risk---the minimal risk---as the sample size grows.) Several related algorithms---regularized versions of AdaBoost---have been shown to be universally consistent, but AdaBoost's universal consistency has not been established. Jiang has demonstrated that, for each probability distribution satisfying certain smoothness conditions, there is a stopping time for sample size n, so that if AdaBoost is stopped after iterations, its risk approaches the Bayes risk for that distribution. Our main result is that if AdaBoost is stopped after iterations, it is universally consistent, where n is the sample size and .
Top/Computer_Science/Machine_Learning/Ensemble_Methods	Identifying Feature Relevance using a Random Forest Many feature selection algorithms are limited in that they attempt to identify relevant feature subsets by examining the features individually. This paper introduces a technique for determining feature relevance using the average information gain achieved during the construction of decision tree ensembles. The technique introduces a node complexity measure and a statistical method for updating the feature sampling distribution based upon confidence intervals to control the rate of convergence. Experiments demonstrate the potential of this method for feature selection and subspace identification.
Top/Computer_Science/Machine_Learning/Ensemble_Methods	Large-Margin Thresholded Ensembles for Ordinal Regression We propose a thresholded ensemble model for ordinal regression problems. The model consists of a weighted ensemble of confidence functions and an ordered vector of thresholds. Using such a model, we could theoretically and algorithmically reduce ordinal regression problems to binary classification problems in the area of ensemble learning. Based on the reduction, we derive novel large-margin bounds of common error functions, such as the classification error and the absolute error. In addition, we also design two novel boosting approaches for constructing thresholded ensembles. Both our approaches have comparable performance to the state-of-the-art algorithms, but enjoy the benefit of faster training. Experimental results on benchmark datasets demonstrate the usefulness of our boosting approaches.
Top/Computer_Science/Machine_Learning/Ensemble_Methods	An Introduction to Ensemble and Boosting
Top/Computer_Science/Machine_Learning/Ensemble_Methods	Overview of New Developments in Boosting I will give an overview of recent developments in boosting, focusing on three papers which take very different approaches towards making boosting more efficient and effective. Boosters iteratively choose base classifiers via a weak learner and then update a distribution over training examples. Roughly, the three papers show progress on the three issues implicit in this one-sentence description of boosting: the number of iterations required, the computational cost of choosing good base classifiers, and the time and space complexity from maintaining a distribution over training examples. Warmuth, Liao, and Ratsch (2006) propose TotalBoost, which is a 'totally corrective' boosting algorithm. Intuitively, on each round, totally corrective boosters choose base classifiers which give more information not present in previously chosen base classifiers. This leads to fewer iterations and smaller final hypotheses. Barutcuoglu, Long, and Servedio (2007) describe an alternative model for boosting where assumptions about the diversity of base classifiers allow the booster to learn in a single pass over the set of base classifiers. This eliminates the need to optimize over all base classifiers on each round. Bradley and Schapire (2007) propose an algorithm called FilterBoost which trains on examples drawn from an oracle rather than a fixed set of examples. This alternative learning framework can model learning via a single pass over the set of training examples and allows the booster to train efficiently on very large datasets.
Top/Computer_Science/Machine_Learning/Ensemble_Methods	From Trees to Forests and Rule Sets - A Unified Overview of Ensemble Methods Ensemble methods are one of the most influential developments in Machine Learning over the past decade. They perform extremely well in a variety of problem domains, have desirable statistical properties, and scale well computationally. By combining competing models into a committee, they can strengthen weak learning procedures. ;This tutorial explains two recent developments with ensemble methods: :**Importance Sampling** reveals classic ensemble methods (bagging, random forests, and boosting) to be special cases of a single algorithm. This unified view clarifies the properties of these methods and suggests ways to improve their accuracy and speed. :**Rule Ensembles** are linear rule models derived from decision tree ensembles. While maintaining (and often improving) the accuracy of the tree ensemble, the rule-based model is much more interpretable. This tutorial is aimed at both novice and advanced data mining researchers and practitioners especially in Engineering, Statistics, and Computer Science. Users with little exposure to ensemble methods will gain a clear overview of each method. Advanced practitioners already employing ensembles will gain insight into this breakthrough way to create next-generation models. ;**John Elder's lecture**: : In a Nutshell, Examples & Timeline : Predictive Learning : Decision Trees ;**Giovanni Seni's lecture**: : Model Selection (Bias-Variance Tradeoff , Regularization via shrinkage) : Ensemble Learning & Importance Sampling (ISLE) : Generic Ensemble Generation : Bagging, Random Forest, AdaBoost, MART : Rule Ensembles : Interpretation
Top/Computer_Science/Machine_Learning/On-line_Learning	Online Learning and Bregman Divergences L 1: Introduction to Online Learning (Predicting as good as the best expert, Predicting as good as the best linear combination of experts, Additive versus multiplicative family of updates) L 2: Bregman divergences and Loss bounds (Introduction to Bregman divergences, Relative loss bounds for the linear case, Nonlinear case & matching losses, Duality and relation to exponential families) L 3: Extensions, interpretations, applications (Online to Batch Conversions, Prior information on the weight vector, Some applications)
Top/Computer_Science/Machine_Learning/On-line_Learning	Preliminary Experiments with On-Line Adaptive GARCH Models
Top/Computer_Science/Machine_Learning/On-line_Learning	Online Learning and Bregmann Divergences
Top/Computer_Science/Machine_Learning/On-line_Learning	On-line linear learning algorithms Prediction with expert advice. Learning with linear experts. The Perceptron algorithm and its extensions. On-line learning with kernels. Mistake bounds. From mistake bounds to risk bounds.
Top/Computer_Science/Machine_Learning/On-line_Learning	Rank Minimization via Online Learning Minimum rank problems arise frequently in machine learning applications and are notoriously difficult to solve due to the non-convex nature of the rank objective. In this paper, we present the first online learning approach for the problem of rank minimization of matrices over polyhedral sets. In particular, we present two online learning algorithms for rank minimization - our first algorithm is a multiplicative update method based on a generalized experts framework, while our second algorithm is a novel application of the online convex programming framework [Zinkevich, 2003]. In the latter, we flip the role of the decision maker by making the decision maker search over the constraint space instead of feasible points, as is usually the case in online convex programming. A salient feature of our online learning approach is that it allows us to give the first provable approximation guarantees for the rank minimization problem over polyhedral sets. We demonstrate the effectiveness of our methods on synthetic examples, and on the real-life application of low-rank kernel learning.
Top/Computer_Science/Machine_Learning/Density_estimation	Hierarchical Maximum Entropy Density Estimation We study the problem of simultaneously estimating several densities where the datasets are organized into overlapping groups, such as a hierarchy. For this problem, we propose a maximum entropy formulation, which systematically incorporates the groups and allows us to share the strength of prediction across similar datasets. We derive general performance guarantees, and show how some previous approaches, such as hierarchical shrinkage and hierarchical priors, can be derived as special cases. We demonstrate the proposed technique on synthetic data and in a realworld application to modeling the geographic distributions of species hierarchically grouped in a taxonomy. Specifically, we model the geographic distributions of species in the Australian wet tropics and Northeast New South Wales. In these regions, small numbers of samples per species significantly hinder effective prediction. Substantial benefits are obtained by combining information across taxonomic groups.
Top/Computer_Science/Machine_Learning/Density_estimation	Statistical Change Detection for Multi-Dimensional Data This paper deals with detecting change of distribution in multi-dimensional data sets. For a given baseline data set and a set of newly observed data points, we define a statistical test called the density test for deciding if the observed data points are sampled from the underlying distribution that produced the baseline data set. We define a test statistic that is strictly distribution-free under the null hypothesis. Our experimental results show that the density test has substantially more power than the two existing methods for multi-dimensional change detection.
Top/Computer_Science/Machine_Learning/Density_estimation	Fast Clustering based on Kernel Density Estimation The Denclue algorithm employs a cluster model based on kernel density estimation. A cluster is defined by a local maximum of the estimated density function. Data points are assigned to clusters by hill climbing, i.e. points going to the same local maximum are put into the same cluster. A disadvantage of Denclue 1.0 is, that the used hill climbing may make unnecessary small steps in the beginning and never converges exactly to the maximum, it just comes close. We introduce a new hill climbing procedure for Gaussian kernels, which adjusts the step size automatically at no extra costs. We prove that the procedure converges exactly towards a local maximum by reducing it to a special case of the expectation maximization algorithm. We show experimentally that the new procedure needs much less iterations and can be accelerated by sampling based methods with sacrificing only a small amount of accuracy.
Top/Computer_Science/Machine_Learning/Unsupervised_learning	Dirichlet Aggregation: Unsupervised Learning towards an Optimal Metric for Proportional Data Proportional data (normalized histograms) have been frequently occurring in various areas, and they could be mathematically abstracted as points residing in a geometric simplex. A proper distance metric on this simplex is of importance in many applications including classification and information retrieval. In this paper, we develop a novel framework to learn an optimal metric on the simplex. Ma jor features of our approach include: 1) its flexibility to handle correlations among bins/dimensions; 2) widespread applicability without being limited to ad hoc backgrounds; and 3) a 'real' global solution in contrast to existing traditional local approaches. The technical essence of our approach is to fit a parametric distribution to the observed empirical data in the simplex. The distribution is parameterized by affinities between simplex vertices, which is learned via maximizing likelihood of observed data. Then, these affinities induce a metric on the simplex, defined as the earth mover's distance equipped with ground distances derived from simplex vertex affinities.
Top/Computer_Science/Machine_Learning/Unsupervised_learning	Unsupervised Estimation for Noisy-Channel Models Shannons Noisy-Channel model, which describes how a corrupted message might be reconstructed, has been the corner stone for much work in statistical language and speech processing. The model factors into two components: a language model to characterize the original message and a channel model to describe the channels corruptive process. The standard approach for estimating the parameters of the channel model is unsupervised Maximum-Likelihood of the observation data, usually approximated using the Expectation-Maximization (EM) algorithm. In this paper we show that it is better to maximize the joint likelihood of the data at both ends of the noisy-channel. We derive a corresponding bi-directional EM algorithm and show that it gives better performance than standard EM on two tasks: (1) translation using a probabilistic lexicon and (2) adaptation of a part-of-speech tagger between related languages.
Top/Computer_Science/Machine_Learning/Unsupervised_learning	A Self-Organizing Map for Relation Extraction from Wikipedia using Structured Data Representations
Top/Computer_Science/Machine_Learning/Unsupervised_learning	WP2: Learning Web-service Domain Ontologies
Top/Computer_Science/Machine_Learning/Unsupervised_learning	Transitioning legacy applications to ontologies: Hands-on tutorial - Learning Domain Ontologies
Top/Computer_Science/Web_Mining	Applications of Query Mining
Top/Computer_Science/Web_Mining	Efficient and Decentralized PageRank Approximation in a P2P Web Search Network
Top/Computer_Science/Web_Mining	Theoretical analysis of Link Analysis Ranking
Top/Computer_Science/Web_Mining	Using Rank Propagation and Probabilistic Counting for Link-based Spam Detection
Top/Computer_Science/Web_Mining	Mixture Models and Collaborative Filtering Algorithms
Top/Computer_Science/Web_Mining	Current Approaches to Personalized Web Search
Top/Computer_Science/Web_Mining	Graph Fibrations, graph isomorphism and PageRank
Top/Computer_Science/Web_Mining	Large Scale Ranking Problem: some theoretical and algorithmic issues The talk is divided into two parts. The first part focuses on web-search ranking, for which I discuss training relevance models based on DCG (discounted cumulated gain) optimization. Under this metric, the system output quality is naturally determined by the performance near the top of its rank-list. I will mainly focus on various theoretical issues for this learning problem. The second part discusses related algorithmic issues in the context of optimizing the scoring function of a statistical machine translation system according to the BLEU metric (standard measure of translation quality). Our approach treats machine translation as a black-box, and can optimize millions of system parameters automatically. This has not been attempted before in this context. I will present our method and some initial results.
Top/Computer_Science/Web_Mining	Efficient Lazy Algorithms for Minimal-Interval Semantics
Top/Computer_Science/Web_Mining	Boosting Performance of Web Search Engines Using Query Logs
Top/Computer_Science/Web_Mining	iLink: Search and Routing in Social Networks - Part 1 The growth of Web 2.0 and fundamental theoretical breakthroughs have led to an avalanche of interest in social networks. This paper focuses on the problem of modeling how social networks accomplish tasks through peer production style collaboration. We propose a general interaction model for the underlying social networks and then a specific model (iLink) for social search and message routing. A key contribution here is the development of a general learning framework for making such online peer production systems work at scale. The iLink model has been used to develop a system for FAQ generation in a social network (FAQtory), and experience with its application in the context of a full-scale learning-driven workflow application (CALO) is reported. We also discuss methods of adapting iLink technology for use in military knowledge sharing portals and other message routing systems. Finally, the paper shows the connection of iLink to SQM, a theoretical model for social search that is a generalization of Markov Decision Processes and the popular Pagerank model.
Top/Computer_Science/Web_Mining	Web mining
Top/Computer_Science/Web_Mining	Website Privacy Preservation for Query Log Publishing In this work we study privacy preservation for the publi- cation of search engine query logs. In particular, we introduce a new privacy concern, which is that of website privacy (or business privacy). We define the possible adversaries that could be interested in disclosing website information and the vulnerabilities found in the query log, from which they could benefit. We also detail anonymization techniques to protect website information, and explore the different types of attacks that an adversary could use. We then present a graph-based heuristic to validate the effectiveness of our anonymization method, and perform an experimental evaluation of this approach. Our experimental results show that the query log can be appropriately anonymized against a specific attack for website exposure, by only removing approximately 9% of the total volume of queries and clicked URLs.
Top/Computer_Science/Web_Mining	Webpage Understanding: an Integrated Approach Recent work has shown the effectiveness of leveraging layout and tag-tree structure for segmenting webpages and labeling HTML elements. However, how to effectively segment and label the text contents inside HTML elements is still an open problem. Since many text contents on a webpage are often text fragments and not strictly grammatical, traditional natural language processing techniques, that typically expect grammatical sentences, are no longer directly applicable. In this paper, we examine how to use layout and tag-tree structure in a principled way to help understand text contents on webpages. We propose to segment and label the page structure and the text content of a webpage in a joint discriminative probabilistic model. In this model, semantic labels of page structure can be leveraged to help text content understanding, and semantic labels of the text phrases can be used in page structure understanding tasks such as data record detection. Thus, integration of both page structure and text content understanding leads to an integrated solution of webpage understanding. Experimental results on research homepage extraction show the feasibility and promise of our approach.
Top/Computer_Science/Web_Mining	Yahoo! Research Overview
Top/Computer_Science/Web_Mining	Efficient and Decentralized Page Rank Approximation in P2P Networks with Malicious Agents
Top/Computer_Science/Web_Mining	Identifying the Influential Bloggers
Top/Computer_Science/Web_Mining	Ranking Web Sites with Real User Traffic
Top/Computer_Science/Web_Mining	Mining Queries
Top/Computer_Science/Web_Mining	'Lies, Damn Lies, and Statistics': A Critical Assessment of Preferential Attachment-type Network Models of the Internet Basic Question: Do the available Internet-related connectivity measurements and their analysis support the sort of claims that can be found in the existing complex networks literature? Key Issues: What about data hygiene? What about statistical rigor? What about model validation? Author discusses some of the main problems and challenges associated with measuring, inferring, and modeling various types of Internet-related connectivity structures. To this end, he uses some known examples to illustrate the need to understand the process by which Internet connectivity measurements are obtained, explore the sensitivity of inferred graph properties to known ambiguities in the data, be more critical with respect to the dominant, preferential attachmenttype network modeling paradigm, and be more serious/ambitious when it comes to model validation. Ignoring any of these issues is bound to produce results that are best described by the well-known aphorism 'lies, damned lies, and statistics.'
Top/Computer_Science/Web_Mining	Web mining for natural language engineering tasks
Top/Computer_Science/Web_Mining	Machine Reading at Web Scale
Top/Computer_Science/Web_Mining	Latent Variable Models for Document Analysis Wray Buntine will consider various problems in document analysis (named entity recognition, natural language parsing, information retrieval), and look at various probabilistic graphical models and algorithms for addressing the problem. This will not be an extensive coverage of information extraction or natural language processing, but rather a look at some of the theory, methods and practice of particular cases, including the use of software environments.
Top/Computer_Science/Web_Mining	Filtering Multi-Lingual Terrorist Content with Graph-Theoretic Classifi-cation Tools Since the web is increasingly used by terrorist organizations, the ability to automatically detect multi-lingual terrorist-related content is extremely important. In this talk, we present an efficient detection methodology based on the recently developed graph-based web document representation models. Evaluation is performed on corpora in English and Arabic languages.
Top/Computer_Science/Machine_Learning/Manifold_Learning	Non-Isometric Manifold Learning: Analysis and an Algorithm In this work we take a novel view of nonlinear manifold learning. Usually, manifold learning is formulated in terms of finding an embedding or 'unrolling' of a manifold into a lower dimensional space. Instead, we treat it as the problem of learning a representation of a nonlinear, possibly non-isometric manifold that allows for the manipulation of novel points. Central to this view of manifold learning is the concept of generalization beyond the training data. Drawing on concepts from supervised learning, we establish a framework for studying the problems of model assessment, model complexity, and model selection for manifold learning. We present an extension of a recent algorithm, Locally Smooth Manifold Learning (L S M L), and show it has good generalization properties. L S M L learns a representation of a manifold or family of related manifolds and can be used for computing geodesic distances, finding the projection of a point onto a manifold, recovering a manifold from points corrupted by noise, generating novel points on a manifold, and more.
Top/Computer_Science/Machine_Learning/Manifold_Learning	Semi-supervised Learning, Manifold Methods
Top/Computer_Science/Machine_Learning/Manifold_Learning	Robust Non-linear Dimensionality Reduction using Successive 1-Dimensional Laplacian Eigenmapse Non-linear dimensionality reduction of noisy data is a challenging problem encountered in a variety of data analysis applications. Recent results in the literature show that spectral decomposition, as used for example by the Laplacian Eigenmaps algorithm, provides a powerful tool for non-linear dimensionality reduction and manifold learning. In this paper, we discuss a significant shortcoming of these approaches, which we refer to as the repeated eigendirections problem. We propose a novel approach that combines successive 1dimensional spectral embeddings with a data advection scheme that allows us to address this problem. The proposed method does not depend on a non-linear optimization scheme; hence, it is not prone to local minima. Experiments with artificial and real data illustrate the advantages of the proposed method over existing approaches. We also demonstrate that the approach is capable of correctly learning manifolds corrupted by significant amounts of noise.
Top/Computer_Science/Software_and_Tools	GATE APIs, CREOLE lifecycle, JAVA for JAPE
Top/Computer_Science/Software_and_Tools	Link analysis with pajek Pajek is a program (for Windows) for large network analysis and visualization. It is freely available for noncommercial use at [[http://vlado.fmf.uni-lj.si/pub/networks/pajek/|http://vlado.fmf.uni-lj.si/pub/networks/pajek/]] Besides ordinary networks Pajek supports also multi-relational and temporal networks. In large network analysis we are often interested in important parts of given network. There are several ways how to determine them. The islands approach is based on an importance measure of vertices or lines. Let (V,L,p) be a network with vertex property p : V ? R and let t be a real number. If we delete all vertices (and corresponding links) with the property value less than t, we get subnetwork called vertex-cut at level t. The number and sizes of its components depend on t. Often we consider only components of size at least k and not exceeding K. The components of size smaller than k are discarded as noninteresting, while the components of size larger than K are cut again at some higher level. Vertex-island is a connected subnetwork which vertices have greater property value than the vertices in its neighborhood. It is easy to see that the components of vertex-cuts are all vertex-islands. We developed an efficient algorithm that identifies all maximal vertex-islands of sizes in the interval k..K in a given network. For networks with weighted lines we can similarly define line-islands. The line-islands algorithm is based on line-cuts. Both algorithms are very general - they can be applied for any vertex/line importance measure. Their complexity is for sparse networks subquadratic - they can be applied to very large networks. We will illustrate them applying different importance measures on selected (large) networks. We will also present the use of pattern searching in analysis of genealogies and some approaches to analysis of (multi-relational) temporal networks.
Top/Computer_Science/Software_and_Tools	Other ML/DM software (R, Weka, Yale)
Top/Computer_Science/Software_and_Tools	Introduction to CLOP Machine Learning Toolbox
Top/Computer_Science/Software_and_Tools	Trans Lab
Top/Computer_Science/Software_and_Tools	Development eco-system: building components, usage in Eclipse, unit tests
Top/Computer_Science/Software_and_Tools	GATE and IBMs UIMA - interoperability layer
Top/Computer_Science/Software_and_Tools	Interactive derivation viewer This describes the IDV, a tool for graphically rendering derivations that are written in the Thousands of Problems for Theorem Provers (TPTP) language.
Top/Computer_Science/Software_and_Tools	Corpora, evaluation tools
Top/Computer_Science/Software_and_Tools	Architecture Proposal
Top/Computer_Science/Software_and_Tools	Git When you have hundreds of people simultaneously patching 25000 files of the Linux Kernel in sometimes conflicting ways, you might need some scheme or plan to sort all that out before you can build your next kernel and reboot. The Linux team uses 'git' for their source code repository management, a homegrown solution that is optimized for highly distributed development, working with huge sets of files, merging independent work at multiple levels, and seeing who broke what. (Git has also since been notably adopted by the Cairo, x.org, and Wine teams, and is being transitioned to by the Mozilla codebase.) In my talk, I describe what 'git'; is and isn't, and why you should use it instead of CVS, Subversion, SVK, Arch, Darcs, Mercurial, Monotone, Bazaar, and just about every other repository manager. I'll also walk though the basic concepts so that the manpages might start making sense. If I have time, I'll even do a live walkthrough, where you can watch how fast I make typos.
Top/Computer_Science/Data_Mining/Time_Series_Analysis	Path Integral Method for Estimation of Time Series
Top/Computer_Science/Data_Mining/Time_Series_Analysis	Preliminary Experiments with On-Line Adaptive GARCH Models
Top/Computer_Science/Data_Mining/Time_Series_Analysis	Signal Processing
Top/Computer_Science/Data_Mining/Time_Series_Analysis	Matching pursuit and unification in EEG analysis
Top/Computer_Science/Data_Mining/Time_Series_Analysis	Temporal Causal Modeling with Graphical Granger Methods The need for mining causality, beyond mere statistical correlations, for real world problems has been recognized widely. Many of these applications naturally involve temporal data, which raises the challenge of how best to leverage the temporal information for causal modeling. Recently graphical modeling with the concept of Granger causality, based on the intuition that a cause helps predict its effects in the future, has gained attention in many domains involving time series data analysis. With the surge of interest in model selection methodologies for regression, such as the Lasso, as practical alternatives to solving structural learning of graphical models, the question arises whether and how to combine these two notions into a practically viable approach for temporal causal modeling. In this paper, we examine a host of related algorithms that, loosely speaking, fall under the category of graphical Granger methods, and characterize their relative performance from multiple viewpoints. Our experiments show, for instance, that the Lasso algorithm exhibits consistent gain over the canonical pairwise graphical Granger method. We also characterize conditions under which these variants of graphical Granger methods perform well in comparison to other benchmark methods. Finally, we apply these methods to a real world data set involving key performance indicators of corporations, and present some concrete results.
Top/Computer_Science/Data_Mining/Time_Series_Analysis	Evolving Systems One of the important research challenges today is to develop new theoretical methods, algorithms, and implementations of systems with a higher level of flexibility and autonomy, we can say with higher level of intelligence. These systems have to be able to evolve their structure and knowledge on the environment and ultimately evolve their intelligence. To address the problems of modelling, control, prediction, classification and data processing in a dynamically changing and evolving environment, a system must be able to fully adapt its structure and adjust its parameters, rather than use a pre-trained and a fixed structure. That is, the system must be able to evolve, to self-develop, to self-organize, to self-evaluate and to self-improve. The talk will concentrate on the problems and results the author encountered during last several years of research in this emerging area as well as on the approach to on-line identification of a particular type of fuzzy models so called Takagi-Sugeno fuzzy models including some applications, in particular to mobile robots, mobile communications, process modelling and control, on-line evolving classification intelligent (inferential) sensors.
Top/Computer_Science/Data_Mining/Time_Series_Analysis	Analysis of Time Series The study of time series is an essential aspect of Intelligent Data Analysis. The field is very broad, and it has been treated with very different methodological approaches, ranging from differential equations to stochastic models and to AI-based systems. The lesson will present time series analysis as a part of the general problem of modelling dynamical systems. The framework of systems theory will provide a general view of such problem, and it will permit to coherently overview the majority of the time series analysis approaches. In more detail, the principles of systems theory will be first discussed; the concept of dynamical system will be investigated and some results of systems theory will be presented. The notions of state, equilibrium, linearity, observability and reachability will be discussed. Some modelling tools will be then introduced, ranging from black-box to structural models. Stochastic linear and non linear models will be briefly described, including AR, MA, and ARMAX models. Moreover, a method to obtain structural information from input/output data will be introduced. The lesson will finally show how the knowledge on systems dynamics can be effectively exploited in the time series clustering problem. Distance-based, model-based and template-based will be revisited in order to account for information on the systems dynamics.
Top/Computer_Science/Speech_Analysis	Facial expression recognition and emotion recognition from speech The presentation tackles the problem of recognizing the emotions based on video and audio data analysis. A fully automatic facial expression recognition system is based on three components: face detection, facial characteristic point extraction and classification. Face detection is employed by boosting simple rectangle Haar-like features that give a decent representation of the face. These features also allow the differentiation between a face and a non-face. The boosting algorithm is combined with an Evolutionary Search to speed up the overall search time. Facial characteristic points (FCP) are extracted from the detected faces. The same technique applied on faces is utilized for this purpose. Additionally, FCP extraction using corner detection methods and brightness distribution has also been considered. Finally, after retrieving the required FCPs the emotion of the facial expression can be determined.
Top/Computer_Science/Speech_Analysis	Speech-to-Speech Translation Services for the Olympic Games 2008
Top/Computer_Science/Speech_Analysis	Confidence Measures in Speech Recognition A confidence measure (CM) is a number between 0 and 1 that is applied to speech recognition output. A CM gives an indication of how confident we are that the unit to which it has been applied (e.g. a phrase, word, phone) is correct. Confidence measures are extremely useful in any speech application that involves a dialogue, because they can guide the system towards a more intelligent dialogue that is faster and less frustrating for the user.
Top/Computer_Science/Speech_Analysis	IBM Speech Activity System
Top/Computer_Science/Speech_Analysis	Open Vocabulary Speech Analysis in VITALAS Automatic indexing of TV and radio speech data requires robust components for both speech recognition and spoken document retrieval. Due to the high topic variability and the resulting large vocabularies, classic word-based approaches have to cope with a high number of out-of-vocabulary words. This talk presents a phonetic approach to open vocabulary indexing based on syllable decoding and retrieval. Current experimental results are presented, followed by a demonstration of the Fraunhofer IAIS AudioMining system for spoken term detection.
Top/Computer_Science/Speech_Analysis	Speaker Localization: introduction to system evaluation
Top/Computer_Science/Speech_Analysis	Words in puddles of sound Words in a sea of sound (Saffran, 2001) Discovering words from continuous speech; with no reliable cues to word boundaries (Jones, 1918; Liberman et al., 1967); where words are realised variably (Pollack & Pickett, 1964).
Top/Computer_Science/Speech_Analysis	Overlap in Meetings: ASR Effects and Analysis by Dialog Factors, Speakers, and Collection Site
Top/Computer_Science/Speech_Analysis	Multistream Recognition of Dialogue Acts in Meetings
Top/Computer_Science/Speech_Analysis	On the Adequacy of Baseform Pronunciations and Pronunciation Variants This paper presents an approach to automatically extract and evaluate the ``stability'' of pronunciation variants (i.e., adequacy of the model to accommodate this variability), based on multiple pronunciations of each lexicon words and the knowledge of a reference baseform pronunciation. Most approaches toward modelling pronunciation variability in speech recognition are based on the inference (through an ergodic HMM model) of a pronunciation graph (including all pronunciation variants), usually followed by a smoothing (e.g., Bayesian) of the resulting graph.
Top/Computer_Science/Speech_Analysis	Tandem Connectionist Feature Extraction for Conversational Speech Recognition Multi-Layer Perceptrons (MLPs) can be used in automatic speech recognition in many ways. A particular application of this tool over the last few years has been the Tandem approach, as described by Hermansky et al in a number of publications. Here we discuss the characteristics of the MLP-based features used for the Tandem approach, and conclude with a report on their application to conversational speech recognition. The paper shows that MLP transformations yield variables that have regular distributions, which can be further modified by using logarithm to make the distribution easier to model by a Gaussian-HMM. Two or more vectors of these features can easily be combined without increasing the feature dimension. We also report recognition results that show that MLP features can significantly improve recognition performance for the NIST 2001 Hub-5 evaluation set with models trained on the Switchboard Corpus, even for complex systems incorporating MMIE training and other enhancements.
Top/Computer_Science/Data_Mining/Frequent_Itemsets	Mining, Indexing, and Searching Graphs in Large Data Sets Recent research on pattern discovery has progressed from mining frequent itemsets and sequences to mining structured patterns including trees, lattices, and graphs. As a general data structure, graph can model complicated relations among data with wide applications in Web, social network analysis, and bioinformatics. However, mining and searching large graphs in graph databases is challenging due to the presence of an exponential number of frequent subgraphs. In this talk, we present our recent progress on developing efficient and scalable methods for mining and searching of graphs in large databases. We introduce gSpan and CloseGraph, two efficient methods for mining frequent graph patterns in graph databases. Then we introduce constraint-based graph mining methods. Further, we introduce a graph indexing method, gIndex, and a graph approximate searching method, grafil, both taking advantages of frequent graph mining to construct a compact but highly effective graph index and perform similarity search with such indexing structures. These methods not only facilitate mining and querying graph patterns in massive datasets but also claim broad applications in other fields, including DB/OS systems and software engineering.
Top/Computer_Science/Data_Mining/Frequent_Itemsets	Finding low-entropy sets and trees from binary data The discovery of subsets with special properties from binary data has been one of the key themes in pattern discovery. Pattern classes such as frequent itemsets stress the co-occurrence of the value 1 in the data. While this choice makes sense in the context of sparse binary data, it disregards potentially interesting subsets of attributes that have some other type of dependency structure. We consider the problem of finding all subsets of attributes that have low complexity. The complexity is measured by either the entropy of the projection of the data on the subset, or the entropy of the data for the subset when modeled using a Bayesian tree, with downward or upward pointing edges. We show that the entropy measure on sets has a monotonicity property, and thus a levelwise approach can find all low-entropy itemsets. We also show that the treebased measures are bounded above by the entropy of the corresponding itemset, allowing similar algorithms to be used for finding low-entropy trees. We describe algorithms for finding all subsets satisfying an entropy condition. We give an extensive empirical evaluation of the performance of the methods both on synthetic and on real data. We also discuss the search for high-entropy subsets and the computation of the Vapnik-Chervonenkis dimension of the data.
Top/Computer_Science/Data_Mining/Frequent_Itemsets	MINI: Mining Informative Non-redundant Itemsets
Top/Computer_Science/Data_Mining/Frequent_Itemsets	Realistic Synthetic Data for Testing Association Rule Mining Algorithms for Market Basket Databases
Top/Computer_Science/Information_Retrieval/Multimedia_Retrieval	Introduction to Multimedia Digital Libraries
Top/Computer_Science/Information_Retrieval/Multimedia_Retrieval	Cross-modal analysis
Top/Computer_Science/Information_Retrieval/Multimedia_Retrieval	Multimedia representation standards
Top/Computer_Science/Information_Retrieval/Multimedia_Retrieval	Color Image Segmentation: Kernel Do the Feature Space
Top/Computer_Science/Information_Retrieval/Multimedia_Retrieval	Research 17: Integrating and Querying Parallel Leaf Shape Descriptions
Top/Computer_Science/Information_Retrieval/Multimedia_Retrieval	Feature extraction & content description II
Top/Computer_Science/Information_Retrieval/Multimedia_Retrieval	Multimodal visual resource discovery
Top/Computer_Science/Information_Retrieval/Multimedia_Retrieval	Chorus Plans and Progress, Meeting Objectives - Part 1
Top/Computer_Science/Artificial_Intelligence	Can diagrammatic reasoning be automated Theorems in automated theorem proving are usually proved by formal logical proofs. However, there is a subset of problems which humans can prove by the use of geometric operations on diagrams, so called diagrammatic proofs. Insight is often more clearly perceived in these proofs than in the corresponding algebraic proofs; they capture an intuitive notion of truthfulness that humans find easy to see and understand. We are investigating and automating such diagrammatic reasoning about mathematical theorems. Concrete, rather than general diagrams are used to prove particular concrete instances of the universally quantified theorem. The diagrammatic proof is captured by the use of geometric operations on the diagram. These operations are the 'inference steps' of the proof. An abstracted schematic proof of the universally quantified theorem is induced from these proof instances. The constructive omega-rule provides the mathematical basis for this step from schematic proofs to theoremhood. In this way we avoid the difficulty of treating a general case in a diagram. One method of confirming that the abstraction of the schematic proof from the proof instances is sound is proving the correctness of schematic proofs in the meta-theory of diagrams. These ideas have been implemented in the system, called DIAMOND, which is presented here.
Top/Computer_Science/Artificial_Intelligence	Learning techniques in Planning In this lecture, I aim to provide an overview of the learning techniques that have found use in automated planning. Unlike most the clustering and classification tasks that have dominated the recent machine learning literature, learning in planning requires handling relational and first order representations, and foregrounds the need for knowledge-intensive learning techniques. I will start with a brief review of the planning models, and discuss the opportunities for learning in planning. I will then provide a survey of the explanation-based, case-based and inductive learning techniques that have been successfully used to tackle them.
Top/Computer_Science/Artificial_Intelligence	Ground Facts, Rules and Probabilistic Inference for Cyc One aspect of Cyc is a very large, logic-based knowledge base that includes, inter-alia, large amounts of background knowledge over a wide variety of domains, but it is more than that; the Cyc project is an attempt to move towards general artificial intelligence by supporting automated reasoning about a very wide variety of real-world concerns. To support that goal, Cyc also encompasses, obviously enough, and inference engine able to reason over a large, contextual, knowledge base, but it also includes components for interpreting and producing natural language, acquiring knowledge and responding to user queries, and for interfacing with other software. Applying logic to representation of general knowledge, /at scale/, and using it in the production of intelligent behaviors has been difficult enough; unfortunately it is becoming clear that doing so using traditional logics is probably not sufficient, either for satisfying a long term goal of supporting general intelligence, or even for shorter term goals, like recognizing, interpreting, and elaborating descriptions of piracy events. In this talk, I'll briefly describe what Cyc is, and has been, and how it is growing, touch on an early approach to abductive reasoning and classification in a traditional logical framework, and some difficulties with that approach, and then describe recent, very initial work training the Markov Logic networks based on ground facts and rules within the millions of axioms of the Cyc KB. Finally I'll sketch a vision for a system that truly integrates both sound, deductive reasoning, and the bounded unsoundness of probabilistic classification, induction, abduction and deduction.
Top/Computer_Science/Artificial_Intelligence	Interview with David Hardoon
Top/Computer_Science/Artificial_Intelligence	A Unified Approach to Deduction and Induction
Top/Computer_Science/Artificial_Intelligence	Learning for Control from Multiple Demonstrations We consider the problem of learning to follow a desired trajectory when given a small number of demonstrations from a sub-optimal expert. We present an algorithm that (i) extracts the---initially unknown---desired trajectory from the sub-optimal expert's demonstrations and (ii) learns a local model suitable for control along the learned trajectory. We apply our algorithm to the problem of autonomous helicopter flight. In all cases, the autonomous helicopter's performance exceeds that of our expert helicopter pilot's demonstrations. Even stronger, our results significantly extend the state-of-the-art in autonomous helicopter aerobatics. In particular, our results include the first autonomous tic-tocs, loops and hurricane, vastly superior performance on previously performed aerobatic maneuvers (such as in-place flips and rolls), and a complete airshow, which requires autonomous transitions between these and various other maneuvers.
Top/Computer_Science/Decision_Support	Systems and Techniques for Decision Support
Top/Computer_Science/Decision_Support	Decision Support, Multi-Attribute Decission Modelling and DEXI
Top/Computer_Science/Decision_Support	Decision Support for Everyone: Olap in Ms Excel
Top/Computer_Science/Algorithms_and_Data_Structures	Compact indexing of versioned data
Top/Computer_Science/Algorithms_and_Data_Structures	A Quadratic Programming Approach to the Graph Edit Distance Problem In this paper we propose a quadratic programming approach to computing the edit distance of graphs. Whereas the standard edit distance is defined with respect to a minimum-cost edit path between graphs, we introduce the notion of fuzzy edit paths between graphs and provide a quadratic programming formulation for the minimization of fuzzy edit costs. Experiments on real-world graph data demonstrate that our proposed method is able to outperform the standard edit distance method in terms of recognition accuracy on two out of three data sets.
Top/Computer_Science/Algorithms_and_Data_Structures	Approximating TSP Solution by MST based Graph Pyramid The traveling salesperson problem (TSP) is difficult to solve for input instances with large number of cities. Instead of finding the solution of an input with a large number of cities, the problem is approximated into a simpler form containing smaller number of cities, which is then solved optimally. Graph pyramid solution strategies, in a bottom-up manner using Boruvkas minimum spanning tree, convert a 2D Euclidean TSP problem with a large number of cities into successively smaller problems (graphs) with similar layout and solution, until the number of cities is small enough to seek the optimal solution. Expanding this tour solution in a top-down manner to the lower levels of the pyramid approximates the solution. The new model has an adaptive spatial structure and it simulates visual acuity and visual attention. The model solves the TSP problem sequentially, by moving attention from city to city with the same quality as humans. Graph pyramid data structures and processing strategies are a plausible model for finding near-optimal solutions for computationally hard pattern recognition problems.
Top/Computer_Science/Algorithms_and_Data_Structures	Succint Data Structures
Top/Computer_Science/Chemoinformatics	Graph kernels and applications in chemoinformatics Several problems in chemistry can be formulated as classification or regression problems over molecules which, when represented by their planar structure, can be seen as labeled graphs. Several approaches have been proposed recently to define positive definite kernels over labeled graphs, paving the way to the use of powerful kernel methods in chemoinformatics. In this talk I will review some of these approaches and present relevant applications in computational chemistry.
Top/Computer_Science/Machine_Learning/Instance-based_Learning	Machine Learning for Stock Selection In this paper, we propose a new method called Prototype Ranking (PR) designed for the stock selection problem. PR takes into account the huge size of real-world stock data and applies a modified competitive learning technique to predict the ranks of stocks. The primary target of PR is to select the top performing stocks among many ordinary stocks. PR is designed to perform the learning and testing in a noisy stocks sample set where the top performing stocks are usually the minority. The performance of PR is evaluated by a trading simulation of the real stock data. Each week the stocks with the highest predicted ranks are chosen to construct a portfolio. In the period of 1978-2004, PRs portfolio earns a much higher average return as well as a higher risk-adjusted return than Coopers method, which shows that the PR method leads to a clear profit improvement.
Top/Computer_Science/Machine_Learning/Instance-based_Learning	Learning to Combine Distances for Complex Representations The k-Nearest Neighbors algorithm can be easily adapted to classify complex objects (e.g. sets, graphs) as long as a proper dissimilarity function is given over an input space. Both the representation of the learning instances and the dissimilarity employed on that representation should be determined on the basis of domain knowledge. However, even in the presence of domain knowledge, it can be far from obvious which complex representation should be used or which dissimilarity should be applied on the chosen representation. In this paper we present a framework that allows to combine different complex representations of a given learning problem and/or different dissimilarities defined on these representations. We build on ideas developed previously on metric learning for vectorial data. We demonstrate the utility of our method in domains in which the learning instances are represented as sets of vectors by learning how to combine different set distance measures.
Top/Computer_Science/Machine_Learning/Instance-based_Learning	Support Feature Machine for Classification of Abnormal Brain Activity In this study, a novel multidimensional time series classification technique, namely support feature machine (SFM), is proposed. SFM is inspired by the optimization model of support vector machine and the nearest neighbor rule to incorporate both spatial and temporal of the multi-dimensional time series data. This paper also describes an application of SFM for detecting abnormal brain activity. Epilepsy is a case in point in this study. In epilepsy studies, electroencephalograms (EEGs), acquired in multidimensional time series format, have been traditionally used as a gold-standard tool for capturing the electrical changes in the brain. From multi-dimensional EEG time series data, SFM was used to identify seizure pre-cursors and detect seizure susceptibility (pre-seizure) periods. The empirical results showed that SFM achieved over 80% correct classification of per-seizure EEG on average in 10 patients using 5-fold cross validation. The proposed optimization model of SFM is very compact and scalable, and can be implemented as an online algorithm. The outcome of this study suggests that it is possible to construct a computerized algorithm used to detect seizure pre-cursors and warn of impending seizures through EEG classification.
Top/Computer_Science/Grid_Computing	Concepts of grid computing
Top/Computer_Science/Natural_Language_Processing	POWERSET - Natural Language and the Semantic Web The **Semantic Web** promises to revolutionize access to information by adding machine-readable semantic information to content which is normally interpretable only by people. In addition, it will also revolutionize access to services by adding semantic information to create machine-readable service descriptions. This ambitious vision has been slow to take off because of a chicken and egg problem. Markup is required before people will build applications, applications are required before it is worth the hard work of doing markup. **Natural language processing (NLP)** has advanced to the point where it can break the impasse and open up the possibilities of the Semantic Web. First, NLP systems can now automatically create annotations from unstructured text. This provides the data that semantic web applications require. Second, NLP systems are themselves consumers of semantic web information and thus provide economic motivation for people to create and maintain such information. For example, a new generation of natural language search systems, as illustrated by Powerset, can take advantage of semantic web markup and ontologies to augment their interpretation of underlying textual content. They can also expose semantic web services directly in response to natural language queries.
Top/Computer_Science/Natural_Language_Processing	A Concept-based Model for Enhancing Text Categorization Most of text categorization techniques are based on word and/or phrase analysis of the text. Statistical analysis of a term frequency captures the importance of the term within a document only. However, two terms can have the same frequency in their documents, but one term contributes more to the meaning of its sentences than the other term. Thus, the underlying model should indicate terms that capture the semantics of text. In this case, the model can capture terms that present the concepts of the sentence, which leads to discover the topic of the document. A new concept-based model that analyzes terms on the sentence and document levels rather than the traditional analysis of document only is introduced. The concept-based model can effectively discriminate between non-important terms with respect to sentence semantics and terms which hold the concepts that represent the sentence meaning. The proposed model consists of concept-based statistical analyzer, conceptual ontological graph representation, and concept extractor. The term which contributes to the sentence semantics is assigned two different weights by the concept-based statistical analyzer and the conceptual ontological graph representation. These two weights are combined into a new weight. The concepts that have maximum combined weights are selected by the concept extractor. A set of experiments using the proposed concept-based model on different datasets in text categorization is conducted. The experiments demonstrate the comparison between traditional weighting and the concept-based weighting obtained by the combined approach of the concept-based statistical analyzer and the conceptual ontological graph. The evaluation of results is relied on two quality measures, the Macro-averaged F1 and the Error rate. These quality measures are improved when the newly developed concept-based model is used to enhance the quality of the text categorization
Top/Computer_Science/Natural_Language_Processing	Machine Learning for Sequential Data: A Comparative Study with Applications to Natural Language Processing
Top/Computer_Science/Natural_Language_Processing	Textual Entailment Recognition Based on Inversion Transduction Grammar
Top/Computer_Science/Natural_Language_Processing	Recognizing Textual Entailment with LCCs GROUNDHOG System We introduce a new system for recognizing textual entailment (known as GROUNDHOG) which utilizes a classification-based approach to combine lexico-semantic information derived from text processing applications with a large collection of paraphrases acquired automatically from the WWW. Trained on 200,000 examples of textual entailment extracted from newswire corpora, our system managed to classify more than 75% of the pairs in the 2006 PASCAL RTE Test Set correctly.
Top/Computer_Science/Natural_Language_Processing	Two Related Lexico-Syntactic Approaches to Entailment Two approaches to Textual Entailment are presented. They both rely on lexico-syntactic information. The two approaches differ mainly in the way the syntactic relationships are derived. In one approach, the syntactic relationships are drawn from a phrase-based parse tree. In the other, we use information provided by a dependency parser. The first approach performs at 0.59 precision and 0.6047 average precision. The second systems performance is 0.5837 precision and 0.5785 average precision.
Top/Computer_Science/Natural_Language_Processing	Combining Shallow and Deep NLP Methods for Recognizing Textual Entailment
Top/Computer_Science/Natural_Language_Processing	UCD IIRG Approach to the Textual Entailment Challenge
Top/Computer_Science/Natural_Language_Processing	Morphological Learning as Principled Argument We develop a morphological learner that evaluates evidence supporting specific claims that a string of letters is a distributional meaningful unit. The distributional evidence is evaluated by selectional properties of morphs, while evidence towards meaning is modelled by looking at the relationship between stems and words. To assess a proposed affix, it gets a probability measure of meaning by comparing all the possible stems the affix occur with to the particular subset that also occur as words. Since for a stem to be a word counts as evidence towards its meaning, the ratio formed by taking stems that are words to the whole set of possible stems for an affix gives a predictive probability measure for the affix that measures the chance that it has combined with a meaningful stem. This measure, taken in conjunction with the selectional statistics of stems and affixes, provides a basis for deciding on the best morphological structure for a given word. The results for English show a combined precision and recall of 45.
Top/Computer_Science/Natural_Language_Processing	Unsupervised Learning for Natural Language Processing Given the abundance of text data, unsupervised approaches are very appealing for natural language processing. We present three latent variable systems which achieve state-of-the-art results in domains previously dominated by fully supervised systems. For syntactic parsing, we describe a grammar induction technique which begins with coarse syntactic structures and iteratively refines them in an unsupervised fashion. The resulting coarse-to-fine grammars admit efficient coarse-to-fine inference schemes and have produced the best parsing results in a variety of languages. For co reference resolution, we describe a discourse model in which entities are shared across documents using a hierarchical Dirichlet process. In each document, entities are repeatedly rendered into mention strings by a sequential model of attentional state and anaphoric constraint. Despite being fully unsupervised, this approach is competitive with the best supervised approaches. Finally, for machine translation, we present a model which learns translation lexicons from non-parallel corpora. Alignments between word types are modeled by a prior over matchings. Given any fixed alignment, a joint density over word vectors derives from probabilistic canonical correlation analysis. This approach is capable of discovering high-precision translations, even when the underlying corpora and languages are divergent.
Top/Computer_Science/Natural_Language_Processing	Structured Prediction Problems in Natural Language Processing Modeling language at the syntactic or semantic level is a key problem in natural language processing, and involves a challenging set of structured prediction problems. In this talk I'll describe work on machine learning approaches for syntax and semantics, with a particular focus on lexicalized grammar formalisms such as dependency grammars, tree adjoining grammars, and categorial grammars. ;I'll address key issues in the following areas: :1) the design of learning algorithms for structured linguistic data; :2) the design of representations that are used within these learning algorithms; :3) the design of efficient approximate inference algorithms for lexicalized grammars, in cases where exact inference can be very expensive. In addition, I'll describe applications to machine translation, and natural language interfaces.
Top/Computer_Science/Natural_Language_Processing	Web mining for natural language engineering tasks
Top/Computer_Science/Natural_Language_Processing	Machine Reading at Web Scale
Top/Computer_Science/Natural_Language_Processing	Latent Variable Models for Document Analysis Wray Buntine will consider various problems in document analysis (named entity recognition, natural language parsing, information retrieval), and look at various probabilistic graphical models and algorithms for addressing the problem. This will not be an extensive coverage of information extraction or natural language processing, but rather a look at some of the theory, methods and practice of particular cases, including the use of software environments.
Top/Computer_Science/Machine_Learning/Regression	Tree Augmented Naive Bayes for Regression Using Mixtures of Truncated Exponentials: Application to Higher Education Management In this paper we explore the use of Tree Augmented Naive Bayes (TAN) in regression problems where some of the independent variables are continuous and some others are discrete. The proposed solution is based on the approximation of the joint distribution by a Mixture of Truncated Exponentials (MTE). The construction of the TAN structure requires the use of the conditional mutual information, which cannot be analytically obtained for MTEs. In order to solve this problem, we introduce an unbiased estimator of the conditional mutual information, based on Monte Carlo estimation. We test the performance of the proposed model in a real life context, related to higher education management, where regression problems with discrete and continuous variables are common. This work has been supported by the Spanish Ministry of Education and Science, project TIN2004-06204-C03-01 and by Junta de Andaluca, project P05-TIC-00276.
Top/Computer_Science/Machine_Learning/Regression	Large-Margin Thresholded Ensembles for Ordinal Regression We propose a thresholded ensemble model for ordinal regression problems. The model consists of a weighted ensemble of confidence functions and an ordered vector of thresholds. Using such a model, we could theoretically and algorithmically reduce ordinal regression problems to binary classification problems in the area of ensemble learning. Based on the reduction, we derive novel large-margin bounds of common error functions, such as the classification error and the absolute error. In addition, we also design two novel boosting approaches for constructing thresholded ensembles. Both our approaches have comparable performance to the state-of-the-art algorithms, but enjoy the benefit of faster training. Experimental results on benchmark datasets demonstrate the usefulness of our boosting approaches.
Top/Computer_Science/Machine_Learning/Regression	Multiplicative Updates for L1-Regularized Linear and Logistic Regression Multiplicative update rules have proven useful in many areas of machine learning. Simple to implement, guaranteed to converge, they account in part for the widespread popularity of algorithms such as nonnegative matrix factorization and Expectation-Maximization. In this paper, we show how to derive multiplicative updates for problems in L1-regularized linear and logistic regression. For L1regularized linear regression, the updates are derived by reformulating the required optimization as a problem in nonnegative quadratic programming (NQP). The dual of this problem, itself an instance of NQP, can also be solved using multiplicative updates; moreover, the observed duality gap can be used to bound the error of intermediate solutions. For L1regularized logistic regression, we derive similar updates using an iteratively reweighted least squares approach. We present illustrative experimental results and describe efficient implementations for large-scale problems of interest (e.g., with tens of thousands of examples and over one million features).
Top/Computer_Science/Machine_Learning/Regression	Utility-Based Regression
Top/Computers/Telecommunications	Next Generation OSS Workshop
Top/Biology/Genetics	Gene, Organism and Environment: Bad Metaphors and Good Biology The standard metaphors used to describe DNA and development are examined, including the claim that DNA 'makes' protein, that DNA is&#160;'self-replicating' and the organisms 'adapt' to their environments. In this lecture by distinguished evolutionary geneticist Richard Lewontin, he explains that DNA is manufactured by the cell machinery, that proteins are folded by rules that are not related to DNA sequence and that organisms, rather than adapting to their environment, are actively engaging in constructing their own environments, so that organisms and environments co-evolve.
Top/Biology/Genetics	Where Mind and Matter Meet Recent advances in cellular science are heralding an important evolutionary turning point. For almost fifty years we have held the illusion that our health and fate were preprogrammed in our genes, a concept referred to as genetic determinacy. Though mass consciousness is currently imbued with the belief that the character of one's life is genetically predetermined, a radically new understanding is unfolding at the leading edge of science. Cellular biologists now recognize that the environment, the external universe and our internal physiology, and more importantly, our perception of the environment, directly controls the activity of our genes. This video will broadly review the molecular mechanisms by which environmental awareness interfaces genetic regulation and guides organismal evolution.
Top/Biology/Genetics	Return to the RNAi World: Rethinking Gene Expression and Evolution While investigating the genetic workings of the microscopic worm, C. elegans, Mello and&#160;colleague Andrew Fire, PhD, of the Carnegie Institution of Washington, discovered RNAi, a natural but previously unrecognized process by which a certain form of RNA can be manipulated to silenceor interfere withthe expression of a selected gene. The discovery, published in the journal Nature in 1998, has had two extraordinary impacts on biological science. One is as a research tool: RNAi is now the state-of-the-art method by which scientists can knock out the expression of specific **genes** in cells, to thus define the biological functions of those **genes**. But just as important has been the finding that RNA interference is a normal process of genetic regulation that takes place during development. Thus, RNAi has provided not only a powerful research tool for experimentally knocking out the expression of specific **genes**, but has opened a completely new and totally unanticipated window on developmental gene regulation. RNAi is now showing promising in the clinic as a new class of gene-specific therapeutics.
Top/Biology/Genetics	Mysteries of the Human Genome The human genome, the hereditary material we pass on to our progeny, can be cast as a 3 billion letter string over a DNA alphabet of four. We currently understand 1.5% of this mass, mostly in the form of genes, DNA substrings that code for proteins, the quintessential constituents of every living cell. The remainder 98.5% of our genome was often deemed as 'junk'. This picture changed when the genome of related species became available. By comparison we are suddenly able to pinpoint the locations of a staggering one million additional human subsequences that must be important to the human cell. The functions of these regions remain largely unknown, while their sheer volume overwhelms any comprehensive experimental approach. Guided by experimental results for handfuls of these subsequences, computational approaches can be employed to tackle the tremendous challenge of understanding this data and providing key biological observations. In this talk, I will describe ultraconserved elements, some of the most perplexing regions within the human genome, and track down a phenomenon of turning genomic junk into gold. The talk will assume no prior knowledge in Molecular Biology.
Top/Biology/Genetics	Hierarchical Analysis of Piecewise Affine Models of Gene Regulatory Networks We propose in this paper a method to hierarchically organize a certain type of piecewise affine differential system. This specific class of dynamical systems has been extensively studied for the past few years, as it provides a good framework to model gene regulatory networks. Using the hierarchical organization of a piecewise affine system, we present a technique to qualitatively analyze the asymptotic behavior of the whole system thanks to the analysis of several smaller subsystems. Specifically adapted to these networks, an algorithm of threshold elimination is presented, that refines in certain cases the hierarchical decomposition and therefore improves the analysis.
Top/Biology/Genetics	Interacting Random Boolean Networks Random Boolean networks (RBN) have been extensively studied asmodels of genetic regulatory networks. While many studies have been devoted tothe dynamics of isolated random Boolean networks, which may considered asmodels of isolated cells, in this paper we consider a set of interacting RBNs,which may be regarded as a simplified model of a tissue or a monoclonal colony.In order to do so, we introduce a cellular automata (CA) model, where each cellsite is occupied by a RBN. The mutual influence among cells is modelled byletting the activation of some genes in a RBN be affected by that of some genes inneighbouring RBNs. It is shown that the dynamics of the CA is far from trivial.Different measures are introduced to provide indications about the overallbehaviour. In a sense which is made precise in the text, it is shown that the degreeof order of the CA is affected by the interaction strength, and that markedlydifferent behaviours are observed. We propose a classification of these behavioursinto four classes, based upon the way in which the various measures of order areaffected by the interaction strength. It is shown that the dynamical properties ofisolated RBNs affect the probability that a CA composed by those RBNs belongsto one of the four classes, and therefore also affects the probability that a higherinteraction strength leads to a greater, or a smaller, degree of order.
Top/Biology/Genetics	Relationship between structure and dynamics of gene regulatory networks Gene regulatory networks in living cells are comprised of recurring network motifs, which perform key functions to control cellular responses. Mathematical models of network motifs are already developed and experimentally validated. We analyze the stability of previously validated mathematical models of network motifs against the fluctuations in their associated biochemical reaction rates and find that all of these elementary functional modules are stable against any perturbation in their rates of reaction. In gene regulatory networks the motifs are connected to each other in a directed acyclic manner. Any large assembly of stable and robust motifs connected with each other in a directed acyclic manner exhibits stable and robust dynamics. All the motifs except those having feedback loops in their structure preserve certain dynamic properties when embedded within large operational networks. Our study also suggests that evolutionary mechanisms selected stable and robust functional modules with which to build regulatory networks in order to ensure stability and reliability on a larger scale instead of finding the simplest canonical representation with which to serve the same purpose.
Top/Biology/Genetics	Lost in Translation from Genes to Organisms Data mining genomes and developmental genetics provides information for building mathematical gene-network models on development. These models can be used to provide insilico predictions about processes such as developmental expression and regulation of genes.
Top/Biology/Genetics	Some Challenging Machine Learning Problems in Computational Biology: Time-Varying Networks Inference and Sparse Structured Input-Out Learning Recent advances in high-throughput technologies such as microarrays and genome-wide sequencing have led to an avalanche of new biological data that are dynamic, noisy, heterogeneous, and high-dimensional. They have raised unprecedented challenges in machine learning and high-dimensional statistical analysis; and their close relevance to human health and social welfare has often created unique demands on performance metric different from standard data mining or pattern recognition problems. In this talk, I will discuss two of such problems. First, I will present a new statistical formalism for modeling network evolution over time, and several new algorithms based on temporal extensions of the sparse graphical logistic regression, for parsimonious reverse-engineering the latent time varying networks. I will show some promising results on recovering the latent sequence of temporally rewiring gene networks over more than 4000 genes during the life cycle of Drosophila melanogaster from microarray time course, at a time resolution only limited by sample frequency. Second, I will present a family of sparse structured regression models in the context of uncovering true associations between linked genetic variations (inputs) in the genome and networks of human traits (outputs) in the phenome. If time allows, I will also present another class of new models known as the maximum entropy discrimination Markov networks, which address the same problem in the maximum margin paradigm, but using a entropic regularizer that lead to a distribution of structured prediction functions that are simultaneously primal and dual sparse (i.e., with few support vectors, and of low effective feature dimension). Joint work with Amr Ahmed, Seyoung Kim, Mladen Kolar, Le Song and Jun Zhu.
Top/Computers/Social_Networking	Our Lives, Our Facebooks Students at a large number of American colleges and universities have come to rely on The Facebook ([[http://facebook.com/]]) as a vital supplement to their social lives. A social connector website, Facebook serves the information needs of students who have perpetually in-flux social networks. As a result, frequency and penetration of student use is remarkable. In this presentation, a longitudinal analysis of Facebook use by freshmen at the University of North Carolina at Chapel Hill will be presented. Use patterns will be analyzed, with a special concentration on factors that contributed to the product's success.
Top/Biology/Molecular_biology	How the Body Fights Infection Miniaturized battles are waged continuously by heroic micro-warriors that protect us from viruses, bacteria, fungi and parasites. Join Dr. Richard Locksley for a look at how these unseen victories (and occasional defeats) are played out, and how vaccination stacks the deck in our favor.
Top/Biology/Molecular_biology	Consistency Principle in Biological Dynamical Systems We propose consistency principle between different levels, to understand a biological system. Three topics are discussed. First, as a result of consistency between molecule replication and cell reproduction, universal statistical laws on chemical abundances over cells are derived as also confirmed experimentally. They include power-law distribution of gene expressions,log-normal distribution of chemical abundances over cells, and embedding of the power law into the network connectivity. Second, as a result of consistency between gene and phenotype,a general relationship between phenotype fluctuations by genetic variation and isogenic phenotypic fluctuation by developmental noise is derived. Third, we touch upon the chaos mechanism for stem cell differentiation with autonomous regulation, as a result of consistency of cell reproduction and growth of cell ensemble.
Top/Biology/Molecular_biology	Endocytosis and Signaling: Competition of Rab Proteins Endocytosis and signaling are tightly linked. A dynamic network of subcellular compartments, termed endosomes, actively controls signal propagation, amplitude and timing by uptake and transport of membrane-associated signaling molecules. The small GTPase Rab5 is the central organizer of the protein machinery that assembles on early endosomes, providing a unique identity to the compartment that controls cargo sorting, transport and signaling potential. We interlink experimental and computational approaches to (I) unravel the design principles underlying the function and regulation of the endocytic pathway and to (II) understand the molecular links that govern endosomal control of signaling events. Our approach takes into account both newly identified Rab5 effectors as well as the perspective of an on-going genome-wide RNAi screening for novel endocytic transport regulators. We develop and analyze a mathematical core model of the kinetic interactions between the endosomal organizers Rab4, Rab5 and Rab7. The values of the model parameters are estimated from literature and FRAP experiments. Simulations and bifurcation analysis of the model reveal bistable behavior of Rab5 versus Rab7 dominance that reproduces the controled and robust transition from early to late endosomal compartments as observed by means of computational motion tracking of individual endosomes in fluorescence live cell imaging data.The modeled Rab5 module serves as a blueprint for the sequential assembly of the endocytic pathway, therewith contributing an important spatial aspect to systems biology. Our kinetic model explains the experimentally observed transitions between endosomal compartments as the result of cargo-regulated competition between Rab proteins.
Top/Biology/Molecular_biology	Information Transfer in Calcium Signal Transduction Calcium ions act as second messenger in many cell types. They transfer extracellular signals (e.g. from hormones) to targets within the cell, like Ca2+-dependent enzymes or transcription factors. Since a number of different effectors and cellular targets exist, it has been suggested, that specific information is encoded in the amplitude, frequency and waveform of the Ca-signal and decoded again, later on, by cellular targets. After stimulation, the calcium concentration in the cytosol of hepatocytes, for example, can display complex dynamic behavior including spiking and bursting oscillations. Using the information-theoretic measure Transfer Entropy (Schreiber 2000) we studied the properties of this signal transduction under different conditions. Therefore, we coupled a simple Ca2+-dependent enzyme activation process to a model of calcium oscillations (Kummer 2000) and to experimentally measured calcium time series. We simulated the system stochastically to account for random fluctuations in the case of low particle numbers. To approximate the rate of information transfer we analyzed the resulting time series for different levels of activation and different numbers of particles using kernel density estimation.
Top/Biology/Microbiology	How Bacteria Cause Disease Join Warren Levinson to learn about the various agents that cause infectious diseases: bacteria, viruses, fungi, protozoa and worms, with a focus on how bacteria are transmitted and cause disease, and how exotoxins and endotoxins cause symptoms of disease.
Top/Biology/Microbiology	BacGrid: Simulations of Bacteria using the GRID Bacterial biofilms provide systems of the complexity needed to&#160;exemplify many of the generic features of multi-cellular behaviour, without such complexity at once becoming overwhelming. They are in addition of enormous environmental, industrial and medical importance. Many processes in biofilms operate at the macroscopic scale and are thus susceptible continuum modelling approaches. It is essential, however, that models incorporate in an appropriate way information about the micro-scale behaviour and their results must in turn be coupled back into the rules adopted in the cell-scale modelling, motivating the use of agent-based modelling. The simulation of such complex systems typically requires huge computing resources. The Grid provides an unrivalled technology for large scale distributed simulation and is exceptionally well suited to addressing the challenges raised by integrative-biology. In this paper we present BacGrid, a system for performing distributed simulation of bacteria using the High Level Architecture (HLA) and the Grid. We present the bacterial model and show results from initial experiments investigating the role of quorum sensing molecule (QSM) in the development of the bacterial colony. We go on to sketch out the design for the distribution of bacterial simulation components across the grid and indicate how this technology can be used to create large scale simulations. We conclude with a discussion of the current status of the system and our plans for future work and experiments.
Top/Biology/Microbiology	Individual-based Model of Bacterial Mobility in Biofilms We propose a new individual-based model of mobile bacteria producing a biofilm. The model is based on the assumption that bacteria have a brownian motion and produce a polymeric substance that reduce their mobility by increasing local viscosity. We define the biofilm as the polymeric substance together with the bacteria it contains. Our simulation results show that bacteria mobility increases the heterogeneity and the complexity of the biofilm (that we evaluate with an entropy measure). The obtained biofilm patterns show good qualitative agreement with experimental observation of biofilm micrographs.
Top/Biology/Evolutionary_biology	Evolution of the Human Species Eminent evolutionary biologist Christopher Wills takes you on an exploration of human evolutionary history and how it is derived from both&#160;the genetic and fossil records.
Top/Biology/Evolutionary_biology	Evolution of Sex It has been a century and a half since Darwin provided the first mechanistic explanation for the complexity of the living things we see around us. Only in the last 30 years or so have computational systems been employed to try out natural selection on complex artificial problems. There have been some successes, but the complexity of artificially evolved systems remains a very long way short of the complexity that is easy to find in biology. Why is this? Is our understanding of natural evolution missing something important? How can we improve our artificial problem solving methods to make them work better on large-scale complex problems?
Top/Biology/Cell_biology	Firewalls in Atrial Myocytes Atrial myocoytes play a prominent role in the generation of heart beats. Their contraction is controlled by Calcium signals that emerge at the cellular periphery and then proceed centripetally to engage the force-generating myofilaments. Experiments have demonstrated that these initial signals need to overcome a barrier just below the cell membrane before they move inward. Since atrial myoctes lack transverse tubules that transmit external signals to the cell interior as e.g. in ventricular myocytes, such a firewall represents a crucial determinant of atrial dynamics. For instances, it allows atrial myocytes to fine tune their responses to a wide range of vital stimuli. Here, we present a computationally advantageous model to investigate the mechanisms that give rise to these graded centripetal signals. Our framework takes into account the three dimensional organisation of atrial myocytes, especially the spatially restricted release of Calcium from internal storage compartments. We employ a fire-diffuse-fire (FDF) model to examine the spatio-temporal patterns and to probe the dependence of wave propagation on physiologically relevant parameters. Mimicking an excitable medium, the FDF approach reflects the significance of noise in intracellullar Calcium dynamics. The explicit construction of the corresponding Green's function allows for a detailed analysis.
Top/Biology/Cell_biology	Interacting Random Boolean Networks Random Boolean networks (RBN) have been extensively studied asmodels of genetic regulatory networks. While many studies have been devoted tothe dynamics of isolated random Boolean networks, which may considered asmodels of isolated cells, in this paper we consider a set of interacting RBNs,which may be regarded as a simplified model of a tissue or a monoclonal colony.In order to do so, we introduce a cellular automata (CA) model, where each cellsite is occupied by a RBN. The mutual influence among cells is modelled byletting the activation of some genes in a RBN be affected by that of some genes inneighbouring RBNs. It is shown that the dynamics of the CA is far from trivial.Different measures are introduced to provide indications about the overallbehaviour. In a sense which is made precise in the text, it is shown that the degreeof order of the CA is affected by the interaction strength, and that markedlydifferent behaviours are observed. We propose a classification of these behavioursinto four classes, based upon the way in which the various measures of order areaffected by the interaction strength. It is shown that the dynamical properties ofisolated RBNs affect the probability that a CA composed by those RBNs belongsto one of the four classes, and therefore also affects the probability that a higherinteraction strength leads to a greater, or a smaller, degree of order.
Top/Computer_Science/Semantic_Web/Semantic_Web_Services	Research 7: Causal link matrix and AI planning: A model for Web service composition Automated composition of Web services or the process of forming new value added Web services is one of the most promising challenges in the semantic Web service research area. Semantics is one of the key elements for the automated composition of Web services because such a process requires rich machine-understandable descriptions of services that can be shared. Semantics enables Web service to describe their capabilities and processes, nevertheless there is still some work to be done. Indeed Web services described at functional level need a formal context to perform the automated composition of Web services. The suggested model (i.e., Causal link matrix) is a necessary starting point to apply problem-solving techniques such as regression-based search for Web service composition. The model supports a semantic context in order to find a correct, complete, consistent and optimal plan as a solution. In this paper an innovative and formal model for an AI planning-oriented composition is presented.
Top/Computer_Science/Semantic_Web/Semantic_Web_Services	Internet of Services
Top/Philosophy	Truth in Fan Fiction The lecture is an introduction to 'fan fiction' or 'amateur fiction'. The first part comprises philosophy and moves to fan fiction itself with examples. At the end we will discuss philosophical puzzels that fan fiction poses wihtin theory of fiction. At the end we will solve the problem or we will atleast try to.
Top/Philosophy	Virtual Worlds, Contextualism, and the Myth of Fiction In this lecture I will tell you a story about my exploits in Second Life, at the end of that I will extract two philosophical conclusions. At the beginning of the lecture we will have fun and discuss about various philosophical aspects and at the end we will get serious and try to establish a conclusion. We will start off by discussing about two virtual worlds...
Top/Philosophy	Rules, Race, and Mel Gibson 2006 Slavoj Zizek talking about the explicit, truth, rules, politics, Mel Gibson, society, race, racism, antisemitism; lecturing and developing a psychoanalysis of culture and societies. Public open lecture for the students of the European Graduate School EGS, Media and Communication Studies department program, Saas-Fee, Switzerland, Europe, 2006,
Top/Philosophy	On Violence iek's lecture on his book Violence at the University of Leeds. Expounds on violence in all its dimensions, he introduces aspects that got left out in the book. The lecture was organized by the International Journal of iek Studies. Below is a brief chronological summary of the lecture's main themes: **Marxism and psychoanalysis** - They have a problematic relationship in so far as Marxism mis-uses psychoanalysis to fill in its theoretical gaps & shortcomings. The way forward is to open up a new domain that can be described by psychoanalysis but offers the grounds for a new revolutionary subjectivity. The possibility of a radical/revolutionary new subjectivity - a purportedly post-traumatic subjectivity typical of the 21st Century - with reference to Catherine Malabous recent book - Les nouveaux blesss (The New Wounded). **Impossible objects** - The core of humanity as the drive and fascination for things we cannot do or fully understand. **Freud & Lacan** - The relationship between external traumatic events/stimuli and internal psychical traumas. Sexuality is less about content and more about the form of the fantasy through which sexuality is expressed. Trauma/fantasy sexualizes sexuality itself, only through trauma can sexuality occur. Sexuality only occurs through the distortion of trauma. Catherine Malabou's critique of the Freudian/Lacanian position - our socio-political reality today has produced so many brutal external intrusions that our traditional symbolic structures have been destroyed. **Malabou's reproach** - that Freud always looks for the psychic trauma at the expense of the overwhelmingly destructive external shocks of profoundly violent events that have no rational meaning to be processed. The post-traumatic subject that results is emptied of his/her substance - there is nowhere to regress in the Freudian sense because your entire identity has been erased, you become a pure subject deprived of the symbolic content that the hermeneutics of psychoanalysis depend upon. This creates a new form of the living dead. The big political and psychoanalytical problem of today is how to think through this new traumatized subject. **Zizek's Lacanian response** - the underlying function of such Lacanian categories of castration etc. is to describe this manner in which the subject is deprived of his/her substance. Similarly, in German idealism we learn that the subject is substanceless. **Psychoanalytical theory's response** - The Big Other is what are you deprived of when you are subjected to this destructive violence, not as an all-knowing figure of authority but rather the Big Other as appearance. The fool and the grain of seed story (the mentally ill man who thought he was a grain of corn, returns to the asylum and is told off because he has been cured - to which he replies 'yes, but does the chicken know this' ... This reveals a central component of ideology - we all need a chicken not to know. E.g. the fact that whilst drawing to the end of his life, Tito was not informed of the parlous state of Yugoslavia's economy. So huge amounts of money was borrowed to keep Tito happy till his death. Economic crisis ensued with the subsequent outbreaks of genocidal nationalism - because the chicken was not allowed to know. **Culture today** - we can afford to be as cynical as we want, but we still need a chicken that doesn't know. E.g. in today's greatest remaining taboo is paedophilia and the corruption of childhood innocence - we still need a chicken, unlike the post-traumatic subjects who have lost their chicken. In cultural politics - this means we need more not less distance from our cultural neighbours. We need more codes of discretion, not more understanding. **The rise of gonzo pornography** - this marks the end of engagement as the ridiculous narratives of traditional pornography have been replaced by gonzo porn where all narrative has been removed. The basic lesson of the post-traumatic subject - the stories we are telling ourselves about ourselves, our innermost narratives, are ultimately lies. Psychoanalysis helps us to understand this truth. The rise of the detached subject & a new revolutionary perspective - there are a whole new series of antagonisms that capitalism cannot solve: 1. ecological concerns 2. intellectual property rights 3. excluded populations - gated communities These constitute 3 new forms of proletarianization in a basic philosophical sense in addition to the classic Marxist sense of exploitation. The common philosophical element of all three domains is the way in which the subject is deprived of his/her key substance. This is the grounds for the building of a new proletarianization e.g. Neo from The Matrix - deprived of his whole symbolic inner life. The new Cartesian subjectivity - Descartes's conception of the subject is now more relevant than ever before. The conceptualization of the subject that survives his/her own death - maybe we can provide a frame that will provide us with a common denominator for the shared struggles that we have today. Proletarianization has not disappeared - it has extended its reach.
Top/Philosophy	Why Only an Atheist Can Believe: Politics Between Fear and Trembling iek addresses the complicated relationship between belief, or what we take to be belief, and our desire to see all. The lecture is followed by a brief period of questions and answers.
Top/Philosophy	The Spectator's Malevolent Neutrality Slavoj iek is philosopher and Psychoanalyst from Ljubljana. His lecture on the specific roles of viewers and doers is entitled 'The Spectators Malevolent Neutrality' and was held on June 8, 2004 during the Theaterformen festival in Brunswick (DE).
Top/Philosophy	Politeness and Civility in the Function of Contemporary Ideology 'Maybe we just need a different chicken...' Slavoj iek spoke at Powell's City of Books in downtown Portland, Oregon, on September 9, 2008
Top/Philosophy	Otherness and Immigration
Top/Philosophy	Ecology as a New Opium for the Masses
Top/Philosophy	Fear Thy Neighbor as Thyself Hosted by The Institute for Human Sciences at Boston University Fear Thy Neighbor as Thyself: Antinomies of Tolerant Reason, begins by asking, 'What can philosophy do today? What can it tell the general public haunted by the problems of ecology, racism, religious conflict, and so on?' The role of philosophy, iek says, is not to provide answers, but to analyze how we view questions. 'How we perceive a problem can itself become part of the problem,' he says. To illustrate his various points, he uses such examples as Martin Luther King, Jr. (GRS'55, HON.'59), the doomed passengers on September 11's United Flight 93, and former U.S. Secretary of Defense Donald Rumsfeld, among others. A question-and-answer session follows the lecture.
Top/Philosophy	Structures on the Streets A lecture by Slavoj iek on the 8. conference of managment - EURAM 2008 in Ljubljana.
Top/Philosophy	Politics Between Fear and Terror
Top/Psychology/Developmental_Psychology	Bayesian models of human inductive learning In everyday learning and reasoning, people routinely draw successful generalizations from very limited evidence. Even young children can infer the meanings of words, hidden properties of objects, or the existence of causal relations from just one or a few relevant observations -- far outstripping the capabilities of conventional learning machines. How do they do it? And how can we bring machines closer to these human-like learning abilities? I will argue that people's everyday inductive leaps can be understood as approximations to Bayesian computations operating over structured representations of the world, what cognitive scientists have called 'intuitive theories' or 'schemas'. For each of several everyday learning tasks, I will consider how appropriate knowledge representations are structured and used, and how these representations could themselves be learned via Bayesian methods. The key challenge is to balance the need for strongly constrained inductive biases -- critical for generalization from very few examples -- with the flexibility to learn about the structure of new domains, to learn new inductive biases suitable for environments which we could not have been pre-programmed to perform in. The models I discuss will connect to several directions in contemporary machine learning, such as semi-supervised learning, structure learning in graphical models, hierarchical Bayesian modeling, and nonparametric Bayes.
Top/Biology/Ecology/Animal_ecology	Information Transfer in Moving Animal Groups The movement of animal flocks give us one of the clearest examples of the concept of 'complexity'. Simple interactions between animals lead to patterns that are somehow regular but at the same time difficult to characterise. In this paper we discuss first these models and their predictions about the movement of flocks. We provide novel results about how information is transfered in these systems. We look at recent experiments on locusts and pigeons which show that at least some of the patterns seen in these groups can be explained by the phase transitions and bifurcations that arise from these models. In particular, we look at a phase transition in the marching of locusts and symmetry breaking in the decision-making of pigeons.
Top/Biology/Human_biology	Ventricular Fibrillation in the Human Heart. Why is it different from Fibrillation in the Dog and Pig Heart? Sudden cardiac death is one of the major health problems in the industrialised world, leading to over 300,000 mortalities in the US alone annually. In most cases, it is caused by a cardiac arrhythmia called ventricular fibrillation (VF). Under normal conditions, the coordinated contraction of the heart leads to an effective pumping of blood through the body. In contrast, during fibrillation coordination of contraction is completely lost, rendering the heart incapable of pumping around blood. Despite the huge socio-economical costs of VF and decades of research its causes and mechanisms still remainpoorly understood. In experimental studies into the mechanisms of VF, pig and dog hearts are considered the best model systems for the human heart given their comparable size. In such studies it is found that fibrillation is caused by highly disorganised electrical wave patterns consisting of 50 or more rotating spiral waves. It has been assumed that a similar organisation underlieshuman VF. However, recent clinical studies suggest that fibrillation inthe human heart may have a far more simple organisation.Modelling studies have played an important role and are playing an increasingly important role in cardiac arrhythmia research from the single ion channel to the whole heart level. However, on the whole heart level, most modelling studies thus far have used phenomenological models or small heart animal models to obtain qualitative insights in VF mechanisms and patterns. Instead, we use a detailed model of the human ventricles, to quantitatively study human VF and why it might be different from VF in the pig and dog heart. We indeed find that human VF has a significantly simpler organisation than VF in the pig and dog heart, with wave patterns consisting of around 10 spiral waves only. We then study the dependence of VF wave pattern complexity on various major parameters of our model (excitability, anisotropy, action potential duration (APD) restitution slope, minimum APD). We find that VF wave pattern complexity is most strongly dependent on minimum APD, a factor that is found to differ between human and pig and dog hearts. We thus propose that differences in minimum action potential duration cause the differences in wave pattern complexity during VF in the human and pig and dog hearts. Both the simpler spatial organisation of human VF and it's suggested cause may have important implications for treating and preventing this dangerous arrhythmia in humans.
Top/Business/Transportation_and_Logistics	ERA-Net Transport
Top/Business/Transportation_and_Logistics	Sustainable Pavements for New Member States
Top/Business/Transportation_and_Logistics	International cooperation on Road Transport leading to improved mobility, safety, security and prosperity
Top/Business/Transportation_and_Logistics	TRA 2008 Keynote Address
Top/Business/Transportation_and_Logistics	Concluding remarks
Top/Business/Transportation_and_Logistics	The Road Transport Management System: A Self regulation initiative to promote load optimisation, vehicle maintenance and driver wellness in heavy vehicle transport in South Africa
Top/Business/Transportation_and_Logistics	TRA 2008 Keynote Address
Top/Business/Transportation_and_Logistics	Sustainable mobility for everybody - complementary solutions for passenger and commercial transportation
Top/Business/Transportation_and_Logistics	GOOD ROUTE: The aim, the approach and the outcomes
Top/Business/Transportation_and_Logistics	Road safety in Europe
Top/Business/Transportation_and_Logistics	New tools for linking transport and land use planning
Top/Business/Transportation_and_Logistics	The Nordic Road and Transport Research Program enters its 5th year
Top/Business/Transportation_and_Logistics	Strategies for future power-trains
Top/Computer_Science/Web_Search	An Empirical Analysis of Sponsored Search Performance in Search Engine Advertising
Top/Computer_Science/Web_Search	Mining Queries
Top/Business/Networked_Organizations	ECOLEAD, visions and goals
Top/Business/Networked_Organizations	Knowledge technologies for network organisations This lecture presents the current research work at JSI on Knowledge technologies and potentials these technologies have to solve problems in Networked organisations. Some real prototypes and solutions has been presented as well as some visionary plans.
Top/Business/Networked_Organizations	Supply Network Shannon
Top/Business/Networked_Organizations	Extended Products in VOs
Top/Business/Networked_Organizations	Emerging technologies for CNO supporting infrastructures
Top/Business/Networked_Organizations	2014: Semantic Technologies in Large Distributed Organisations
Top/Mathematics/Logic	A Unified Approach to Deduction and Induction
Top/Computer_Science/Databases	Relational Algebra for Ranked Tables with Similarities: Properties and Implementation The paper presents new developments in an extension of Codds relational model of data. The extension consists in equipping domains of attribute values with a similarity relation and adding ranks to rows of a database table. This way, the concept of a table over domains (i.e., relation over a relation scheme) of the classical Codds model extends to the concept of a ranked table over domains with similarities. When all similarities are ordinary identity relations and all ranks are set to 1, our extension becomes the ordinary Codds model. The main contribution of our paper is twofold. First, we present an outline of a relational algebra for our extension. Second, we deal with implementation issues of our extension. In addition to that, we also comment on related approaches presented in the literature.
Top/Society/Information_Society	Citizen Centric Approach
Top/Society/Information_Society	E-health
Top/Business/Management	From Pioneering to Prime Organization: Implications for Our Business Schools
Top/Business/Management	Management Education a personal view by Charles Handy
Top/Business/Transportation_and_Logistics/Traffic_Stafety	Road safety and its modification
Top/Business/Transportation_and_Logistics/Traffic_Stafety	Intelligent car = safer roads
Top/Humanities/Languages	Why European Translators Should Want to De-Westernize Translation Studies
Top/Humanities/Languages	That's close! On translators, interpreters, researchers, texts - and their interrelations
Top/Politics	The i2010 Agenda a European Success Story in the Making
Top/Technology/Nanotechnology/Nanomaterials	Novel routes to nano-materials for Li-ion batteries Advanced Li-ion batteries providing enhanced storage capacities and improved power performances are currently required not only by the fast-growing market of portable electronics, but also by emerging electric or hybrid-electric vehicles. We are investigating two novel techniques for this purpose: Spark Discharge Generation (SDG) and Electrostatic Spray Reductive Precipitation (ESRP). SDG uses a physical top down approach that relies on the atomization of two metal rods via a sudden spark. Two cylindrical rods are connected to high voltage and parallel to a variable capacitance. The capacitors are periodically charged to the break-down voltage of the system determined by the gap between the rods. Through the high temperature of the generated spark, electrode material is rapidly evaporated, and the vapour condenses to form nano-sized metallic particles. In addition, an unconventional densification technique, called Magnetic Pulse Compaction (MPC), is being used for self-manufacturing metal or alloy rods to be atomized. ESRP is a physical-chemical technique relying on a combined top-down and bottom-up approach, which bridges aerosol generation with chemical precipitation in order to form nanoparticles. Electro-Spraying of liquids consists in the creation of charged aerosols by applying a high voltage between a nozzle, through which the liquid to be sprayed is fed, and a counter-electrode. Interesting properties of this phenomenon are the narrow size distribution of the emitted droplets, as well as tuning the droplet size by controlling experimental parameters. Moreover, high net surface charge on the generated droplets causes repulsive interaction, preventing droplet coalescence. These beneficial aspects have been exploited in combination with a well-known technique for the synthesis of metallic and alloyed particles, namely Reductive Precipitation (RP) of metal chlorides by NaBH4. Dissolved metal chlorides precursors are forced to flow by a syringe pump which provides a constant supply to the nozzle with a controlled flow rate. Under the influence of the high electric field small charged droplets are formed and attracted towards a ring-shaped counter electrode, which is placed in the reductive solution. In this way, the droplets containing precursor metal ions are driven into the reductive bath, where they are immediately reduced to their zero-valent state. Primary particles with size in the range of 2-5 nm can be obtained by proper selection of the experimental parameters.
Top/Technology/Nanotechnology/Nanomaterials	Preparation of the Bi12SiO20 thin films by the sol-gel method Bismuth silicon oxide Bi12SiO20 (BSO), which has a sillenite structure, is a piezoelectric, electro-optic, photo-refractive, and optically active material. Recently, the sillenites have begun to be considered for use as dielectric in the field of electronics. They are used as new material in LTCC (Low-temperature co-fired ceramic) technology. Because of the low sintering temperature, good chemical and dielectric properties of the bulk BSO1 we decided to prepare thin films of this material. In this paper we will report on the preparation of BSO thin films using sol-gel method on various substrates. The aim of work was to achieve pure BSO thin films with good control over their microstructure and thickness.
Top/Technology/Nanotechnology/Nanomaterials	Photosensitive titanium oxide sols and gels for solar energy conversion and storage Due to the potential applications in the field of environmental protection, the photochemistry of TiO,,2,, is a fast growing area both in terms of research and commercial activity. Beside to the white pigment properties of rutile and anatase (e.g. paints and cosmetic products), titanium dioxide is used in heterogeneous catalysis and photocatalysis (water purification, air cleaning), in photoelectrochemical solar cells for the production of hydrogen and electricity, as an active layer in the design of electrochromic devices, as a gas sensor, as a corrosion-protective coating, in ceramics and in electric devices such as varistors, to name few. In such applications, the performance of titanium oxide could be optimized with specific nanostructural control over the morphology of the material. Under UV irradiation, an electron-hole pair is generated in TiO,,2,, then technological devices are based on chemical reactions or induced electron transfers. Due to the large band gap of TiO,,2,, (3.2eV), only 10% of the solar spectrum is used. A major objective for future work is the development of a semiconductor photocatalyst film which is able to utilize visible as well as UV light. Nanoscience has the potential to provide entirely new classes of materials with capabilities that transcend these limitations and generate the performance breakthroughs required for a viable economy based on sustainable energy. Recent investigations in this area allowed us to synthesize novel photo-sensitive titanium oxide sols and gels by controlling the condensation of titanium species in non aqueous solvents. Depending on different parameters such as concentration in Ti^^4+^^, ageing or thermal treatment, to emphasise few, the structuration of the inorganic framework leads to various layered structure. The adsorbed organic species control the growth of the nano-objects present in the sol or gel. Due to the enhanced surface area to volume ratio, these nanostructured sols and gels produce singular photo-electrochemical properties that are drastically different from their bulk counterparts. When irradiated, these materials can absorb photons with a lower energy than the bandgap energy of the original semiconductor, and thus a significant increase in the limiting efficiency of conventional solar cells is expected. These new materials are principally characterized by a partially occupied intermediate band, isolated from the valence and conduction bands. Our purpose is to use these intermediate band materials as sensitizers in both third generation photoelectrochemical solar cells and photo-batteries (ultracapacitors).
Top/Technology/Nanotechnology/Nanomaterials	The nanostructure of silicon thin films for solar cells A typical thin-film silicon solar cell is deposited on a glass substrate covered with a conductive transparent metal oxide. The active part of a solar cell consists of three to six silicon layers, each with a thickness of ten to several hundred nanometers, deposited in a layer-by-layer fashion. In these structures layers with different individual optical gaps is stacked together, in order to cover as much of the solar spectrum as possible. By changing the structure of the material, going from pure anorphous to monocrystalline, it is possible to obtain the variation in optical gap using the same material. Silicon in the form of nanocrystals drags in that sense particular attention in last decade. For any practical use, it is important to know size and size distribution of nano particles in this kind of structure. A series of multilayered silicon thin films was prepared by the decomposition of silane gas, diluted with hydrogen, in a radio-frequency glow discharge. Films with nanocrystallites (nc-Si) of different sizes were processed by varying the silane-to-hydrogen ratio. The nanostructures of the silicon thin films were studied by Raman spectroscopy (RS) and high-resolution transmission electron microscopy (HRTEM). Raman spectrum of the microcrystalline Si shows one intensive sharp band at 521 cm-1. For crystallites smaller than 30 nm this band (transversal optical - TO mode) is broadened and its position is shifted to lower frequencies. The shift is dependent on the average crystallite sizes. The size of the nanocrystallites in the investigated samples was estimated from the shift of the TO mode in the Raman spectra of the nc-Si after the deconvolution of the spectra. The volume fraction of the crystalline phase can be estimated from the ratio of the integrated intensities of the crystalline TO and the amorphous TO modes after the deconvolution of the Raman spectra. Since the deconvolution procedure influences the accuracy of obtained result, various methods of deconvolution were applied. Therefore, for the calculation of the Raman shifts and the integrated intensities, the spectra are frequently fitted as the sum of amorphous (Gaussian) contributions and crystalline contribution (Voight). We further deconvoluted the spectra, but using a somewhat different procedure than that one usually described in the literature. Since the spectra of the multilayered nc-Si thin films can be deconvoluted to the modes belonging to the a-Si and the TO mode of the nc-Si crystalline fraction, we first removed the amorphous contribution directly by subtracting the experimental spectra of completely a-Si from the spectra of our multilayered nc-Si samples. The TO band in the nc-Si appears as asymmetric and broad, which suggests the coexistence of smaller and larger crystals, so we deconvoluted the TO mode of the nc-Si into two components. The two components can be assigned as one belonging to the small crystallites, and the other to the larger crystallites.
Top/Technology/Nanotechnology/Nanomaterials	In-situ Fabrication, Manipulation and Property Measurements of Single Nanotubes and Nanowires with Near Atomic Resolution Carbon nanotube (CNT) and nanowire materials are important building
Top/Technology/Nanotechnology/Nanomaterials/Nanostructures	Nanostructuring of polycristalline gold thin films, deposited on glass, by means of ion beam Thin solid films appear most commonly in polycrystalline form, which means that they have higly constrained single-crystalline grains. Polycrystalline films are used in a large variety of devices, such as magnetic storage media, catalytic and thermal elements, protective coatings. It is thus desirable to extend to polycrystalline films, the approaches which have been developed for the self-organised formation of nanostructures on single-crystalline metal substrates. Ion beam sputtering can be used to modify surfaces on a nanoscale level, in most cases the result is formation of ripples on the surface. Substrates with well-defined vertical roughness, controlled orientation and periodicity can be achieved by varying macroscopic parameters that influence ripple formation, such as ion beam energy and ion beam dose. Thin Au films (150 nm thick) were deposited on glass microscope slides by two different deposition techniques: thermal and sputter deposition, thus resulting in different initial grain sizes and grain size distributions. The films were then ion beam sputtered in a sequence of different times to determine the evolution of the morphology and the role grain size plays in the morphological characteristics of ion beam sputtered thin films. Resulting morphology was then characaterised by FE-SEM imaging and by AFM, giving data on roughness, wavelength and underlaying grain size evolution. For comparison, commercially obtained gold films grown on mica, which had grain sizes in the order of a few hundred nm, were also included in the experiment.
Top/Technology/Nanotechnology/Nanomaterials/Nanostructures	Self-assembly of Nematic colloids Nematic colloids are dispersions of micrometric particles in nematic liquid crystals which show unusual properties of self-assembly into regular geometrical patterns. The reason for this type of behavior is the pressure of the liquid crystal among the particles. These pressures are called structures, and are the consequence of the orientation of the liquid crystals and the deformation of the structure of the crystal near the colloid particles. It has been known for quite a while that the nematic colloids are unifying into one dimensional chain. We also know about two dimensional structures on the border of liquid crystals and isotropic liquids. Latest research has shown progress in the understanding of the nature of self-assembly of nematic colloids; they have shown that nematic colloids can spontaneously make also regular two dimensional structures which are very tightly bounded. With these facts a new path has been laid for the self-assembly of colloids in 3 dimensional crystals. There are also interesting possibilities in the manufacturing of materials with the self arranging of nanoparticles.
Top/Technology/Nanotechnology/Nanomedicine	Formulation of PLGA nanoparticles for intracellular delivery of protein drug Design and formulation of advanced drug delivery systems (DDS), such as nanoscale carriers, presents an attractive research area in the field of drug formulation. A vast contribution is expected in delivery of biopharmaceuticals as is clearly recognized that inadequate delivery is the single most important factor delaying their application in clinical practise. In spite of some successful guidelines, formulation of protein drugs in DDS requires step-by step strategy and methods differing from those used for classical pharmaceutical drugs since proteins are the most delicate ones in term of retaining their biological function. A model protein drug cystatin was selected in our work, having high potential for inactivating cysteine proteases, enzymes involved in processes of tumour invasion and metastasis. Nanoparticles was used as carrier system with the aim to increase the bioavailability of the protein drug by protecting it from premature degradation in biological environment and faciliting its intracellular delivery. Cystatin was incorporated in poly(lactide-co-glycolide) (PLGA) nanoparticles by the water-in-oil-in-water emulsion solvent diffusion method. To preserve its biological activity an optimized technique was developed, adjusting physical and chemical parameters of processes during nanoparticle production. Cystatin-loaded NPs had size of 300-350 nm diameter, and contained 1.6 % (w/w) of cystatin, retaining 85% of its starting activity. To follow cellular uptake of nanoparticles, cystatin was labelled with fluorescent dye (Alexa Fluor 488) prior to its encapsulation into NPs. Image analysis showed rapid internalization of NPs into MCF-10A neoT cells as the fluorescence spots were detected after treatment with NPs. On the other hand, labelled free cystatin was internalised very slowly, suggesting that NPs facilitate the delivery of its cargo into the cells. Cystatin, delivered by NPs, also exerted its inhibitory activity on intracellular target cathepsin B, suggesting that its integrity was preserved throughout the processes of formulation and delivery. On the other hand, free cystatin did not impart proteolityic activity of cathepsin B, when tested under the same conditions using the substate, specific for intracellular cathepsin B. Our results show that protein drug can be formulated in the active form into PLGA NPs, when suitably selecting the process parameters of NP-production. NPs are also able to facilitate delivery of protein drug into the cells, enabling its activity on the intracellular target
Top/Technology/Nanotechnology/Molecular_nanotechnology/Molecular_self-assembly	Self-catalysed growth of GaAs nanowires by MBE Semiconductor nanowires (NWs) growth is typically assisted by a metal particle, called the catalyst. The use as the catalyst of a material different from the components of the wire may change the semiconductor properties due to the diffusion of the catalyst in the nanowire body during the growth. Moreover, the most commonly used catalyst is Au, a metal that is incompatible with the Si technology. For the above mentioned reasons it is therefore of importance to develop a technology leading to a catalyst-free growth of semiconductor nanowires. Here we report preliminary results on catalyst-free growth of GaAs NWs by molecular beam epitaxy. The GaAs NWs have been grown on cleaved edges of Si wafers, with no catalyst pre-deposition. The growth lasted 30 minutes and has been performed at 580 T 620 C. Two kinds of nanowires have been obtained. The NWs of the first type are as long as 5 m with a section diameter in the range of tens of nm and have a spherical particle at their end tip. Energy dispersive X-ray spectroscopy (EDS) demonstrates that the spherical particle is composed of Ga, and that the NW body is GaAs. The NWs density depends on the crystallographic orientation of the facets that compose the cleaved edges of the Si-wafers. The second type of NWs are generally characterised by a smaller aspect ratio, clearly faceted lateral and tip surfaces, and no metallic particle on their tip. EDS curves reveal that they are completely made of GaAs. The EDS results suggest that a Ga induced self-catalysed growth occurred on specific surface locations where Ga droplets formed and were trapped during the first stages of the GaAs deposition. Work is in progress to understand the growth process and in particular to understand whether the droplet-less NWs grow through a different process or the absence of a Ga droplet is due to its lost during growth.
Top/Technology/Nanotechnology/Molecular_nanotechnology/Molecular_self-assembly	MoSIx nanowires funtionalization for molecula-scale connectivity We report on a new highly reproducible route to recognitive self-assembly of molecular-scale circuits using sulfur-terminated subnanometer diameter Mo6S9-xIx (MoSIx) molecular nanowires. We demonstrate solution-processed attachment of MoSIx connecting leads to gold nanoparticles (GNPs). We also show that naked nanowires have the potential to bind thiolated proteins such as green fluorescent protein directly, thus providing a universal construct to which almost any protein could be attached. We further demonstrate three-terminal branched circuits with GNPs, opening a self-assembly route to multiscale complex molecular-scale architectures at the single-molecule level.
Top/Technology/Nanotechnology/Nanoelectronics	Developments of high performance n-type carbon nanotube field-effect transistors As scaling down with Moore's law, the modern silicon complementary metaloxidesemiconductor (CMOS) technology is facing great challenges and people are looking for alternatives. Carbon nanotube (CNT), due to its novel structure and properties, has been regarded as one of the most promising building blocks for the future integrated circuits. Since the first CNT fieldeffect transistor (CNTFET) was designed in 1998, device performance hasbeen continually improved. By using palladium (Pd) electrodes and highk materials (which are less prone to current leakage) as gate dielectrics, ptype CNTFETs have now surpassed the capabilities of stateoftheart silicon pMOSFETs. However, the development of ntype CNTFETs has lagged behind. This is mainly due to the difficulty of fabricating a Schottky barrierfree contact between metal electrodes and the conduction band of the CNT. The slow progress in producing nCNTFETs has greatly hindered the development of CNTbased integrated circuits. We Recently discovered that scandium (Sc) can be used to generate an ohmic contact with the conduction band of a CNT and high performance ntype CNTFETs can be easily fabricated. Based on this discovery, we proposed an CNTbased dopingfree CMOS technology and pushed the limits of n type CNTFETs. We also demonstrated a design of whole carbon nanotube circuits by integrating MultiWalled CNTs with the SingleWalled CNTFETs which serve as interconnects. All of our results show a prospective future of CNTbased integrated circuits. ----
Top/Technology/Nanotechnology/Nanoelectronics	Synthesis of metallic Ag and semiconducting ZnS nanoparticles in self -assembled polyelectrolyte templates Metal and semiconductor nanoparticles have attracted much interest lately due to their unique size dependant properties, stemming from their quantum confinement effects and large surface areas. The main problem in nanoparticle synthesis is their aggregation which often prohibits tailoring of particle size. One of the convenient methods to manipulate and process these nanoparticles in technologically useful formulations is an in-situ formation of nanoparticles in polymer matrices which prevents the aggregation of the nanoparticles and enables their uniform distribution. The polyelectrolyte multilayers (PEMs) of polyallylamine (PAH) and polyacrylic acid (PAA) were assembled on hydrophilic polystyrene tissue-cultured and surface modified quartz substrates by alternately dipping the substrates into polyanion and polycation aqueous solutions with various pH values until the desired thickness of the PEMs was obtained. The linear charge density or concentration of free carboxylic acid repeating units of PAA was controlled by adjusting the pH of the dipping PAA solutions. Thus formed PEMs were exposed to the metal ion solution at nominally neutral pH. By subsequent reduction of Ag ions or sulfidication of the Zn ions, metallic (Ag) or semiconductor nanoparticles (ZnS) are formed within the PEMs. Since upon nanoparticle formation carboxylic groups of PAA are regenerated, the synthesis methodology can be repeatedly cycled to incorporate more metal ions, which enables the formation of nanoparticulate films where the nanoparticle size and concentration can be manipulated. The aim of this work is to control the size and concentration of the in-situ formed inorganic nanoparticles by varying the synthesis conditions.
Top/Technology/Nanotechnology/Microscopy	Novel scanning microscope for visualization of individual emission sites on flat field emission cathodes We present first results obtained by a novel scanning projection field emission microscope (SPFEM) designed to study flat broad-area field emission cathodes. The instrument merges capabilities of measuring the electron field emission current from an individual emitting site and genuine projection of electrons onto a luminescent screen. This is achieved by an optimized shape of the anode probe having a 0.04 mm aperture which generates an uniform macroscopic electric field across the investigated area of the cathode. This fact also enables presentation of the relation between the current density and the applied electric field. The magnification of the electron-optical system alone was calculated by computational modeling for some cathode probe distances and for some voltages. The unique SPFEM performance is demonstrated on smooth sulfur doped nanodiamond films synthesized on molybdenum substrates.
Top/Technology/Nanotechnology/Microscopy	FT-IR microspectroscopy: a powerful tool for spatially resolved studies on supports for solid phase organic synthesis Solid Phase Organic Synthesis (SPOS) is an efficient technique for the synthesis of fine chemicals and for the development of compounds libraries through combinatorial approaches. SOPS performances can be optimized carefully tuning the chemo-physical properties of polymeric supports, usually porous beads, with particular reference to the distribution of the reaction products into the beads, which gives information on the pore accessibility, on the efficiency of the reactants diffusion process into pores and on the load capacity of the bead. Differently from optical transparent polymers, for which all parameters of interest can be quantified by two-photon microscopy, opaque supports are difficult to characterize by conventional analytical techniques. We propose a new method to systematically study parameters affecting performance of opaque supports for SPOS, based on FT-IR microspectroscopy on thin bead slides in transmission mode (All data collected with Bruker VERTEX 70 interferometer coupled with Hyperion 3000 IR microscope). Opaque amino-methacrylate beads with different pore diameters, Synbeads by Resindion s.r.l (Mitsubishi Chem. Corp., Milan, Italy), are acylated via chemical coupling with 3-nitropropionic acid at different reactions times. Functionalized and non-functionalized beads, cut in 5 microns slices, are chemically imaged with Focal Plane Array detector (FPA, 64X64 pixel) coupled with a IR conventional source, allowing a fast chemical imaging of the nitro functional group presence and distribution within the bead. In order to achieve a better S/N ratio and then more accurate details on nitro-group distribution, selected bead sections are also mapped at 5 microns spatial resolution along their diameter using a Mercury-Cadmium-Telluride (MCT) single-point detector operated with Synchrotron Radiation (SR). The combined approached proposed has the main advantage to be useful for each type of SPOS support material and allows, by FT-IR imaging, an easy and fast access to data as bead functionalization and functional group distribution. More accurate quantitative relations between bead polymer type, its degree of polymerization, bead pore dimension and mean porosity, supported reaction and synthesis condition can be achieved exploiting the high brightness of SR source and the major sensitivity of MCT detector.
Top/Technology/Nanotechnology/Microscopy	Analytical electron microscopy of nanoparticles Analytical Electron Microscopy (AEM) is an essential tool for microstructural investigations of nanostructured materials. Dedicated FEG instruments enable study of nanosized volumes using various methods, such as high resolution transmission electron microscopy (HRTEM), hig-resolution Z-contrast (STEM/HAADF) imaging, different techniques of electron diffraction (SAED, CBED, nanodiffraction), X-ray energy dispersive spectroscopy (XEDS) and electron energy loss spectroscopy (EELS). In the work the main stress will be on AEM study of particles, determination of their size, morphology, chemical and structural composition, orientation, etc. Study of the nucleation and crystallization of nanosized particles from amorphous phase, analysis of light elements in small volumes with complicated geometries, determination of atomic clusters of secondary element in monocrystals, investigation of self-assembly of quantum dots in an amorphous matrix are examples that will be presented and commented in the work. Determination of the crystallinity is in many cases quite complicated and ambiguous. The boundary between amorphous and crystalline phase in certain materials is very broad and smeared. Amount, crystallite size and shape and defectiveness of the crystal structure of nanoparticles (clusters, embryos, nuclei, precipitates, nanowires, nanorods, etc.) could be in usual cases determined using classical approaches (bright-field, dark-field experiments, selected area electron diffraction, high-resolution TEM). In specific cases some novel approaches should be used to extract the useful information from the sample. Such approaches are the use of chemical composition fluctuations of nano-regions in the determination of the presence of nanoclusters, the comparison of calculated and experimental electron diffraction pattern in the particle size and the degree of crystallinity determination and the use of non-standard geometries for absorption correction procedures during the chemical composition determination of particle using X-ray energy dispersive spectroscopy.
Top/Technology/Nanotechnology/Quantum_electronics	Magnetic impurity formation in quantum point contacts A quantum point contact (QPC), a narrow region separating two wider electron reservoirs, is the standard building block of sub-micron devices, such as quantum dots - small boxes of electrons, and qubits - the proposed basic elements of quantum computers. As a function of its width, the conductance through a QPC changes in integer steps of G0 = 2e2/h, signalling the quantization of its transverse modes. Such measurements also reveal an additional shoulder at a value around 0.7 G0 which has become known as the 0.7 anomaly. Recently it has been suggested that this phenomenon can be explained if one invokes the existence of a magnetic impurity in the QPC at low densities. Here we report on our extensive density functional calculations that reveal the formation of an electronic state with a spin-1/2 magnetic moment in the channel as the density increases above pinch-off, under very general conditions.
Top/Technology/Nanotechnology/Nanoelectronics/Nanowires	Developments of high performance n-type carbon nanotube field-effect transistors As scaling down with Moore's law, the modern silicon complementary metaloxidesemiconductor (CMOS) technology is facing great challenges and people are looking for alternatives. Carbon nanotube (CNT), due to its novel structure and properties, has been regarded as one of the most promising building blocks for the future integrated circuits. Since the first CNT fieldeffect transistor (CNTFET) was designed in 1998, device performance hasbeen continually improved. By using palladium (Pd) electrodes and highk materials (which are less prone to current leakage) as gate dielectrics, ptype CNTFETs have now surpassed the capabilities of stateoftheart silicon pMOSFETs. However, the development of ntype CNTFETs has lagged behind. This is mainly due to the difficulty of fabricating a Schottky barrierfree contact between metal electrodes and the conduction band of the CNT. The slow progress in producing nCNTFETs has greatly hindered the development of CNTbased integrated circuits. We Recently discovered that scandium (Sc) can be used to generate an ohmic contact with the conduction band of a CNT and high performance ntype CNTFETs can be easily fabricated. Based on this discovery, we proposed an CNTbased dopingfree CMOS technology and pushed the limits of n type CNTFETs. We also demonstrated a design of whole carbon nanotube circuits by integrating MultiWalled CNTs with the SingleWalled CNTFETs which serve as interconnects. All of our results show a prospective future of CNTbased integrated circuits. ----
Top/Technology/Nanotechnology/Nanoelectronics/Nanowires	Structural and electronic properties of molybdenum chalcohalide nanowires We combine ab initio density functional and quantum transport calculations based on the nonequilibrium Greens function formalism to compare structural, electronic, and transport properties of Mo6S6-xIx nanowires with carbon nanotubes. We find systems with x=2 to be particularly stable and rigid, with their electronic structure and conductance close to that of metallic (13,13) single-wall carbon nanotubes. Mo6S6-xIx nanowires are conductive irrespective of their structure, more easily separable than carbon nanotubes, and capable of forming ideal contact to Au leads through thio groups.
Top/Technology/Nanotechnology/Nanoelectronics/Nanowires	Nanowires and Nanocrystals for Nanotechnology Nanowires and nanocrystals represent important nanomaterials with one-dimensional and zero-dimensional morphology, respectively. Here I will give an overview on the research about how these nanomaterials impact the critical applications in faster transistors, smaller nonvolatile memory devices, efficient solar energy conversion, high-energy battery and nanobiotechnology
Top/Technology/Nanotechnology/Nanoelectronics/Nanowires	Self-catalysed growth of GaAs nanowires by MBE Semiconductor nanowires (NWs) growth is typically assisted by a metal particle, called the catalyst. The use as the catalyst of a material different from the components of the wire may change the semiconductor properties due to the diffusion of the catalyst in the nanowire body during the growth. Moreover, the most commonly used catalyst is Au, a metal that is incompatible with the Si technology. For the above mentioned reasons it is therefore of importance to develop a technology leading to a catalyst-free growth of semiconductor nanowires. Here we report preliminary results on catalyst-free growth of GaAs NWs by molecular beam epitaxy. The GaAs NWs have been grown on cleaved edges of Si wafers, with no catalyst pre-deposition. The growth lasted 30 minutes and has been performed at 580 T 620 C. Two kinds of nanowires have been obtained. The NWs of the first type are as long as 5 m with a section diameter in the range of tens of nm and have a spherical particle at their end tip. Energy dispersive X-ray spectroscopy (EDS) demonstrates that the spherical particle is composed of Ga, and that the NW body is GaAs. The NWs density depends on the crystallographic orientation of the facets that compose the cleaved edges of the Si-wafers. The second type of NWs are generally characterised by a smaller aspect ratio, clearly faceted lateral and tip surfaces, and no metallic particle on their tip. EDS curves reveal that they are completely made of GaAs. The EDS results suggest that a Ga induced self-catalysed growth occurred on specific surface locations where Ga droplets formed and were trapped during the first stages of the GaAs deposition. Work is in progress to understand the growth process and in particular to understand whether the droplet-less NWs grow through a different process or the absence of a Ga droplet is due to its lost during growth.
Top/Technology/Nanotechnology/Nanoelectronics/Nanocircuitry	MoSIx nanowires funtionalization for molecula-scale connectivity We report on a new highly reproducible route to recognitive self-assembly of molecular-scale circuits using sulfur-terminated subnanometer diameter Mo6S9-xIx (MoSIx) molecular nanowires. We demonstrate solution-processed attachment of MoSIx connecting leads to gold nanoparticles (GNPs). We also show that naked nanowires have the potential to bind thiolated proteins such as green fluorescent protein directly, thus providing a universal construct to which almost any protein could be attached. We further demonstrate three-terminal branched circuits with GNPs, opening a self-assembly route to multiscale complex molecular-scale architectures at the single-molecule level.
Top/Technology/Nanotechnology/Nanomaterials/Carbon_nanotubes	Developments of high performance n-type carbon nanotube field-effect transistors As scaling down with Moore's law, the modern silicon complementary metaloxidesemiconductor (CMOS) technology is facing great challenges and people are looking for alternatives. Carbon nanotube (CNT), due to its novel structure and properties, has been regarded as one of the most promising building blocks for the future integrated circuits. Since the first CNT fieldeffect transistor (CNTFET) was designed in 1998, device performance hasbeen continually improved. By using palladium (Pd) electrodes and highk materials (which are less prone to current leakage) as gate dielectrics, ptype CNTFETs have now surpassed the capabilities of stateoftheart silicon pMOSFETs. However, the development of ntype CNTFETs has lagged behind. This is mainly due to the difficulty of fabricating a Schottky barrierfree contact between metal electrodes and the conduction band of the CNT. The slow progress in producing nCNTFETs has greatly hindered the development of CNTbased integrated circuits. We Recently discovered that scandium (Sc) can be used to generate an ohmic contact with the conduction band of a CNT and high performance ntype CNTFETs can be easily fabricated. Based on this discovery, we proposed an CNTbased dopingfree CMOS technology and pushed the limits of n type CNTFETs. We also demonstrated a design of whole carbon nanotube circuits by integrating MultiWalled CNTs with the SingleWalled CNTFETs which serve as interconnects. All of our results show a prospective future of CNTbased integrated circuits. ----
Top/Technology/Nanotechnology/Nanomaterials/Carbon_nanotubes	Carbon nanotubes added hexagonal WO3 films A world-wide concern for enviromental safety that demands monitoring the emission of hazardous gases into the atmosphere, combined with recent advances in wireless senzor networks is increasing the need of low-power gas sensors and low-cost. Among metal oxides, between theirselves the tungsten oxides are among the most used materials in electro-, photo- and gasochromic applications. In this work, a soft chemical nanocrystalline processing route has been demonstrated for the preparation of hexagonal tungsten oxides by the acidic precipitation of Na2WO4.2H2O solution at temperatures as low as 120 C and 330 C. The structural properties of films were investigated by TEM Philips CM-20. The sensing properties of films were measured to gaseous ammonia at various temperatures. We founded the correlation between structure and gas sensing properties of WO3 films. The average size of WO3.1/3 H2O crystallites is ~ 50 100 nm. After the head treatment of films, the average size of WO3 crystallites decreased to ~ 30 50 nm. The electron diffraction of the film confirmed the phase change from orthorhombic to hexagonal one. The gas measurements were performed by direct injection using a gas serynge, and the arrows in the graphics indicate the total amount of gas present in the measurement chamber after successive injections.
Top/Technology/Nanotechnology/Nanomaterials/Carbon_nanotubes	Carbon nanotubes wrapped by DNA molecules Complexes of carbon nanotubes (CNTs) and nucleic acids allow fully exploit the potential of the CNTs in nanoelectronic devices, both by a size-specific matching of the two components and by the possibility to anchor also non-polar CNTs on the polar substrates such as oxides. The wrapping CNTs by the nucleic acid molecules allow also a transfer of CNTs into water solutions and a performance for their radii and lengths separation using chromatographical methods. In the present work for the first time the stability and electronic properties of the associates of the single-walled carbon nanotubes wrapped by homopolymeric single-stranded DNA molecules (CNT@DNA) are studied using a dispersion corrected modification of quantum mechanical density-functional tight-binding method (DFTB). A phenomenological model of the CNT@DNA formation energy depending on the nanotube radii is developed, which shows that the decoration of a CNT by a few (not single) DNA chains leads to a high water solubility of CNT@DNA. Pyrimidine-based DNAs are found to be more effective to wrap the CNTs, whereas purine-based DNAs are in wrapping more sensitive to the change of radii. The densities-of-states of the CNT@DNA complexes are close to the superposition of those of the free components with some additional states below Fermi level. The band gap in a hybrid CNT@DNA system is determined by the competition between the Fermi levels of the free DNA and CNT. In a few specific cases (complexes of polycytosine-DNA and a chiral metallic CNT) a considerable charge transfer from the DNA to the CNT was observed, combined with an additional gain in the CNT@DNA formation energy.
Top/Technology/Nanotechnology/Nanomaterials/Carbon_nanotubes	Organic Functionalization of Carbon Nanotubes
Top/Technology/Nanotechnology/Nanomaterials/Nanoparticles	Synthesis of metallic Ag and semiconducting ZnS nanoparticles in self -assembled polyelectrolyte templates Metal and semiconductor nanoparticles have attracted much interest lately due to their unique size dependant properties, stemming from their quantum confinement effects and large surface areas. The main problem in nanoparticle synthesis is their aggregation which often prohibits tailoring of particle size. One of the convenient methods to manipulate and process these nanoparticles in technologically useful formulations is an in-situ formation of nanoparticles in polymer matrices which prevents the aggregation of the nanoparticles and enables their uniform distribution. The polyelectrolyte multilayers (PEMs) of polyallylamine (PAH) and polyacrylic acid (PAA) were assembled on hydrophilic polystyrene tissue-cultured and surface modified quartz substrates by alternately dipping the substrates into polyanion and polycation aqueous solutions with various pH values until the desired thickness of the PEMs was obtained. The linear charge density or concentration of free carboxylic acid repeating units of PAA was controlled by adjusting the pH of the dipping PAA solutions. Thus formed PEMs were exposed to the metal ion solution at nominally neutral pH. By subsequent reduction of Ag ions or sulfidication of the Zn ions, metallic (Ag) or semiconductor nanoparticles (ZnS) are formed within the PEMs. Since upon nanoparticle formation carboxylic groups of PAA are regenerated, the synthesis methodology can be repeatedly cycled to incorporate more metal ions, which enables the formation of nanoparticulate films where the nanoparticle size and concentration can be manipulated. The aim of this work is to control the size and concentration of the in-situ formed inorganic nanoparticles by varying the synthesis conditions.
Top/Technology/Nanotechnology/Applications	Novel routes to nano-materials for Li-ion batteries Advanced Li-ion batteries providing enhanced storage capacities and improved power performances are currently required not only by the fast-growing market of portable electronics, but also by emerging electric or hybrid-electric vehicles. We are investigating two novel techniques for this purpose: Spark Discharge Generation (SDG) and Electrostatic Spray Reductive Precipitation (ESRP). SDG uses a physical top down approach that relies on the atomization of two metal rods via a sudden spark. Two cylindrical rods are connected to high voltage and parallel to a variable capacitance. The capacitors are periodically charged to the break-down voltage of the system determined by the gap between the rods. Through the high temperature of the generated spark, electrode material is rapidly evaporated, and the vapour condenses to form nano-sized metallic particles. In addition, an unconventional densification technique, called Magnetic Pulse Compaction (MPC), is being used for self-manufacturing metal or alloy rods to be atomized. ESRP is a physical-chemical technique relying on a combined top-down and bottom-up approach, which bridges aerosol generation with chemical precipitation in order to form nanoparticles. Electro-Spraying of liquids consists in the creation of charged aerosols by applying a high voltage between a nozzle, through which the liquid to be sprayed is fed, and a counter-electrode. Interesting properties of this phenomenon are the narrow size distribution of the emitted droplets, as well as tuning the droplet size by controlling experimental parameters. Moreover, high net surface charge on the generated droplets causes repulsive interaction, preventing droplet coalescence. These beneficial aspects have been exploited in combination with a well-known technique for the synthesis of metallic and alloyed particles, namely Reductive Precipitation (RP) of metal chlorides by NaBH4. Dissolved metal chlorides precursors are forced to flow by a syringe pump which provides a constant supply to the nozzle with a controlled flow rate. Under the influence of the high electric field small charged droplets are formed and attracted towards a ring-shaped counter electrode, which is placed in the reductive solution. In this way, the droplets containing precursor metal ions are driven into the reductive bath, where they are immediately reduced to their zero-valent state. Primary particles with size in the range of 2-5 nm can be obtained by proper selection of the experimental parameters.
Top/Technology/Nanotechnology/Applications	Photosensitive titanium oxide sols and gels for solar energy conversion and storage Due to the potential applications in the field of environmental protection, the photochemistry of TiO,,2,, is a fast growing area both in terms of research and commercial activity. Beside to the white pigment properties of rutile and anatase (e.g. paints and cosmetic products), titanium dioxide is used in heterogeneous catalysis and photocatalysis (water purification, air cleaning), in photoelectrochemical solar cells for the production of hydrogen and electricity, as an active layer in the design of electrochromic devices, as a gas sensor, as a corrosion-protective coating, in ceramics and in electric devices such as varistors, to name few. In such applications, the performance of titanium oxide could be optimized with specific nanostructural control over the morphology of the material. Under UV irradiation, an electron-hole pair is generated in TiO,,2,, then technological devices are based on chemical reactions or induced electron transfers. Due to the large band gap of TiO,,2,, (3.2eV), only 10% of the solar spectrum is used. A major objective for future work is the development of a semiconductor photocatalyst film which is able to utilize visible as well as UV light. Nanoscience has the potential to provide entirely new classes of materials with capabilities that transcend these limitations and generate the performance breakthroughs required for a viable economy based on sustainable energy. Recent investigations in this area allowed us to synthesize novel photo-sensitive titanium oxide sols and gels by controlling the condensation of titanium species in non aqueous solvents. Depending on different parameters such as concentration in Ti^^4+^^, ageing or thermal treatment, to emphasise few, the structuration of the inorganic framework leads to various layered structure. The adsorbed organic species control the growth of the nano-objects present in the sol or gel. Due to the enhanced surface area to volume ratio, these nanostructured sols and gels produce singular photo-electrochemical properties that are drastically different from their bulk counterparts. When irradiated, these materials can absorb photons with a lower energy than the bandgap energy of the original semiconductor, and thus a significant increase in the limiting efficiency of conventional solar cells is expected. These new materials are principally characterized by a partially occupied intermediate band, isolated from the valence and conduction bands. Our purpose is to use these intermediate band materials as sensitizers in both third generation photoelectrochemical solar cells and photo-batteries (ultracapacitors).
Top/Technology/Nanotechnology/Microscopy/Scanning_tunneling_microscopy	STM Manipulation of Atoms and Molecules Novel quantum structures can be realized by manipulating surface single atoms and molecules. Recent efforts in manipulating these basic construction species by means of a low-temperature ultra high vacuum scanning tunneling microscope, combined with a variety of tunneling spectroscopic methods, will be presented. The performed experiments are important for both, fundamental understanding and construction of novel nano-devices. Described will be i.a. measurements of lateral forces required to move individual atoms, realization of a multi-step single molecule switch, and an hybrid device, composed of atoms and molecules.
Top/Technology/Nanotechnology/Nanomaterials/Quantum_dots	Magnetic impurity formation in quantum point contacts A quantum point contact (QPC), a narrow region separating two wider electron reservoirs, is the standard building block of sub-micron devices, such as quantum dots - small boxes of electrons, and qubits - the proposed basic elements of quantum computers. As a function of its width, the conductance through a QPC changes in integer steps of G0 = 2e2/h, signalling the quantization of its transverse modes. Such measurements also reveal an additional shoulder at a value around 0.7 G0 which has become known as the 0.7 anomaly. Recently it has been suggested that this phenomenon can be explained if one invokes the existence of a magnetic impurity in the QPC at low densities. Here we report on our extensive density functional calculations that reveal the formation of an electronic state with a spin-1/2 magnetic moment in the channel as the density increases above pinch-off, under very general conditions.
Top/Environment/Global_Warming	The American Denial of Global Warming Polls show that between one-third and one-half of Americans still believe that there is 'no solid' evidence of global warming, or that if warming is happening it can be attributed to natural variability. Others believe that scientists are still debating the point. Join scientist and renowned historian Naomi Oreskes as she describes her investigation into the reasons for such widespread mistrust and misunderstanding of scientific consensus and probes the history of organized campaigns designed to create public doubt and confusion about science.
Top/Environment/Global_Warming	In Antarctica: The Global Warning Antarctica's environment today is a microcosm of the world environment's future: as endangered creatures, such as the chinstrap penguins, humpback whales, and albatrosses, continue to face extinction, research scientists have concluded that this icy ecosystem serves as a final warning of impending environmental deterioration. In Antarctica: The Global Warning, Sebastian Copeland's photographs have captured both the incredible beauty of the continent and the devastation that climate changes have wreaked on it. His data, photographs, and conclusions along with contributions from Will Steger, David De Rothschild, Stephen Schneider, Zac Goldsmith, Mikhail Gorbachev, and Leonardo DiCaprio bring to readers with insight and urgency the momentous reality of the not so distant future with insight and urgency.
Top/Environment/Global_Warming	Global Warming: Nation Under Siege The rapid depletion of fossil fuels and the rising sea level from the warming of the earth's atmosphere are converging to dramatically alter our future. Edward Mazria, founder of Architecture 2030, unveils a new study of sea level rise showing fly-over 3D images depicting potentially calamitous coastal and national impacts.
Top/Environment/Climate_Change	Redefining the relationship of development and climate change
Top/Society/Social_Networks	Battling Networks of Rival Social Movements
Top/Society/Social_Networks	Social Networks from the Perspective of Physics 'In the history of public speaking, there have been many famous denials. One sunny day in 1880, Karl Marx declared: 'I am not a Marxist'. On a less auspicious occasion in 1973, Richard Nixon insisted 'I am not a crook'. Neither Marx nor Nixons audience gave much credence to their denials, and you too may respond with disbelief when I tell you that 'I am not a networker'. (Marc Granovetter, Connections, 1990) ... 'Instead, the slogan of the day will be 'We are all networkers now'.
Top/Society/Social_Networks	Dynamics of Information and Evaluation on Social Networks The lecture will concern dynamics of two different processes occurring in social networks: the flow of information and the process of evaluation. Similarities and differences in how networks structure shapes the spread of information and governs social influence in the process of evaluation will be discussed. Research in social psychology suggests that information is not only acquired but also evaluated and interpreted in the process of interaction, as individuals construct common social representation. Both simulation and empirical data show that transmission of information and evaluations operate in a very different ways. Empirical data concerning the structure of selected social networks and dynamics of information on the networks will be presented.
Top/Society/Social_Networks	Social Networks and Ideological Movements in History: Burning and the Rise of English Protestantism There is a historical consensus that at the beginning of the reign of the Catholic Queen Mary (1553-58), the Protestant reforms instituted by Henry VIII in the 1530s and continued under Edward VI (1547-53) had engaged the support of only a tiny minority of the population. The restoration of Catholicism met with widespread approval. But a mere six years later the re-introduction of Protestantism on the Edwardian model by Elizabeth I in 1559 met with virtually no protest. A good historical case can be made that the persecution and burning of high-profile Protestants by Mary was an important factor in reversing public opinion. A network approach, in which society is envisaged as a scale-free network with each individual influenced in their religious beliefs by a small number of others to whom they pay attention yields the result that the burnings may well have been the decisive factor. The highly connected individuals here are of course the Protestant martyrs. England in 1559 had not become a nation of Protestant zealots, but sufficient people were impressed by the martyrs demeanour to acquiesce in the new faith. This analysis is supported by contemporary evidence that Protestant leaders under Mary stressed that executions were opportunities to display public fortitude and piety to influence people.
Top/Society/Social_Networks	The Evolution of Cooperation: Simple Games and Complex Societies
Top/Computer_Science/Web_Mining/Link_Analysis	Link Analysis and Text Mining : Current State of the Art and Applications for Counter Terrorism The information age has made it easy to store large amounts of data.The proliferation of documents available on the Web, on corporate intranets, on news wires, and elsewhere is overwhelming. However, while the amount of data available to us is constantly increasing, our ability to absorb and process this information remains constant. Search engines only exacerbate the problem by making more and more documents available in a matter of a few key strokes. Link Analysis is a new and exciting research area that tries to solve the information overload problem by using techniques from data mining, machine learning, Information Extraction, Text Categorization, Visualization and Knowledge Management.
Top/Computer_Science/Web_Mining/Link_Analysis	Theoretical analysis of Link Analysis Ranking
Top/Computer_Science/Web_Mining/Link_Analysis	Text Mining and Link Analysis for Web and Semantic Web The tutorial on Text Mining and Link Analysis for Web Data will focus on two main analytical approaches when analyzing web data: text mining and link analysis for the purpose of analyzing web documents and their linkage. First, the tutorial will cover some basic steps and problems when dealing with the textual and network (graph) data showing what is possible to achieve without very sophisticated technology. The idea of this first part is to present the nature of un-structured and semi-structured data. Next, in the second part, more sophisticated methods for solving more difficult and challenging problems will be shown. In the last part, some of the current open research issues will be presented and some practical pointers on the available tolls for solving previously mentioned problems will be provided.
Top/Computer_Science/Web_Mining/Link_Analysis	Link analysis with pajek Pajek is a program (for Windows) for large network analysis and visualization. It is freely available for noncommercial use at [[http://vlado.fmf.uni-lj.si/pub/networks/pajek/|http://vlado.fmf.uni-lj.si/pub/networks/pajek/]] Besides ordinary networks Pajek supports also multi-relational and temporal networks. In large network analysis we are often interested in important parts of given network. There are several ways how to determine them. The islands approach is based on an importance measure of vertices or lines. Let (V,L,p) be a network with vertex property p : V ? R and let t be a real number. If we delete all vertices (and corresponding links) with the property value less than t, we get subnetwork called vertex-cut at level t. The number and sizes of its components depend on t. Often we consider only components of size at least k and not exceeding K. The components of size smaller than k are discarded as noninteresting, while the components of size larger than K are cut again at some higher level. Vertex-island is a connected subnetwork which vertices have greater property value than the vertices in its neighborhood. It is easy to see that the components of vertex-cuts are all vertex-islands. We developed an efficient algorithm that identifies all maximal vertex-islands of sizes in the interval k..K in a given network. For networks with weighted lines we can similarly define line-islands. The line-islands algorithm is based on line-cuts. Both algorithms are very general - they can be applied for any vertex/line importance measure. Their complexity is for sparse networks subquadratic - they can be applied to very large networks. We will illustrate them applying different importance measures on selected (large) networks. We will also present the use of pattern searching in analysis of genealogies and some approaches to analysis of (multi-relational) temporal networks.
Top/Computer_Science/Web_Mining/Link_Analysis	Generating Social Network Features for Link-based Classification
Top/Computer_Science/Network_Analysis/Social_Networks	Battling Networks of Rival Social Movements
Top/Computer_Science/Network_Analysis/Social_Networks	A Framework For Community Identification in Dynamic Social Networks We propose frameworks and algorithms for identifying communities in social networks that change over time. Communities are intuitively characterized as unusually densely knit subsets of a social network. This notion becomes more problematic if the social interactions change over time. Aggregating social networks over time can radically misrepresent the existing and changing community structure. Instead, we propose an optimization-based approach for modeling dynamic community structure. We prove that finding the most explanatory community structure is NP-hard and APX-hard, and propose algorithms based on dynamic programming, exhaustive search, maximum matching, and greedy heuristics. We demonstrate empirically that the heuristics trace developments of community structure accurately for several synthetic and real-world examples.
Top/Computer_Science/Network_Analysis/Social_Networks	iLink: Search and Routing in Social Networks - Part 1 The growth of Web 2.0 and fundamental theoretical breakthroughs have led to an avalanche of interest in social networks. This paper focuses on the problem of modeling how social networks accomplish tasks through peer production style collaboration. We propose a general interaction model for the underlying social networks and then a specific model (iLink) for social search and message routing. A key contribution here is the development of a general learning framework for making such online peer production systems work at scale. The iLink model has been used to develop a system for FAQ generation in a social network (FAQtory), and experience with its application in the context of a full-scale learning-driven workflow application (CALO) is reported. We also discuss methods of adapting iLink technology for use in military knowledge sharing portals and other message routing systems. Finally, the paper shows the connection of iLink to SQM, a theoretical model for social search that is a generalization of Markov Decision Processes and the popular Pagerank model.
Top/Computer_Science/Network_Analysis/Social_Networks	A Social Network Based Approach to Personalized Recommendation of Participatory Media Content
Top/Computer_Science/Network_Analysis/Social_Networks	iLink: Search and Routing in Social Networks - Part 2 The growth of Web 2.0 and fundamental theoretical breakthroughs have led to an avalanche of interest in social networks. This paper focuses on the problem of modeling how social networks accomplish tasks through peer production style collaboration. We propose a general interaction model for the underlying social networks and then a specific model (iLink) for social search and message routing. A key contribution here is the development of a general learning framework for making such online peer production systems work at scale. The iLink model has been used to develop a system for FAQ generation in a social network (FAQtory), and experience with its application in the context of a fullscale learning-driven workflow application (CALO) is reported. We also discuss methods of adapting iLink technology for use in military knowledge sharing portals and other message routing systems. Finally, the paper shows the connection of iLink to SQM, a theoretical model for social search that is a generalization of Markov Decision Processes and the popular Pagerank model.
Top/Computer_Science/Network_Analysis/Social_Networks	Social Networks from the Perspective of Physics 'In the history of public speaking, there have been many famous denials. One sunny day in 1880, Karl Marx declared: 'I am not a Marxist'. On a less auspicious occasion in 1973, Richard Nixon insisted 'I am not a crook'. Neither Marx nor Nixons audience gave much credence to their denials, and you too may respond with disbelief when I tell you that 'I am not a networker'. (Marc Granovetter, Connections, 1990) ... 'Instead, the slogan of the day will be 'We are all networkers now'.
Top/Computer_Science/Network_Analysis/Social_Networks	Influence and Correlation in Social Networks In many online social systems, social ties between users play an important role in dictating users' behavior. One of the ways this can happen is through social influence, the phenomenon that the actions of a user can induce his/her friends to behave in a similar way. In systems where social influence exists, ideas, modes of behavior, or new technologies can diffuse through the network like an epidemic. Therefore, identifying and understanding social influence is of tremendous interest from both an analysis (e.g., predicting the future of the system) and a design (e.g., designing viral marketing strategies) point of view. In this talk, I will give a general overview of models for diffusion in social network, and then discuss the problem of identifying social influence in the data. This is a difficult task in general, since there are many other factors such as homophily or unobserved confounding variables that can induce statistical correlation between the actions of friends in a social network. Thus, distinguishing influence from those other factors is essentially the problem of distinguishing correlation from causality, a notoriously hard problem. Despite this, I will show how in an environment where the time stamp of the actions are observable, we can design simple statistical tests that distinguish between models of social influence and those that replicate the aforementioned sources of social correlation. I will sketch the proof of a theoretical justification of one of the tests, and present simulation results on randomly generated data and real tagging data from Flickr. The results exhibit that while there is significant social correlation in tagging behavior on this system, this correlation cannot be attributed to social influence.
Top/Computer_Science/Network_Analysis/Social_Networks	A Large-Scale Study of MySpace: Observations and Implications for Online Social Networks
Top/Computer_Science/Network_Analysis/Social_Networks	Directed, Overlapping Clusters in Social Networks Recently an efficient search technique locating communities or networkmodules (densely connected groups of nodes) was introduced fordirected networks (Palla et al: New Journal of Physics 9, 186 (2007)). Here we investigate the centrality properties of directed module membersin social networks obtained from e-mail exchanges and fromsociometric questionnaires.Our results indicate that nodes in the overlaps betweenmodules play a central role in the studied systems. Furthermore,the two different types of networks show interesting differencesin the relation between the centrality measures and therole of the nodes in the directed modules.
Top/Computer_Science/Network_Analysis/Social_Networks	Dynamics of Information and Evaluation on Social Networks The lecture will concern dynamics of two different processes occurring in social networks: the flow of information and the process of evaluation. Similarities and differences in how networks structure shapes the spread of information and governs social influence in the process of evaluation will be discussed. Research in social psychology suggests that information is not only acquired but also evaluated and interpreted in the process of interaction, as individuals construct common social representation. Both simulation and empirical data show that transmission of information and evaluations operate in a very different ways. Empirical data concerning the structure of selected social networks and dynamics of information on the networks will be presented.
Top/Computer_Science/Network_Analysis/Social_Networks	Influence and Correlation in Social Networks
Top/Computer_Science/Network_Analysis/Social_Networks	Modeling social networks on large scale Recent development in information and communication technology has enabled to study networks of social interactions of unprecedented size. Such systems include email or phone networks and e-communities. In contrast to the traditional, questionnaire-based investigations, in these cases a natural quantitative measure of the strength of the interactions is present (like the frequency or duration of calls) leading to weighted network representations. One important observation is that this strength of the interactions varies over many orders of magnitude. A natural conclusion is that the weights play important roles both in the evolution of the networks and in the dynamics of the processes on them. Based on simple rules borrowed from sociology we construct a model, where the emergence of the community structure is a consequence of the interplay between topology and weights. We show that the model reflects well the observations made on a huge call network. References: [1] J.M. Kumpula, J.-P. Onnela, J. Saramki, K. Kaski, J. Kertsz: Emergence of communities in weighted networks, Phys. Rev. Lett. 99, 228701 (2007) [2] J.-P. Onnela, J. Saramki, J. Hyvonen, G. Szab, D. Lazer, K. Kaski, J. Kertsz, A.-L. Barabsi: Structure and tie strengths in mobile communication networks, PNAS 104, 7332-7336 (2007) [3] J.-P. Onnela, J. Saramki, J. Hyvonen, G. Szab, M. Argollo de Menezes, K. Kaski, A.-L. Barabsi, J. Kertsz: Analysis of a large-scale weighted network of one-to-one human communication, New J. Phys. 9, 179 (2007)
Top/Computer_Science/Network_Analysis/Social_Networks	On Social networks with an overview of graph drawing with demo of a system Pajek Network = Graph + Data. The data can be measured or computed/derived from the network. The graph drawing is already well established field with its own conference http://www.gd2005.org/ (started in 1992). In traditional graph drawing the goal is to produce the best layout of given graph. SNA (Social Network Analysis) is a part of data analysis. Its goal is to get insight into the structure and characteristics of given network.
Top/Computer_Science/Network_Analysis/Social_Networks	Exploring Semantic Social Networks using Virtual Reality We present Redgraph, the first generic virtual reality visualization program for Semantic Web data. Redgraph is capable of handling large data-sets, as we demonstrate on social network data from the U.S. Patent Trade Office. We develop a Semantic Web vocabulary of virtual reality terms compatible with GraphXML to map graph visualization into the Semantic Web itself. Our approach to visualizing Semantic Web data takes advantage of user-interaction in an immersive environment to bypass a number of difficult issues in 3-dimensional graph visualization layout by relying on users themselves to interactively extrude the nodes and links of a 2-dimensional graph into the third dimension. When users touch nodes in the virtual reality environment, they retrieve data formatted according to the datas schema or ontology. We applied Redgraph to social network data constructed from patents, inventors, and institutions from the United States Patent and Trademark Office in order to explore networks of innovation in computing. Using this data-set, results of a user study comparing extrusion (3-D) vs. no-extrusion (2-D) are presented. The study showed the use of a 3-D interface by subjects led to significant improvement on answering of fine-grained questions about the data-set, but no significant difference was found for broad questions about the overall structure of the data. Furthermore, inference can be used to improve the visualization, as demonstrated with a data-set of biotechnology patents and researchers.
Top/Computer_Science/Network_Analysis/Social_Networks	Scalable Collaborative Filtering Algorithms for Mining Social Networks Social networking sites such as Orkut, MySpace, Hi5, and Facebook attract billions of visits a day, surpassing the page views of Web Search. These social networking sites provide applications for individuals to establish communities, to upload and share documents/photos/videos, and to interact with other users. Take Orkut as an example. Orkut hosts millions of communities, with hundreds of communities created and tens of thousands of blogs/photos uploaded each hour. To assist users to find relevant information, it is essential to provide effective collaborative filtering tools to perform recommendations such as friend, community, and ads matching. In this talk, I will first describe both computational and storage challenges to traditional collaborative filtering algorithms brought by aforementioned information explosion. To deal with huge social graphs that expand continuously, an effective algorithm should be designed to 1) run on thousands of parallel machines for sharing storage and speeding up computation, 2) perform incremental retraining and updates for attaining online performance, and 3) fuse information of multiple sources for alleviating information sparseness. In the second part of the talk, I will present algorithms we recently developed including parallel Spectral Clustering [1], parallel PF-Growth [2], parallel combinational collaborative filtering [3], parallel LDA, parallel spectral clustering, and parallel Support Vector Machines [4].
Top/Computer_Science/Network_Analysis/Social_Networks	Topographic Analysis of an Empirical Human Sexual Network Spreading of electronic viruses, among computers and mobile phones, typically depends on address/phone number lists. The network formed by these lists is not symmetric: the fact that A has Bs address does not ensure that B has As address. Thus the underlying network on which such spreading takes place is directed: the links are in general one-way. We present an extension of our analysis for spreading on undirected graphs, to the case of directed graphs. We find that some ideas from Web link analysis lead us to a concrete prediction: that the epidemic coverage changes qualitatively when the rate of infections from outside the network exceeds a threshold rate. Specifically, for low rate of infections from outside, with high probability, only the giant component and its out-components are infected; while for above-threshold infection rate from outside, the whole graph is likely infected.
Top/Computer_Science/Network_Analysis/Social_Networks	Socioeconomic Networks with Long-range Interactions In well networked communities, information is often shared informally among an individual's direct and indirect acquaintances. Here we study a model previously proposed by Jackson and Wolinsky to account for communicating information and allocating goods in socioeconomic networks. The model defines a utility function of node i which is a weighted sum of contributions from all nodes accessible from i. First, we show that scale-free networks are more efficient than Poisson networks for the range of average degree typically found in real world networks. We then study an evolving network mechanism where new nodes attach to existing ones preferentially by utility. We found the presence of three regimes: scale-free (rich-get-richer), fit-get-rich, and Poisson degree distribution. The fit-get-rich regime is characterized by a decrease in average path length.
Top/Computer_Science/Network_Analysis/Social_Networks	Using Social Network Analysis, Geotemporal Reasoning and RDFS++ Reasoning for Business Intelligence Most of the attention in the Semantic Web world is currently focused on using ontologies, rdfs and owl reasoning to get more value out of enterprise data. Many enterprise databases are full of information about people, companies, relationships between people and companies, places and events. The Semantic Web literature also carries the promise of analyzing networks of people, networks of companies and events in time and space. This talk will show how Business Intelligence problems can be solved with a combination of basic semantic web reasoning and complementary techniques such as social network analysis and geotemporal reasoning. We will be using AllegroGraph in this talk, but the concepts learned will transfer to other Semantic Web solutions.
Top/Computer_Science/Network_Analysis/Social_Networks	Research 1: Extracting Relations in Social Networks from Web using Similarity between Collective Contexts
Top/Computer_Science/Network_Analysis/Social_Networks	The Structure of Information Pathways in a Social Communication Network
Top/Computer_Science/Network_Analysis/Social_Networks	Social dimension of social media
Top/Computer_Science/Network_Analysis/Social_Networks	Challenges in Social Network Data: Processes, Privacy and Paradoxes The proliferation of rich social media, on-line communities, and collectively produced knowledge resources has accelerated the convergence of technological and social networks, producing environments that reflect both the architecture of the underlying information systems and the social structure on their members. In studying the consequences of these developments, we are faced with the opportunity to analyze social network data at unprecedented levels of scale and temporal resolution; this has led to a growing body of research at the intersection of the computing and social sciences. **Do you have a question for this lecturer&#160;at the KDD 2007?&nbsp; We encourage you to start a debate, comment on each lecturers video or send us an email and we will ask them for you! ** //**Disclamer:**// //Videolectures.Net emphasises that the quality of this video was notably improved, because of low light quality conditions provided in the lecture auditorium. //
Top/Computer_Science/Network_Analysis/Social_Networks	Microscopic Evolution of Social Networks
Top/Physics/Particle_Physics	Worrying About the LHC, a Lesson from Astrophysics? To worry about the LHC is a popular sport. I shall share my own worries, hopefully original, and do it via a parable (for this method, I can quote earlier authors). The parable concerns a topic in astrophysics (gamma-ray bursts) which happens to be a simple exercise --but quite an interesting one-- on elementary particle-physics and beam dynamics, topics not unrelated to the LHC. Though most of the talk will be dedicated to the physics and, in particular, to its recent developments, the allegory will allow me to detect what, I shall argue, may be dangerous 'viruses' invading science. I do not have the decisive antidotes, but I shall discuss some possible ones.
Top/Physics/Particle_Physics	Fermilab Plan with a High Intensity Proton Source Fermilab, the USs primary laboratory for particle physics, proposes a plan to maintain leadership for the laboratory and U.S. particle physics in the quest to discover the fundamental nature of the physical universe in the decades ahead. Discoveries of the physics of the Quantum Universe would come from powerful next generation particle accelerators. Fermilabs Tevatron, currently the worlds most powerful particle accelerator, will shut down by the end of this decade after the LHC at CERN begins operations. At the LHC, U.S. physicists will join scientists from around the world in the exploration of the physics of the Terascale. To follow the LHC, physicists propose the International Linear Collider, a globally funded and operated accelerator to build on LHC results and illuminate Terascale science. Fermilab will work to host the proposed ILC in the U.S. as soon as possible, maintaining the nations historic leadership of frontier particle physics. Should events postpone the start of the ILC, Fermilab would build an intensity-frontier accelerator at one percent of the ILCs length and combine it with existing accelerators to create Project X. Project Xs intense beams would give Fermilabs scientific users a new way into the world of neutrinos and precision physics. With its ILC technology, Project X would spur U.S. industrialization and reduce costs of ILC components while advancing accelerator science for future applications in particle physics and beyond. In addition, Project X would drive forward the technology for still higher-energy accelerators of the future, such as a muon collider. Fermilabs plan would maintain the nations leadership in particle physics, keeping the laboratory and U.S. particle physics on the pathway to discovery both at the Terascale with the ILC and beyond, and in the domain of neutrinos and precision physics at the intensity frontier.
Top/Physics/Particle_Physics	'LHCb: Search for New Physics at a Hadron B factory' Many essential features of the quark sector of the Standard Model have first been probed by s- and b-quark decays in a indirect manner: notable examples include the SU(2) doublet structure, the existence of three generations, and the masses of the charm and top quarks. Recent progress made by the experiments at B factories and Tevatron in the study of b-hadron decays now put serious constraints in the characteristics of possible physics beyond the Standard Model, through measurements related to the Flavour Changing Neutral Current. When the LHC becomes operational, it will be the most powerful source of b-hadrons. The LHCb experiment will then be able to study such processes in the Bs system with a similar accuracy to that achieved by the B factories in the Bd system, allowing to improve further the stringent tests of the Standard Model. The experiment will also improve the measurements in the Bd system as well. It may observe a sign of new physics first and, in combination with the ATLAS and CMS results, will probe the flavour structure of the new physics. In this presentation, we review the evolution of the LHCb experiment in conjunction with the development of physics, the current status of the experiment and its prospects.
Top/Physics/Particle_Physics	TOTEM, a different LHC experiment TOTEM will pursue a physics program (complementary to that of the other LHC detectors) spanning a wide range from total cross-section and elastic scattering measurements to the study of diffractive and forward phenomena. The TOTEM program will lead to a better understanding of the fundamental aspects of strong interactions. For the first time at hadron colliders, the very forward rapidity range, containing 90% of the energy flow and explored in high-energy cosmic ray experiments, is covered, allowing the search for unusual phenomena hinted at by cosmic ray experiments. The technical implementation of all TOTEM detectors is described. Silicon sensors housed in so-called Roman pots allow measurements of elastic and diffractive protons at distances as small as 1 mm from the beam centre. A scheme to tag events from Double-Pomeron-Exchange by diffractive protons on both sides transforms the LHC into an almost clean gluon collider, where the centre-of-mass energy is determined by the momentum losses of the forward protons, thus offering an interesting way to search for new particles. In a later stage, the combination of CMS and TOTEM will provide an unprecedented almost complete rapidity coverage, allowing a variety of new studies, including hard diffraction.
Top/Technology/Energy	Energy, Sustainability and Development A huge increase in energy use is expected in the coming decades see the IEAs business as usual/reference scenario below. While developed countries could use less energy, a large increase is needed to lift billions out of poverty, including over 25% of the worlds population who still lack electricity. Meeting demand in an environmentally responsible manner will be a huge challenge. The World Bank estimates that coal pollution leads to 300,000 deaths in China each year, while smoke from cooking and heating with biomass kills 1.3 million world-wide more than malaria. The IEAs alternative scenario requires a smaller increase in energy use than the reference scenario and is also less carbon intensive, but it still implies that CO2 emissions will increase 30% by 2030 (compared to 55% in the reference scenario). Frighteningly, implementing the alternative scenario faces formidable hurdles according to the IEA, despite the fact that it would yield financial savings for consumers that far exceed the initial additional investment cost. I shall give an overview of the energy outlook and the portfolio of technological and economic measures that are need to meet the energy challenge and do better than the alternative scenario.
Top/Physics/Quantum_Mechanics	Hanbury Brown and Twiss and Other Atom-atom Correlations: Advances in Quantum Atom Optics Fifty years ago, two astronomers, R. Hanbury Brown and R. Q. Twiss, invented a new method to measure the angular diameter of stars, in spite of the atmospheric fluctuations. Their proposal prompted a hot debate among physicists : how might two particles (photons), emitted independently (at opposite extremities of a star) , behave in a correlated way when detected ? It was only after the development of R Glauber's full quantum analysis that the effect was understood as a two particle quantum interference effect. From a modern perspective, it can be viewed as an early example of the amazing properties of pairs of entangled particles. The effect has now been observed with bosonic and fermionic atoms, stressing its fully quantum character. After putting these experiments in a historical perspective, I will present recent results, and comment on their significance. I will also show how our single atom detection scheme has allowed us to demonstrate the creation of atom pairs by non linear mixing of matter waves. This result paves the way to experiments aiming at probing entanglement in atom pairs.
Top/Physics/Theoretical_Physics	Summary Summary of Strings 2008, which was be held at CERN in the year during which the LHC started up. This conference, the largest and most important one on String Theory, runs annually and usually draws several hundred participants, mostly active researchers in the field. The main purpose of the conference is to review the latest developments for experts, but there were also be free talks for the general public.
Top/Physics/Theoretical_Physics	Remodeling the Topological String, Perturbatively and Nonperturbatively Lecture held at Strings 2008. On 1823 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
Top/Physics/Theoretical_Physics	Tadpole Cancellation in the Topological String Lecture held at Strings 2008. On 1823 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
Top/Physics/Theoretical_Physics	Cosmological Unification of String Theories Lecture held at Strings 2008. On 1823 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
Top/Physics/Theoretical_Physics	S-duality and Boundary Conditions in N=4 SYM Lecture held at Strings 2008. On 1823 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
Top/Physics/Theoretical_Physics	Developments in BPS Wall-crossing Lecture held at Strings 2008. On 1823 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
Top/Physics/Theoretical_Physics	Extremal Black Holes and AdS2/CFT1 Correspondence Lecture held at Strings 2008. On 1823 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
Top/Physics/Theoretical_Physics	BPS Black Holes and Topological Strings: A Review Lecture held at Strings 2008. On 1823 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
Top/Physics/Theoretical_Physics	Loop Quantum Gravity Lecture held at Strings 2008. On 1823 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
Top/Physics/Theoretical_Physics	4-loop Perturbative Konishi from AdS5xS5 String Sigma Model Lecture held at Strings 2008. On 1823 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
Top/Physics/Theoretical_Physics	General Gauge Mediation Lecture held at Strings 2008. On 1823 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
Top/Physics/Theoretical_Physics	Holographic Gauge Mediation Lecture held at Strings 2008. On 1823 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
Top/Physics/Theoretical_Physics	Holographic Recipes at Finite Density and Low Temperature Lecture held at Strings 2008. On 1823 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
Top/Physics/Theoretical_Physics	Scattering Amplitudes via AdS/CFT Lecture held at Strings 2008. On 1823 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
Top/Physics/Theoretical_Physics	Nonlinear Fluid Dynamics from Gravity Lecture held at Strings 2008. On 1823 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
Top/Physics/Theoretical_Physics	Integrability of the AdS/CFT System Lecture held at Strings 2008. On 1823 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
Top/Physics/Theoretical_Physics	The Search for New Physics at the LHC Lecture held at Strings 2008. On 1823 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
Top/Physics/Theoretical_Physics	String Phenomenology on the Eve of the LHC Lecture held at Strings 2008. On 1823 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
Top/Physics/Theoretical_Physics	F-theory, GUTs, and the Weak Scale Lecture held at Strings 2008. On 1823 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
Top/Physics/Theoretical_Physics	A New Infinite Class of Anti-de Sitter Flux Vacua Lecture held at Strings 2008. On 1823 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
Top/Physics/Theoretical_Physics	D-brane Instantons in Type II Orientifolds Lecture held at Strings 2008. On 1823 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
Top/Physics/Theoretical_Physics	N=6 Chern Simons Matter Theories, M2 Branes and Supergravity Lecture held at Strings 2008. On 1823 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
Top/Physics/Theoretical_Physics	Unveiling the Structure of Amplitudes in Gauge Theory and Gravity Lecture held at Strings 2008. On 1823 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
Top/Physics/Theoretical_Physics	What is the Simplest Quantum Field Theory? Lecture held at Strings 2008. On 1823 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
Top/Physics/Theoretical_Physics	Fermionic T-duality Lecture held at Strings 2008. On 1823 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
Top/Physics/Theoretical_Physics	Dual Superconformal Symmetry of Scattering Amplitudes in N=4 SYM Lecture held at Strings 2008. On 1823 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
Top/Physics/Theoretical_Physics	The Gauge-string Duality and QCD at Finite Temperature Lecture held at Strings 2008. On 1823 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
Top/Physics/Theoretical_Physics	Forty Years of High Energy String Collisions Lecture held at Strings 2008. On 1823 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
Top/Physics/Theoretical_Physics	Maximal Supersymmetry, Duality and Scattering Amplitudes Lecture held at Strings 2008. On 1823 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
Top/Physics/Theoretical_Physics	Multiple Membrane Dynamics Lecture held at Strings 2008. On 1823 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
Top/Physics/Theoretical_Physics	Lagrangians for Multiple M2-branes Lecture held at Strings 2008. On 1823 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
Top/Physics/Theoretical_Physics	Heterotic Standard Models Lecture held at Strings 2008. On 1823 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
Top/Physics/Theoretical_Physics	Superstring Amplitudes: Formal Progress and Implications for LHC Lecture held at Strings 2008. On 1823 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
Top/Physics/Theoretical_Physics	Outlook Outlook for Strings 2008. On 1823 August 2008, CERN hosted the 2008 edition of Strings, the annual conference that focuses on superstring theory and related matters.
Top/Computer_Science/Programming_languages	Advanced Topics in Programming Languages Series: Python Design Patterns
Top/Physics/Applied_Physics	Optoacoustic imaging, a promising technique for non-invasive diagnosis of cancer Optoacoustics provides high optical contrast without the handicap of poor resolution in imaging of optically turbid tissues. In biomedical optoacoustics, tissue is illuminated with short laser pulses. The light is scattered inside the tissue and heats absorbing structures, such as blood vessels, hidden deeply inside the tissue. Image contrast is therefore provided by light absorbing chromophores, either endogenous (such as oxy- or deoxyhemoglobin) or exogenous (e.g. dyes, nanoparticles or quantum dots). By means of the thermoelastic effect, the inhomogeneous heating generates pressure transients exactly representing the absorbing structures. These acoustic transients propagate to the tissue surface and can be detected with an appropriate ultrasound transducer. In one-dimensional optoacoustic measurements, time delay between the laser pulse and detected pressure transient, its amplitude and temporal profile provide information about the location, strength and spatial dimension of the acoustic source. Three-dimensional images can be reconstructed by scanning the transducer. The image quality depends on a number of factors, including the irradiation geometry and image reconstruction algorithm. The talk will give an overview of the possibilities and limitations of optoacoustic imaging in turbid tissues, especially in terms of image contrast and depth resolution.
Top/Computer_Science/Optimization_Methods/Stochastic_Optimization/Genetic_Algorithms	A Genetic Algorithm for Text Classification Rule Induction This paper presents a Genetic Algorithm, called Olex-GA, for the induction of rule-based text classifiers of the form ``classify document $d$ under category $c$ if $t_1 in d$ or ... or $t_n in d$ and not ($t_{n+1} in d$ or ... or $t_{n+m} in d$) holds'', where each $t_i$ is a term. Olex-GA relies on an efficient emph{several-rules-per-individual} binary representation and uses the $F$-measure as the fitness function. The proposed approach is tested over the standard test sets Reuters and Ohsumed and compared against several classification algorithms (namely, Naive Bayes, Ripper, C4.5, SVM). Experimental results demonstrate that it achieves very good performance on both data collections, showing to be competitive with (and indeed outperforming in some cases) the evaluated classifiers. ; Note: : A prototype of the rule induction system Olex-GA described in that paper is available at the address [[http://www.mat.unical.it/Olex-GA]]
